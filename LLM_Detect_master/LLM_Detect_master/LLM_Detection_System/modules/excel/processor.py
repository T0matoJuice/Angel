#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelå·¥å•æ™ºèƒ½å¤„ç†å™¨æ¨¡å— - æä¾›åŸºäºç¡…åŸºæµåŠ¨å¤§æ¨¡å‹çš„å·¥å•æ•°æ®æ™ºèƒ½å¡«å……å’Œåˆ†æåŠŸèƒ½
"""

import os
import pandas as pd
from pathlib import Path
from openai import OpenAI
from modules.common.prompts import load_prompt

class Processor:
    """Excelå·¥å•æ™ºèƒ½å¡«å……å¤„ç†å™¨

    ä½¿ç”¨ç¡…åŸºæµåŠ¨å¤§æ¨¡å‹è¿›è¡Œå·¥å•æ•°æ®çš„æ™ºèƒ½å­¦ä¹ å’Œæ¨ç†å¡«å……ï¼Œæ”¯æŒä¸¤é˜¶æ®µå¤„ç†ï¼š
    1. ä»è®­ç»ƒæ•°æ®å­¦ä¹ è§„åˆ™å’Œæ¨¡å¼
    2. å°†å­¦åˆ°çš„è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•æ•°æ®è¿›è¡Œæ™ºèƒ½å¡«å……
    """

    def __init__(self):
        """åˆå§‹åŒ–å¤§æ¨¡å‹å¤„ç†å™¨

        é…ç½®Kimiå®˜æ–¹APIå®¢æˆ·ç«¯å’Œç›¸å…³å‚æ•°ï¼Œå‡†å¤‡è¿›è¡Œæ™ºèƒ½å¤„ç†
        """
        
        # è·å–ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥å’Œæ¨¡å‹
        # ä½¿ç”¨Kimiå®˜æ–¹APIé…ç½®ï¼ˆæ›¿ä»£ç¡…åŸºæµåŠ¨ï¼‰
        # æœ¬åœ°å¤§æ¨¡å‹é…ç½®ï¼ˆå·²åœç”¨ï¼‰:
        # self.api_key = 'Angel@20250428'
        # self.model = 'Qwen3-80B-FP8'
        # self.base_url = 'http://10.2.32.163:8000/v1'
        # self.api_key = os.getenv('EXCEL_API_KEY', 'Angel@20250428')
        # self.model = os.getenv('EXCEL_MODEL_NAME', 'Qwen3-80B-FP8')
        # self.base_url = os.getenv('EXCEL_BASE_URL', 'http://10.2.32.163:8000/v1')
        
        # Kimiå®˜æ–¹APIé…ç½®ï¼ˆMoonshotï¼‰
        self.api_key = os.getenv('MOONSHOT_API_KEY', 'sk-S7PzDUN4aPRl8sr5zxqNe9umeTnPj9AdirkD9wKTN9IluR2i')
        self.model = os.getenv('MOONSHOT_MODEL_0711', 'kimi-k2-0905-preview')  # ä½¿ç”¨K2-0711æ¨¡å‹
        self.base_url = os.getenv('MOONSHOT_BASE_URL', 'https://api.moonshot.cn/v1')

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°MOONSHOT_API_KEYç¯å¢ƒå˜é‡")

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¼å®¹OpenAIæ¥å£æ ¼å¼ï¼‰
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=600  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°10åˆ†é’Ÿï¼Œé€‚åº”å¤§æ‰¹é‡æ•°æ®å¤„ç†
        )

    def _read_excel_to_text(self, excel_path: str) -> str:
        """å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç”¨äºAIåˆ†æ

        Args:
            excel_path (str): Excelæ–‡ä»¶è·¯å¾„

        Returns:
            str: Excelå†…å®¹çš„æ–‡æœ¬è¡¨ç¤º
        """
        try:
            df = pd.read_excel(excel_path, dtype=str)

            # æ¸…ç†æ‰€æœ‰å­—æ®µä¸­çš„æ¢è¡Œç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼
            # è¿™æ ·å¯ä»¥é¿å…AIç”Ÿæˆçš„CSVæ ¼å¼å‡ºç°é—®é¢˜
            for col in df.columns:
                df[col] = df[col].astype(str).str.replace('\r\n', ' ', regex=False)
                df[col] = df[col].astype(str).str.replace('\n', ' ', regex=False)
                df[col] = df[col].astype(str).str.replace('\r', ' ', regex=False)

            # å°†DataFrameè½¬æ¢ä¸ºCSVæ ¼å¼æ–‡æœ¬
            return df.to_csv(index=False)
        except Exception as e:
            raise Exception(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")

    def _fix_csv_format(self, csv_text: str) -> str:
        """ä¿®å¤CSVæ ¼å¼ï¼Œç¡®ä¿åŒ…å«é€—å·ã€æ¢è¡Œç¬¦çš„å­—æ®µè¢«æ­£ç¡®å¼•å·åŒ…è£¹

        Args:
            csv_text (str): åŸå§‹CSVæ–‡æœ¬

        Returns:
            str: ä¿®å¤åçš„CSVæ–‡æœ¬
        """
        try:
            import io
            import csv

            # ä½¿ç”¨StringIOè¯»å–æ•´ä¸ªCSVæ–‡æœ¬ï¼ˆæ”¯æŒå¤šè¡Œå­—æ®µï¼‰
            input_stream = io.StringIO(csv_text.strip())
            output_stream = io.StringIO()

            # ä½¿ç”¨csv.readerè¯»å–ï¼ˆè‡ªåŠ¨å¤„ç†å¼•å·å’Œæ¢è¡Œç¬¦ï¼‰
            reader = csv.reader(input_stream)
            # ä½¿ç”¨csv.writerå†™å…¥ï¼ˆè‡ªåŠ¨æ·»åŠ å¿…è¦çš„å¼•å·ï¼‰
            writer = csv.writer(output_stream, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')

            for row in reader:
                # è·³è¿‡ç©ºè¡Œ
                if not row or all(field.strip() == '' for field in row):
                    continue
                writer.writerow(row)

            return output_stream.getvalue()
        except Exception as e:
            # å¦‚æœä¿®å¤å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            print(f"è­¦å‘Šï¼šCSVæ ¼å¼ä¿®å¤å¤±è´¥: {str(e)}")
            return csv_text

    def _validate_and_fix_csv_fields(self, csv_text: str, expected_field_count: int = 9) -> str:
        """éªŒè¯å¹¶ä¿®å¤CSVå­—æ®µæ•°é‡

        Args:
            csv_text (str): CSVæ–‡æœ¬
            expected_field_count (int): æœŸæœ›çš„å­—æ®µæ•°é‡

        Returns:
            str: ä¿®å¤åçš„CSVæ–‡æœ¬
        """
        try:
            import io
            import csv

            lines = csv_text.strip().split('\n')
            if not lines:
                return csv_text

            # è¯»å–å¤´éƒ¨
            header = lines[0]
            header_fields = header.split(',')

            # å¦‚æœå¤´éƒ¨å­—æ®µæ•°é‡ä¸å¯¹ï¼Œç›´æ¥è¿”å›
            if len(header_fields) != expected_field_count:
                print(f"è­¦å‘Šï¼šCSVå¤´éƒ¨å­—æ®µæ•°é‡ä¸æ­£ç¡®ï¼ŒæœŸæœ›{expected_field_count}ä¸ªï¼Œå®é™…{len(header_fields)}ä¸ª")
                return csv_text

            # ä½¿ç”¨csv.readerè§£ææ¯ä¸€è¡Œ
            input_stream = io.StringIO(csv_text.strip())
            reader = csv.reader(input_stream)

            output_stream = io.StringIO()
            writer = csv.writer(output_stream, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')

            row_num = 0
            for row in reader:
                row_num += 1

                # è·³è¿‡ç©ºè¡Œ
                if not row or all(field.strip() == '' for field in row):
                    continue

                # æ£€æŸ¥å­—æ®µæ•°é‡
                if len(row) != expected_field_count:
                    print(f"è­¦å‘Šï¼šç¬¬{row_num}è¡Œå­—æ®µæ•°é‡ä¸æ­£ç¡®ï¼ŒæœŸæœ›{expected_field_count}ä¸ªï¼Œå®é™…{len(row)}ä¸ª")
                    print(f"  åŸå§‹æ•°æ®: {row[:3]}... (å…±{len(row)}ä¸ªå­—æ®µ)")

                    # å¦‚æœå­—æ®µå¤ªå°‘ï¼Œç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½
                    if len(row) < expected_field_count:
                        row.extend([''] * (expected_field_count - len(row)))
                        print(f"  å·²è¡¥é½ä¸º{expected_field_count}ä¸ªå­—æ®µ")
                    # å¦‚æœå­—æ®µå¤ªå¤šï¼Œæ™ºèƒ½å¤„ç†
                    elif len(row) > expected_field_count:
                        # å¦‚æœæ˜¯10ä¸ªå­—æ®µï¼Œå¾ˆå¯èƒ½æ˜¯æ¼æ‰äº†"æ•…éšœç»„"å­—æ®µ
                        # å°è¯•ä¿ç•™æœ€åä¸€ä¸ªå­—æ®µï¼ˆå·¥å•æ€§è´¨ï¼‰ï¼Œåˆ é™¤ä¸­é—´çš„æŸä¸ªå­—æ®µ
                        if len(row) == 10:
                            # ä¿ç•™æœ€åä¸€ä¸ªå­—æ®µï¼ˆå·¥å•æ€§è´¨ï¼‰
                            last_field = row[-1]
                            # åˆ é™¤ç¬¬3ä¸ªå­—æ®µï¼ˆç´¢å¼•2ï¼‰ï¼Œå› ä¸ºå¾ˆå¯èƒ½æ˜¯"æ•…éšœç»„"è¢«æ¼æ‰ï¼Œå¯¼è‡´åé¢å­—æ®µé”™ä½
                            # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬ä¿ç•™å‰2ä¸ªå’Œæœ€å1ä¸ªï¼Œä¸­é—´çš„æˆªæ–­åˆ°6ä¸ª
                            fixed_row = row[:2] + row[3:9] + [last_field]
                            print(f"  æ™ºèƒ½ä¿®å¤ï¼šä¿ç•™æœ€åå­—æ®µ'{last_field}'ï¼Œåˆ é™¤å¤šä½™å­—æ®µ")
                            row = fixed_row
                        else:
                            # å…¶ä»–æƒ…å†µï¼Œç®€å•æˆªæ–­
                            row = row[:expected_field_count]
                            print(f"  å·²æˆªæ–­ä¸º{expected_field_count}ä¸ªå­—æ®µ")

                writer.writerow(row)

            return output_stream.getvalue()
        except Exception as e:
            print(f"è­¦å‘Šï¼šCSVå­—æ®µéªŒè¯å¤±è´¥: {str(e)}")
            return csv_text


    def learn_rules(self, training_excel: str) -> tuple:
        """ç¬¬ä¸€é˜¶æ®µï¼šä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å·¥å•é—®é¢˜ç‚¹æ¨ç†è§„åˆ™

        åˆ†æè®­ç»ƒExcelæ•°æ®ï¼Œå­¦ä¹ å¦‚ä½•æ ¹æ®å·¥å•ä¿¡æ¯æ¨ç†å‡ºç»´ä¿®é—®é¢˜ç‚¹å’ŒäºŒçº§é—®é¢˜ç‚¹

        Args:
            training_excel (str): è®­ç»ƒæ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (å­¦ä¹ åˆ°çš„è§„åˆ™å’Œå¯¹è¯æ¶ˆæ¯, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–è®­ç»ƒExcelæ–‡ä»¶å†…å®¹ï¼ˆç¡…åŸºæµåŠ¨ä¸æ”¯æŒæ–‡ä»¶ä¸Šä¼ ï¼Œç›´æ¥è¯»å–å†…å®¹ï¼‰
            def _compact_training(df: pd.DataFrame) -> str:
                df = df.copy()
                drop_cols = [c for c in df.columns if str(c).startswith('Unnamed:')]
                if drop_cols:
                    df = df.drop(columns=drop_cols)
                for c in df.columns:
                    df[c] = df[c].astype(str).fillna('')

                def cnt(series_cond):
                    try:
                        return int(series_cond.sum())
                    except Exception:
                        return 0

                nature_counts = df.get('å·¥å•æ€§è´¨', pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
                svc = df.get('æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', pd.Series(dtype=str))
                cat = df.get('æ•…éšœç±»åˆ«', pd.Series(dtype=str))
                bn = df.get('ä¿å†…ä¿å¤–', pd.Series(dtype=str))
                oldp = df.get('æ—§ä»¶åç§°', pd.Series(dtype=str))
                newp = df.get('æ–°ä»¶åç§°', pd.Series(dtype=str))
                call = df.get('æ¥ç”µå†…å®¹', pd.Series(dtype=str))
                diag = df.get('ç°åœºè¯Šæ–­æ•…éšœç°è±¡', pd.Series(dtype=str))
                plan = df.get('å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨', pd.Series(dtype=str))

                core_parts = ['ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ']
                exchange_words = ['æ¢æœº','é€€æœº','é€€è´§']
                filter_words = ['æ¼ç‚­','é»‘ç‚¹','é»‘æ¸£','ç¢³ç²‰','æ´»æ€§ç‚­ç²‰æœ«']

                def contains_any(series, words):
                    s = series.astype(str)
                    return s.apply(lambda x: any(w in x for w in words))

                stats_lines = []
                stats_lines.append(f"æ ·æœ¬é‡: {len(df)}")
                if nature_counts:
                    stats_lines.append(f"å·¥å•æ€§è´¨åˆ†å¸ƒ: {nature_counts}")
                stats_lines.append(f"æœåŠ¡é¡¹ç›®Top10: {svc.value_counts().head(10).to_dict()}")
                stats_lines.append(f"æ•…éšœç±»åˆ«Top10: {cat.value_counts().head(10).to_dict()}")
                core_hit = cnt(oldp.apply(lambda x: any(p in str(x) for p in core_parts)) | newp.apply(lambda x: any(p in str(x) for p in core_parts)))
                stats_lines.append(f"æ ¸å¿ƒéƒ¨ä»¶å‘½ä¸­æ•°: {core_hit}")
                filt_hit = cnt(oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False))
                stats_lines.append(f"æ»¤èŠ¯æ›´æ¢è®°å½•æ•°: {filt_hit}")
                policy_hit = cnt(bn.astype(str).eq('ä¿å¤–è½¬ä¿å†…') & (oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False)))
                stats_lines.append(f"ä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯è®°å½•æ•°: {policy_hit}")
                noise_exchange = cnt(((plan.str.contains('å™ªéŸ³|åˆ†è´', na=False)) | (call.str.contains('å™ªéŸ³|åˆ†è´', na=False))) & (contains_any(plan, exchange_words) | contains_any(call, exchange_words)))
                stats_lines.append(f"å™ªéŸ³/åˆ†è´+æ¢é€€è®°å½•æ•°: {noise_exchange}")
                add_hit = cnt(svc.str.contains('åŠ è£…', na=False))
                safe_hit = cnt(svc.str.contains('å®‰å…¨ç»´æŠ¤', na=False))
                stats_lines.append(f"åŠ è£…è®°å½•æ•°: {add_hit}")
                stats_lines.append(f"å®‰å…¨ç»´æŠ¤è®°å½•æ•°: {safe_hit}")

                preview_cols = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
                for c in preview_cols:
                    if c not in df.columns:
                        df[c] = ''
                df_preview = df[preview_cols].copy()
                for c in ['æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨','åˆ¤å®šä¾æ®']:
                    df_preview[c] = df_preview[c].astype(str).str.slice(0, 120)
                max_rows = min(200, len(df_preview))
                df_preview = df_preview.head(max_rows)
                preview_csv = df_preview.to_csv(index=False)
                summary = "\n".join(stats_lines)
                return f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}\n\n# æ ·æœ¬é¢„è§ˆ(æœ€å¤š{max_rows}è¡Œ)\n{preview_csv}"

            training_content = _compact_training(df_training)

            # æ„å»ºå­¦ä¹ è§„åˆ™çš„æç¤ºè¯
            prompt1 = load_prompt('excel_learn_rules').format(training_content=training_content)

            messages = [{"role": "user", "content": prompt1}]

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹å­¦ä¹ è§„åˆ™
            resp1 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.6,  # æ§åˆ¶åˆ›é€ æ€§
                max_tokens=4096  # å…è®¸è¾ƒé•¿å›ç­”
            )

            # æå–å­¦ä¹ åˆ°çš„è§„åˆ™
            rules = resp1.choices[0].message.content.strip()
            messages.append({"role": "assistant", "content": rules})

            return messages, rules, resp1.usage  # è¿”å›å¯¹è¯å†å²ã€è§„åˆ™å’Œtokenä½¿ç”¨é‡

        except Exception as e:
            raise Exception(f"å­¦ä¹ è§„åˆ™å¤±è´¥: {str(e)}")

    def apply_rules(self, messages: list, test_excel: str) -> tuple:
        """ç¬¬äºŒé˜¶æ®µï¼šåº”ç”¨å­¦ä¹ åˆ°çš„è§„åˆ™å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œæ™ºèƒ½å¡«å……

        å°†ç¬¬ä¸€é˜¶æ®µå­¦åˆ°çš„æ¨ç†è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•Excelæ•°æ®ï¼Œè‡ªåŠ¨å¡«å……ç»´ä¿®é—®é¢˜ç‚¹å’ŒäºŒçº§é—®é¢˜ç‚¹

        Args:
            messages (list): åŒ…å«å­¦ä¹ è§„åˆ™çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            test_excel (str): æµ‹è¯•æ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (å¡«å……ç»“æœCSVå†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–æµ‹è¯•Excelæ–‡ä»¶å†…å®¹
            test_content = self._read_excel_to_text(test_excel)

            # è¯»å–Excelè·å–æ•°æ®è¡Œæ•°ï¼Œç¡®ä¿AIè¾“å‡ºå®Œæ•´çš„ç»“æœ
            df_test = pd.read_excel(test_excel, dtype=str)
            test_row_count = len(df_test)

            # æ„å»ºåº”ç”¨è§„åˆ™çš„æç¤ºè¯ï¼ŒæŒ‡å¯¼AIå°†å­¦åˆ°çš„è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•æ•°æ®
            prompt2 = load_prompt('excel_apply_rules').format(
                test_content=test_content,
                test_row_count=test_row_count
            )

            # å°†åº”ç”¨è§„åˆ™çš„è¯·æ±‚æ·»åŠ åˆ°å¯¹è¯å†å²ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
            messages.append({"role": "user", "content": prompt2})

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹åº”ç”¨è§„åˆ™è¿›è¡Œå¡«å……
            resp2 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,  # åŒ…å«ä¹‹å‰å­¦ä¹ çš„è§„åˆ™
                temperature=0.6,
                max_tokens=4096
            )

            # æå–å¡«å……ç»“æœ
            filled_result = resp2.choices[0].message.content.strip()

            # æ¸…ç†AIè¿”å›çš„ç»“æœï¼Œç§»é™¤Markdownæ ‡è®°
            if filled_result.startswith('```csv'):
                filled_result = filled_result[6:]  # ç§»é™¤å¼€å¤´çš„```csv
            if filled_result.endswith('```'):
                filled_result = filled_result[:-3]  # ç§»é™¤ç»“å°¾çš„```
            filled_result = filled_result.strip()

            # ç¡®ä¿æœ‰æ­£ç¡®çš„CSVå¤´éƒ¨ï¼ˆæ ¹æ®å·¥å•é—®é¢˜ç‚¹æ£€æµ‹çš„å­—æ®µï¼‰
            expected_header = 'ç¼–å·(ç»´ä¿®è¡Œ),æ¥ç”µå†…å®¹(ç»´ä¿®è¡Œ),ç°åœºè¯Šæ–­æ•…éšœç°è±¡(ç»´ä¿®è¡Œ),æ•…éšœéƒ¨ä½(ç»´ä¿®è¡Œ),æ•…éšœä»¶åç§°(ç»´ä¿®è¡Œ),å¤„ç†æ–¹æ¡ˆç®€è¿°(ç»´ä¿®è¡Œ),æ•…éšœç±»åˆ«(ç»´ä¿®è¡Œ),ç»´ä¿®é—®é¢˜ç‚¹,äºŒçº§é—®é¢˜ç‚¹'
            if not filled_result.startswith('ç¼–å·(ç»´ä¿®è¡Œ)'):
                filled_result = expected_header + '\n' + filled_result

            return filled_result, resp2.usage  # è¿”å›æ¸…ç†åçš„å¡«å……ç»“æœå’Œtokenä½¿ç”¨é‡

        except Exception as e:
            raise Exception(f"åº”ç”¨è§„åˆ™å¤±è´¥: {str(e)}")

    def learn_quality_rules_v1_backup(self, training_excel: str) -> tuple:
        """å¤‡ä»½ç‰ˆæœ¬ï¼šä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ è´¨é‡å·¥å•è¯†åˆ«è§„åˆ™

        ç¬¬ä¸€é˜¶æ®µå¤„ç†ï¼šåˆ†æè®­ç»ƒExcelæ•°æ®ï¼Œå­¦ä¹ è´¨é‡å·¥å•çš„åˆ¤æ–­è§„å¾‹å’Œæ ‡æ³¨æ ‡å‡†

        Args:
            training_excel (str): è®­ç»ƒæ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (å­¦ä¹ åˆ°çš„è§„åˆ™å†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–è®­ç»ƒExcelæ–‡ä»¶å†…å®¹
            # å‹ç¼©è®­ç»ƒå†…å®¹ï¼Œé¿å…è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
            def _contains_any(series, words):
                s = series.astype(str)
                return s.apply(lambda x: any(w in x for w in words))
            df_compact = df_training.copy()
            drop_cols = [c for c in df_compact.columns if str(c).startswith('Unnamed:')]
            if drop_cols:
                df_compact = df_compact.drop(columns=drop_cols)
            for c in df_compact.columns:
                df_compact[c] = df_compact[c].astype(str).fillna('')
            nature_counts = df_compact.get('å·¥å•æ€§è´¨', pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
            svc = df_compact.get('æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', pd.Series(dtype=str))
            cat = df_compact.get('æ•…éšœç±»åˆ«', pd.Series(dtype=str))
            bn = df_compact.get('ä¿å†…ä¿å¤–', pd.Series(dtype=str))
            oldp = df_compact.get('æ—§ä»¶åç§°', pd.Series(dtype=str))
            newp = df_compact.get('æ–°ä»¶åç§°', pd.Series(dtype=str))
            call = df_compact.get('æ¥ç”µå†…å®¹', pd.Series(dtype=str))
            diag = df_compact.get('ç°åœºè¯Šæ–­æ•…éšœç°è±¡', pd.Series(dtype=str))
            plan = df_compact.get('å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨', pd.Series(dtype=str))
            core_parts = ['ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ']
            stats_lines = []
            stats_lines.append(f"æ ·æœ¬é‡: {len(df_compact)}")
            if nature_counts:
                stats_lines.append(f"å·¥å•æ€§è´¨åˆ†å¸ƒ: {nature_counts}")
            stats_lines.append(f"æœåŠ¡é¡¹ç›®Top10: {svc.value_counts().head(10).to_dict() if svc is not None else {}}")
            stats_lines.append(f"æ•…éšœç±»åˆ«Top10: {cat.value_counts().head(10).to_dict() if cat is not None else {}}")
            core_hit = int((oldp.apply(lambda x: any(p in str(x) for p in core_parts)) | newp.apply(lambda x: any(p in str(x) for p in core_parts))).sum()) if (oldp is not None and newp is not None) else 0
            stats_lines.append(f"æ ¸å¿ƒéƒ¨ä»¶å‘½ä¸­æ•°: {core_hit}")
            filt_hit = int((oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False)).sum()) if (oldp is not None and newp is not None) else 0
            stats_lines.append(f"æ»¤èŠ¯æ›´æ¢è®°å½•æ•°: {filt_hit}")
            policy_hit = int((bn.astype(str).eq('ä¿å¤–è½¬ä¿å†…') & (oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False))).sum()) if (bn is not None and oldp is not None and newp is not None) else 0
            stats_lines.append(f"ä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯è®°å½•æ•°: {policy_hit}")
            preview_cols = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
            for c in preview_cols:
                if c not in df_compact.columns:
                    df_compact[c] = ''
            df_preview = df_compact[preview_cols].copy()
            for c in ['æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨','åˆ¤å®šä¾æ®']:
                df_preview[c] = df_preview[c].astype(str).str.slice(0, 120)
            max_rows = min(200, len(df_preview))
            df_preview = df_preview.head(max_rows)
            preview_csv = df_preview.to_csv(index=False)
            summary = "\n".join(stats_lines)
            training_content = f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}\n\n# æ ·æœ¬é¢„è§ˆ(æœ€å¤š{max_rows}è¡Œ)\n{preview_csv}"

            # æ„å»ºå­¦ä¹ è§„åˆ™çš„æç¤ºè¯
            prompt1 = load_prompt('quality_learn_rules_optimized').format(training_content=training_content)

            messages = [{"role": "user", "content": prompt1}]

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹å­¦ä¹ è§„åˆ™
            resp1 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.6,
                max_tokens=4096
            )

            # æå–å­¦ä¹ åˆ°çš„è§„åˆ™
            rules = resp1.choices[0].message.content.strip()
            messages.append({"role": "assistant", "content": rules})

            return messages, rules, resp1.usage

        except Exception as e:
            raise Exception(f"å­¦ä¹ éè´¨é‡é—®é¢˜è¯†åˆ«è§„åˆ™å¤±è´¥: {str(e)}")

    def apply_quality_rules_v1_backup(self, messages: list, test_excel: str) -> tuple:
        """å¤‡ä»½ç‰ˆæœ¬ï¼šåº”ç”¨å­¦ä¹ åˆ°çš„è§„åˆ™å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œè´¨é‡å·¥å•åˆ¤æ–­

        ç¬¬äºŒé˜¶æ®µå¤„ç†ï¼šå°†ç¬¬ä¸€é˜¶æ®µå­¦åˆ°çš„è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œè´¨é‡å·¥å•åˆ†ç±»

        Args:
            messages (list): åŒ…å«å­¦ä¹ è§„åˆ™çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            test_excel (str): æµ‹è¯•æ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (åˆ¤æ–­ç»“æœCSVå†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–æµ‹è¯•Excelæ–‡ä»¶å†…å®¹
            test_content = self._read_excel_to_text(test_excel)

            # è¯»å–Excelè·å–æ•°æ®è¡Œæ•°
            df_test = pd.read_excel(test_excel, dtype=str)
            test_row_count = len(df_test)

            # æ„å»ºåº”ç”¨è§„åˆ™çš„æç¤ºè¯
            prompt2 = load_prompt('quality_apply_rules_optimized').format(
                test_content=test_content,
                test_row_count=test_row_count
            )

            # æ·»åŠ åº”ç”¨è§„åˆ™çš„ç”¨æˆ·è¯·æ±‚åˆ°å¯¹è¯å†å²
            messages.append({"role": "user", "content": prompt2})

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹åº”ç”¨è§„åˆ™è¿›è¡Œåˆ¤æ–­
            resp2 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.2,
                max_tokens=4096
            )

            # æå–åˆ¤æ–­ç»“æœ
            quality_result = resp2.choices[0].message.content.strip()

            # æ¸…ç†AIè¿”å›çš„ç»“æœï¼Œç§»é™¤Markdownæ ‡è®°
            if quality_result.startswith('```csv'):
                quality_result = quality_result[6:]  # ç§»é™¤å¼€å¤´çš„```csv
            if quality_result.endswith('```'):
                quality_result = quality_result[:-3]  # ç§»é™¤ç»“å°¾çš„```
            quality_result = quality_result.strip()

            # ç¡®ä¿æœ‰CSVå¤´éƒ¨ï¼ˆ13åˆ—ç»“æ„ï¼‰
            expected_header = 'å·¥å•å•å·,åˆ¤å®šä¾æ®,æ•…éšœéƒ¨ä½åç§°,æ•…éšœç»„,æ•…éšœç±»åˆ«,æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡,æ•…éšœä»¶ç®€ç§°,æ—§ä»¶åç§°,æ–°ä»¶åç§°,æ¥ç”µå†…å®¹,ç°åœºè¯Šæ–­æ•…éšœç°è±¡,å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨,å·¥å•æ€§è´¨'
            if not quality_result.startswith('å·¥å•å•å·'):
                quality_result = expected_header + '\n' + quality_result

            quality_result = self._strip_unexpected_header_rows(quality_result, expected_header)
            quality_result = self._force_realign_columns(quality_result, expected_header, 13)
            quality_result = self._fix_csv_format(quality_result)
            quality_result = self._validate_and_fix_csv_fields(quality_result, expected_field_count=13)
            quality_result = self._fix_quality_column_position(quality_result)
            quality_result = self._ensure_order_numbers(quality_result, df_test)
            quality_result = self._enrich_non_quality_basis(quality_result)

            return quality_result, resp2.usage

        except Exception as e:
            raise Exception(f"åº”ç”¨éè´¨é‡é—®é¢˜è¯†åˆ«è§„åˆ™å¤±è´¥: {str(e)}")

    def learn_quality_rules(self, training_excel: str) -> tuple:
        """æ–°ç‰ˆæœ¬ï¼šä¸¤é˜¶æ®µæ¨ç† - ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ ç»´ä¿®é—®é¢˜ç‚¹æ¨ç†è§„åˆ™ - æ–¹æ³•"""
        try:
            import time

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤1: åŠ è½½è®­ç»ƒæ•°æ®")
            print("="*80)
            print(f"è®­ç»ƒæ–‡ä»¶: {training_excel}")

            # è¯»å–è®­ç»ƒExcelæ–‡ä»¶å†…å®¹
            try:
                df_training = pd.read_excel(training_excel, dtype=str)
                training_rows = len(df_training)
                training_cols = len(df_training.columns)
                print(f"åŠ è½½çŠ¶æ€: âœ… æˆåŠŸ")
                print(f"æ•°æ®è§„æ¨¡: {training_rows}è¡Œ x {training_cols}åˆ—")
                print(f"åˆ—å: {', '.join(df_training.columns.tolist())}")
            except Exception as e:
                print(f"åŠ è½½çŠ¶æ€: âŒ å¤±è´¥")
                print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                raise

            # æ„å»ºç²¾ç®€è®­ç»ƒå†…å®¹ï¼Œé¿å…è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
            df_compact = df_training.copy()
            drop_cols = [c for c in df_compact.columns if str(c).startswith('Unnamed:')]
            if drop_cols:
                df_compact = df_compact.drop(columns=drop_cols)
            for c in df_compact.columns:
                df_compact[c] = df_compact[c].astype(str).fillna('')

            def _safe_vc(series):
                try:
                    return series.value_counts().head(10).to_dict()
                except Exception:
                    return {}

            nature_counts = (df_compact['å·¥å•æ€§è´¨'].value_counts(dropna=False).to_dict() if 'å·¥å•æ€§è´¨' in df_compact.columns else {})
            svc = (df_compact['æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡'] if 'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡' in df_compact.columns else pd.Series(dtype=str))
            cat = (df_compact['æ•…éšœç±»åˆ«'] if 'æ•…éšœç±»åˆ«' in df_compact.columns else pd.Series(dtype=str))
            bn = (df_compact['ä¿å†…ä¿å¤–'] if 'ä¿å†…ä¿å¤–' in df_compact.columns else pd.Series(dtype=str))
            oldp = (df_compact['æ—§ä»¶åç§°'] if 'æ—§ä»¶åç§°' in df_compact.columns else pd.Series(dtype=str))
            newp = (df_compact['æ–°ä»¶åç§°'] if 'æ–°ä»¶åç§°' in df_compact.columns else pd.Series(dtype=str))

            core_parts = ['ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ']
            core_hit = int((oldp.apply(lambda x: any(p in str(x) for p in core_parts)) | newp.apply(lambda x: any(p in str(x) for p in core_parts))).sum())
            filt_hit = int((oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False)).sum())
            policy_hit = int((bn.astype(str).eq('ä¿å¤–è½¬ä¿å†…') & (oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False))).sum())

            stats_lines = []
            stats_lines.append(f"æ ·æœ¬é‡: {len(df_compact)}")
            stats_lines.append(f"å·¥å•æ€§è´¨åˆ†å¸ƒ: {nature_counts}")
            stats_lines.append(f"æœåŠ¡é¡¹ç›®Top10: {_safe_vc(svc)}")
            stats_lines.append(f"æ•…éšœç±»åˆ«Top10: {_safe_vc(cat)}")
            stats_lines.append(f"æ ¸å¿ƒéƒ¨ä»¶å‘½ä¸­æ•°: {core_hit}")
            stats_lines.append(f"æ»¤èŠ¯æ›´æ¢è®°å½•æ•°: {filt_hit}")
            stats_lines.append(f"ä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯è®°å½•æ•°: {policy_hit}")

            preview_cols = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
            for c in preview_cols:
                if c not in df_compact.columns:
                    df_compact[c] = ''
            df_preview = df_compact[preview_cols].copy()
            for c in ['æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨','åˆ¤å®šä¾æ®']:
                df_preview[c] = df_preview[c].astype(str).str.slice(0, 120)
            max_rows = min(80, len(df_preview))
            df_preview = df_preview.head(max_rows)
            preview_csv = df_preview.to_csv(index=False)
            summary = "\n".join(stats_lines)
            training_content = f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}\n\n# æ ·æœ¬é¢„è§ˆ(æœ€å¤š{max_rows}è¡Œ)\n{preview_csv}"
            print("-"*80)

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤2: AIå­¦ä¹ é˜¶æ®µï¼ˆç¬¬ä¸€æ­¥æ¨ç†ï¼‰")
            print("="*80)
            print(f"æç¤ºè¯æ–‡ä»¶: prompts/quality_learn_rules_optimized.txt")

            # ä½¿ç”¨è´¨é‡å·¥å•åˆ¤æ–­çš„å­¦ä¹ æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ - é‡‡ç”¨åˆ†ç‰‡å¤šè½®æ³¨å…¥ï¼Œé¿å…è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦
            # ä»…æŠŠç»Ÿè®¡æ‘˜è¦ä½œä¸ºä¸»æç¤ºè¯å†…å®¹
            summary_text = f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}"
            header_prompt = load_prompt('quality_learn_rules_optimized').format(training_content=summary_text)

            # é¢„è§ˆCSVåˆ†ç‰‡ï¼ˆé™åˆ¶æ€»é•¿åº¦ï¼‰
            chunk_size = 3000
            chunks = []
            for i in range(0, len(preview_csv), chunk_size):
                chunks.append(preview_csv[i:i+chunk_size])
            chunks = chunks[:5]

            messages = [{"role": "user", "content": header_prompt}]
            for idx, ch in enumerate(chunks, start=1):
                messages.append({"role": "user", "content": f"è®­ç»ƒæ ·æœ¬ç‰‡æ®µ{idx}:\n{ch}"})
            messages.append({"role": "user", "content": "è¯·åŸºäºä¸Šè¿°ç»Ÿè®¡æ‘˜è¦ä¸æ ·æœ¬ç‰‡æ®µï¼Œæ€»ç»“è´¨é‡å·¥å•ä¸éè´¨é‡å·¥å•çš„è§„åˆ™ã€æƒé‡ä¸å†³ç­–æµç¨‹ï¼Œä¸¥æ ¼æŒ‰è¦æ±‚æ ¼å¼è¾“å‡ºã€‚"})

            print(f"æç¤ºè¯ä¸»å†…å®¹é•¿åº¦: {len(header_prompt)} å­—ç¬¦ï¼Œæ ·æœ¬ç‰‡æ®µæ•°: {len(chunks)}")
            print("-"*80)

            print("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹å­¦ä¹ åˆ¤æ–­è§„å¾‹...")
            start_time = time.time()

            # è°ƒç”¨AIæ¨¡å‹å­¦ä¹ è§„åˆ™
            resp1 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œæ›´èšç„¦äºè§„åˆ™å­¦ä¹ 
                max_tokens=4096
            )

            elapsed_time = time.time() - start_time

            # æå–å­¦ä¹ åˆ°çš„è§„åˆ™
            rules = resp1.choices[0].message.content.strip()
            messages.append({"role": "assistant", "content": rules})

            print(f"âœ… AIå­¦ä¹ å®Œæˆ")
            print(f"è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"å­¦ä¹ ç»“æœé•¿åº¦: {len(rules)} å­—ç¬¦")
            print(f"å­¦ä¹ ç»“æœé¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
            print(rules[:500] + "...")
            print(f"Tokenä½¿ç”¨: è¾“å…¥={resp1.usage.prompt_tokens}, è¾“å‡º={resp1.usage.completion_tokens}, æ€»è®¡={resp1.usage.total_tokens}")
            print("-"*80)

            return messages, rules, resp1.usage

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: å­¦ä¹ ç»´ä¿®é—®é¢˜ç‚¹æ¨ç†è§„åˆ™å¤±è´¥")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            raise Exception(f"å­¦ä¹ ç»´ä¿®é—®é¢˜ç‚¹æ¨ç†è§„åˆ™å¤±è´¥: {str(e)}")

    def apply_quality_rules(self, messages: list, test_excel: str) -> tuple:
        """å·¥å•ç±»å‹æ£€æµ‹ï¼šä¸¤é˜¶æ®µæ¨ç†å¤„ç†

        ç¬¬ä¸€æ­¥ï¼šæ¨ç†ç»´ä¿®é—®é¢˜ç‚¹å’ŒäºŒçº§é—®é¢˜ç‚¹
        ç¬¬äºŒæ­¥ï¼šåŸºäºé—®é¢˜ç‚¹åˆ¤æ–­æ˜¯å¦ä¸ºè´¨é‡å·¥å•

        Args:
            messages (list): åŒ…å«å­¦ä¹ è§„åˆ™çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            test_excel (str): æµ‹è¯•æ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (è´¨é‡å·¥å•åˆ¤æ–­ç»“æœCSVå†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            import time

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤3: åŠ è½½æµ‹è¯•æ•°æ®")
            print("="*80)
            print(f"æµ‹è¯•æ–‡ä»¶: {test_excel}")

            # è¯»å–Excelæ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†
            try:
                df_test = pd.read_excel(test_excel, dtype=str)
                test_rows = len(df_test)
                test_cols = len(df_test.columns)
                print(f"åŠ è½½çŠ¶æ€: âœ… æˆåŠŸ")
                print(f"æ•°æ®è§„æ¨¡: {test_rows}è¡Œ x {test_cols}åˆ—")
                print(f"åˆ—å: {', '.join(df_test.columns.tolist())}")
            except Exception as e:
                print(f"åŠ è½½çŠ¶æ€: âŒ å¤±è´¥")
                print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                raise

            # æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–åˆ—åå’Œæ·»åŠ å¿…è¦å­—æ®µ
            # å¤„ç†å·¥å•ç¼–å·åˆ—çš„æ ‡å‡†åŒ–ï¼ˆæ”¯æŒ"å·¥å•å•å·"æˆ–"ç»´ä¿®å·¥å•å·"ï¼‰
            if 'å·¥å•å•å·' not in df_test.columns and 'ç»´ä¿®å·¥å•å·' not in df_test.columns:
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè‡ªåŠ¨ç”Ÿæˆåºå·ä½œä¸ºå·¥å•å·
                df_test.insert(0, 'å·¥å•å•å·', range(1, len(df_test) + 1))
                print("âš ï¸  æœªæ‰¾åˆ°å·¥å•ç¼–å·åˆ—ï¼Œå·²è‡ªåŠ¨ç”Ÿæˆåºå·")
            elif 'ç»´ä¿®å·¥å•å·' in df_test.columns and 'å·¥å•å•å·' not in df_test.columns:
                # ç»Ÿä¸€åˆ—åï¼šå°†"ç»´ä¿®å·¥å•å·"é‡å‘½åä¸º"å·¥å•å•å·"
                df_test = df_test.rename(columns={'ç»´ä¿®å·¥å•å·': 'å·¥å•å•å·'})
                print("âœ… å·²å°†'ç»´ä¿®å·¥å•å·'é‡å‘½åä¸º'å·¥å•å•å·'")

            # ç¡®ä¿å­˜åœ¨å·¥å•æ€§è´¨åˆ¤æ–­ç»“æœåˆ—
            if 'å·¥å•æ€§è´¨' not in df_test.columns:
                df_test['å·¥å•æ€§è´¨'] = ''
                print("âœ… å·²æ·»åŠ 'å·¥å•æ€§è´¨'åˆ—")

            if 'æ—§ä»¶åç§°' not in df_test.columns:
                df_test['æ—§ä»¶åç§°'] = ''
                print("âœ… å·²æ·»åŠ 'æ—§ä»¶åç§°'åˆ—")
            if 'æ–°ä»¶åç§°' not in df_test.columns:
                df_test['æ–°ä»¶åç§°'] = ''
                print("âœ… å·²æ·»åŠ 'æ–°ä»¶åç§°'åˆ—")

            # ä¿å­˜é¢„å¤„ç†åçš„æ–‡ä»¶
            processed_file = test_excel.replace('.xlsx', '_processed.xlsx')
            df_test.to_excel(processed_file, index=False)
            print(f"é¢„å¤„ç†åæ–‡ä»¶: {processed_file}")
            print("-"*80)

            # è¯»å–é¢„å¤„ç†åçš„æ–‡ä»¶å†…å®¹
            test_content = self._read_excel_to_text(processed_file)

            test_row_count = len(df_test)

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤4: AIæ™ºèƒ½åˆ¤æ–­å·¥å•æ€§è´¨")
            print("="*80)
            print(f"æµ‹è¯•æ•°æ®: {test_row_count} è¡Œ")
            print(f"æç¤ºè¯æ–‡ä»¶: prompts/quality_learn_rules_optimized.txt")
            print("-"*80)

            # æ„å»ºAIåˆ¤æ–­æç¤ºè¯
            quality_prompt = f"""
æ ¹æ®åˆšæ‰ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„åˆ¤æ–­è§„å¾‹ï¼Œå¯¹ä¸‹é¢çš„æµ‹è¯•æ•°æ®è¿›è¡Œå·¥å•æ€§è´¨åˆ¤æ–­ã€‚

æµ‹è¯•æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰ï¼š
{test_content}

**æ ¸å¿ƒè¦æ±‚ï¼šä¸¥æ ¼æŒ‰ç…§ä½ åˆšæ‰å­¦ä¹ çš„è§„åˆ™è¿›è¡Œåˆ¤æ–­ï¼**

åˆ¤æ–­æµç¨‹ï¼ˆå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š

**ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¡¬æ€§è§„åˆ™ï¼ˆç¬¬ä¸€å±‚ï¼‰**
å¯¹æ¯æ¡è®°å½•ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å‘½ä¸­ä»¥ä¸‹ç¡¬æ€§è§„åˆ™ï¼š

Aç±»ï¼ˆè´¨é‡å·¥å•ï¼‰ï¼š
1. æ–°æœºé»„é‡‘æ³•åˆ™ï¼šè´­æœº/å®‰è£…æ—¥æœŸâ‰¤30å¤© + æ¢æœº/é€€æœº/é€€è´§
2. äº§å“é‰´å®šæ”¿ç­–ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"äº§å“é‰´å®š" + æ¢æœº/é€€æœº/é€€è´§
3. æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢ï¼šæ—§ä»¶/æ–°ä»¶åœ¨æ ¸å¿ƒéƒ¨ä»¶åº“ä¸­
4. æ»¤èŠ¯è´¨é‡ç¼ºé™·ï¼šæ»¤èŠ¯ + æ¼ç¢³/é»‘ç‚¹/é»‘æ¸£/ç¢³ç²‰
5. ä¿å¤–è½¬ä¿å†…ï¼šä¿å¤–è½¬ä¿å†… + æ»¤èŠ¯
6. å™ªéŸ³æ¢æœºï¼šå™ªéŸ³/åˆ†è´ + æ¢æœº/é€€æœº/é€€è´§

Bç±»ï¼ˆéè´¨é‡å·¥å•ï¼‰ï¼š
1. å¤–éƒ¨åŠ è£…ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"åŠ è£…"
2. å®‰å…¨ç»´æŠ¤ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"å®‰å…¨ç»´æŠ¤"
3. ç”¨æˆ·/ç¯å¢ƒè´£ä»»ï¼šå¤„ç†æ–¹æ¡ˆåŒ…å«ç”¨æˆ·/å®¢æˆ·/å°ç›†/å¨æˆ¿/ä¸‹æ°´/ç¬¬ä¸‰æ–¹/æ°´å‹/æ°´è´¨

**å¦‚æœå‘½ä¸­ç¡¬æ€§è§„åˆ™ï¼Œç«‹å³åˆ¤å®šï¼Œä¸å†ç»§ç»­ï¼**

**ç¬¬äºŒæ­¥ï¼šåº”ç”¨å­¦ä¹ çš„æ¨¡å¼ï¼ˆç¬¬äºŒå±‚ï¼‰**
å¦‚æœç¬¬ä¸€æ­¥æœªå‘½ä¸­ï¼Œåˆ™åº”ç”¨ä½ ä»è®­ç»ƒæ•°æ®ä¸­å­¦åˆ°çš„è§„åˆ™ï¼š
- å­—æ®µæƒé‡ä½“ç³»ï¼ˆæœåŠ¡é¡¹ç›®40%ã€åˆ¤å®šä¾æ®30%ã€æ•…éšœç±»åˆ«15%ã€éƒ¨ä»¶æ›´æ¢10%ã€è¾…åŠ©5%ï¼‰
- äº¤å‰éªŒè¯è§„åˆ™ï¼ˆåŠŸèƒ½æ€§æ•…éšœ vs ç»´æŠ¤è¡Œä¸ºï¼‰
- è¾¹ç•Œæƒ…å†µå¤„ç†

**ç¬¬ä¸‰æ­¥ï¼šé»˜è®¤ç­–ç•¥**
å¦‚æœä¿¡æ¯ä¸¥é‡ä¸è¶³ï¼Œé»˜è®¤ä¸º"éè´¨é‡å·¥å•"

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»åŒ…å«æ‰€æœ‰19ä¸ªå­—æ®µï¼šå·¥å•å•å·,å·¥å•æ€§è´¨,åˆ¤å®šä¾æ®,ä¿å†…ä¿å¤–,æ‰¹æ¬¡å…¥åº“æ—¥æœŸ,å®‰è£…æ—¥æœŸ,è´­æœºæ—¥æœŸ,äº§å“åç§°,å¼€å‘ä¸»ä½“,æ•…éšœéƒ¨ä½åç§°,æ•…éšœç»„,æ•…éšœç±»åˆ«,æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡,ç»´ä¿®æ–¹å¼,æ—§ä»¶åç§°,æ–°ä»¶åç§°,æ¥ç”µå†…å®¹,ç°åœºè¯Šæ–­æ•…éšœç°è±¡,å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨
- **å·¥å•æ€§è´¨**ï¼šåªèƒ½æ˜¯"è´¨é‡å·¥å•"æˆ–"éè´¨é‡å·¥å•"
- **åˆ¤å®šä¾æ®**ï¼šå¿…é¡»æ˜ç¡®è¯´æ˜ï¼š
  * å¦‚æœå‘½ä¸­ç¡¬æ€§è§„åˆ™ï¼šRule A[ç¼–å·] æˆ– Rule B[ç¼–å·]ï¼Œå…³é”®è¯ï¼š"XXX"
  * å¦‚æœåº”ç”¨å­¦ä¹ è§„åˆ™ï¼šå­—æ®µæƒé‡åˆ†æï¼Œä¸»è¦ä¾æ®ï¼š"XXX"
  * ç¤ºä¾‹ï¼š"Rule A3: æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢ï¼Œæ—§ä»¶=ä¸»æ¿ï¼Œæ–°ä»¶=ä¸»æ¿"
  * ç¤ºä¾‹ï¼š"å­—æ®µæƒé‡åˆ†æï¼ŒæœåŠ¡é¡¹ç›®=åŠ è£…å‹åŠ›æ¡¶(B1è§„åˆ™)ï¼Œåˆ¤å®šä¸ºéè´¨é‡å·¥å•"
- æ¯è¡Œå¿…é¡»ä¸¥æ ¼åŒ…å«19ä¸ªå­—æ®µ
- ä»…è¾“å‡ºCSVæ ¼å¼æ•°æ®ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š

CSVæ ¼å¼è§„èŒƒï¼š
- å¦‚æœå­—æ®µå†…å®¹åŒ…å«é€—å·(,)ã€å¼•å·(")æˆ–æ¢è¡Œç¬¦ï¼Œå¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹è¯¥å­—æ®µ
- å­—æ®µå†…çš„åŒå¼•å·éœ€è¦è½¬ä¹‰ä¸ºä¸¤ä¸ªåŒå¼•å·("")
- ä¸è¦åœ¨CSVä¸­æ’å…¥ç©ºè¡Œæˆ–åˆ†éš”çº¿

é‡è¦æé†’ï¼š
- å¿…é¡»å…ˆæ£€æŸ¥ç¡¬æ€§è§„åˆ™ï¼ˆç¬¬ä¸€å±‚ï¼‰
- ç¡¬æ€§è§„åˆ™ä¼˜å…ˆçº§æœ€é«˜
- åˆ¤å®šä¾æ®å¿…é¡»è¯¦ç»†ã€æ˜ç¡®
- ä¸è¦è¿›è¡Œä»»ä½•ä¸»è§‚æ¨æ–­æˆ–è¯­ä¹‰æ‰©å±•

ğŸš¨ **å¼ºåˆ¶è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
1. **å¿…é¡»è¾“å‡ºæ‰€æœ‰{test_row_count}æ¡è®°å½•ï¼Œä¸€æ¡éƒ½ä¸èƒ½å°‘ï¼**
2. **ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯è¡¨å¤´è¡Œï¼ˆåˆ—åï¼‰**
3. **ä»ç¬¬äºŒè¡Œå¼€å§‹æ˜¯æ•°æ®è¡Œï¼Œå…±{test_row_count}è¡Œæ•°æ®**
4. **æ€»è¾“å‡ºè¡Œæ•° = 1ï¼ˆè¡¨å¤´ï¼‰+ {test_row_count}ï¼ˆæ•°æ®ï¼‰= {test_row_count + 1}è¡Œ**
5. **ä¸è¦å› ä¸ºè®°å½•ç›¸ä¼¼å°±çœç•¥ï¼Œæ¯æ¡è®°å½•éƒ½å¿…é¡»ç‹¬ç«‹è¾“å‡º**
6. **ä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æ–‡å­—ã€æ€»ç»“æˆ–è§£é‡Šï¼Œåªè¾“å‡ºçº¯CSVæ•°æ®**


è¯·å¼€å§‹åˆ¤æ–­ï¼ˆå…±{test_row_count}æ¡è®°å½•ï¼Œå¿…é¡»å…¨éƒ¨è¾“å‡ºï¼‰ï¼š

âš ï¸ æœ€åæé†’ï¼šè¾“å‡ºå®Œæˆåï¼Œè¯·ç¡®è®¤ä½ è¾“å‡ºäº†{test_row_count + 1}è¡Œï¼ˆ1è¡Œè¡¨å¤´ + {test_row_count}è¡Œæ•°æ®ï¼‰
"""

            print(f"æç¤ºè¯é•¿åº¦: {len(quality_prompt)} å­—ç¬¦")
            print(f"æç¤ºè¯å…³é”®å†…å®¹: åº”ç”¨å­¦ä¹ åˆ°çš„è§„åˆ™è¿›è¡Œåˆ¤æ–­")
            print("-"*80)

            messages.append({"role": "user", "content": quality_prompt})

            print("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹åˆ¤æ–­å·¥å•æ€§è´¨...")
            start_time = time.time()

            # è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ¤æ–­
            resp2 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.0,  # å®Œå…¨ç¡®å®šæ€§ï¼Œæé«˜å‡†ç¡®ç‡
                max_tokens=24576  # å¢åŠ åˆ°24576ï¼Œç¡®ä¿èƒ½è¾“å‡ºå®Œæ•´ç»“æœï¼ˆåŸ16384ï¼‰
            )

            elapsed_time = time.time() - start_time

            # æå–åˆ¤æ–­ç»“æœ
            quality_result = resp2.choices[0].message.content.strip()

            # æ¸…ç†AIè¿”å›çš„ç»“æœ
            if quality_result.startswith('```csv'):
                quality_result = quality_result[6:]
            elif quality_result.startswith('```'):
                quality_result = quality_result[3:]
            if quality_result.endswith('```'):
                quality_result = quality_result[:-3]
            quality_result = quality_result.strip()
            
            # ç¡®ä¿CSVæœ‰æ­£ç¡®çš„è¡¨å¤´
            lines = quality_result.split('\n')
            if lines and not lines[0].startswith('å·¥å•å•å·'):
                print(f"âš ï¸  è­¦å‘Š: AIè¿”å›çš„CSVç¼ºå°‘è¡¨å¤´ï¼Œè‡ªåŠ¨æ·»åŠ ")
                standard_header = 'å·¥å•å•å·,å·¥å•æ€§è´¨,åˆ¤å®šä¾æ®,ä¿å†…ä¿å¤–,æ‰¹æ¬¡å…¥åº“æ—¥æœŸ,å®‰è£…æ—¥æœŸ,è´­æœºæ—¥æœŸ,äº§å“åç§°,å¼€å‘ä¸»ä½“,æ•…éšœéƒ¨ä½åç§°,æ•…éšœç»„,æ•…éšœç±»åˆ«,æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡,ç»´ä¿®æ–¹å¼,æ—§ä»¶åç§°,æ–°ä»¶åç§°,æ¥ç”µå†…å®¹,ç°åœºè¯Šæ–­æ•…éšœç°è±¡,å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨'
                quality_result = standard_header + '\n' + quality_result

            print(f"âœ… å·¥å•æ€§è´¨åˆ¤æ–­å®Œæˆ")
            print(f"è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"åˆ¤æ–­ç»“æœé•¿åº¦: {len(quality_result)} å­—ç¬¦")

            # ç»Ÿè®¡åˆ¤æ–­ç»“æœçš„è¡Œæ•°
            quality_lines = quality_result.split('\n')
            quality_row_count = len([line for line in quality_lines if line.strip()]) - 1  # å‡å»è¡¨å¤´
            print(f"åˆ¤æ–­ç»“æœè¡Œæ•°: {quality_row_count} è¡Œï¼ˆé¢„æœŸ {test_row_count} è¡Œï¼‰")

            print(f"åˆ¤æ–­ç»“æœé¢„è§ˆï¼ˆå‰3è¡Œï¼‰:")
            preview_lines = quality_lines[:4]  # è¡¨å¤´ + å‰3è¡Œæ•°æ®
            for line in preview_lines:
                print(f"  {line[:150]}{'...' if len(line) > 150 else ''}")
            print(f"Tokenä½¿ç”¨: è¾“å…¥={resp2.usage.prompt_tokens}, è¾“å‡º={resp2.usage.completion_tokens}, æ€»è®¡={resp2.usage.total_tokens}")
            print("-"*80)

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤5: æ•°æ®è¾“å‡ºä¸éªŒè¯")
            print("="*80)

            # ç»Ÿè®¡æœ€ç»ˆè¾“å‡ºçš„æ•°æ®è¡Œæ•°
            final_lines = quality_result.split('\n')
            final_row_count = len([line for line in final_lines if line.strip()]) - 1  # å‡å»è¡¨å¤´
            print(f"æœ€ç»ˆè¾“å‡ºè¡Œæ•°: {final_row_count} è¡Œ")
            
            # éªŒè¯è¾“å‡ºå®Œæ•´æ€§
            if final_row_count < test_row_count:
                missing_count = test_row_count - final_row_count
                print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºä¸å®Œæ•´ï¼ç¼ºå°‘ {missing_count} æ¡è®°å½• ({final_row_count}/{test_row_count})")
            elif final_row_count > test_row_count:
                extra_count = final_row_count - test_row_count
                print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºè¡Œæ•°è¶…å‡ºé¢„æœŸï¼å¤šå‡º {extra_count} æ¡è®°å½•")
            else:
                print(f"âœ… è¾“å‡ºå®Œæ•´æ€§éªŒè¯é€šè¿‡")
            
            print("-"*80)

            # åˆå¹¶tokenä½¿ç”¨æƒ…å†µ
            total_usage = {
                'learn_prompt_tokens': messages[0].get('usage', {}).get('prompt_tokens', 0) if len(messages) > 0 else 0,
                'learn_completion_tokens': messages[0].get('usage', {}).get('completion_tokens', 0) if len(messages) > 0 else 0,
                'judge_prompt_tokens': resp2.usage.prompt_tokens,
                'judge_completion_tokens': resp2.usage.completion_tokens,
                'total_tokens': resp2.usage.total_tokens
            }

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] å¤„ç†å®Œæˆ")
            print("="*80)
            print(f"âœ… æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ")
            print(f"è¾“å…¥æ•°æ®: {test_row_count} è¡Œ")
            print(f"è¾“å‡ºæ•°æ®: {final_row_count} è¡Œ")
            print(f"æ€»Tokenä½¿ç”¨: è¾“å…¥={resp2.usage.prompt_tokens}, è¾“å‡º={resp2.usage.completion_tokens}, æ€»è®¡={resp2.usage.total_tokens}")
            print("="*80 + "\n")

            return quality_result, total_usage

        except Exception as e:
            print(f"\n" + "="*80)
            print("âŒ é”™è¯¯: ä¸¤é˜¶æ®µæ¨ç†å¤±è´¥")
            print("="*80)
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            print("="*80 + "\n")
            raise Exception(f"ä¸¤é˜¶æ®µæ¨ç†å¤±è´¥: {str(e)}")

    def batch_process_quality_from_db(self, filename: str, training_excel: str, batch_size: int = 50) -> tuple:
        """åˆ†æ‰¹ä»æ•°æ®åº“è¯»å–æ•°æ®å¹¶è¿›è¡Œè´¨é‡å·¥å•åˆ¤æ–­
        
        Args:
            filename (str): workorder_dataè¡¨ä¸­çš„filenameå­—æ®µå€¼
            training_excel (str): è®­ç»ƒæ•°æ®Excelæ–‡ä»¶è·¯å¾„
            batch_size (int): æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°ï¼Œé»˜è®¤50æ¡
            
        Returns:
            tuple: (åˆå¹¶åçš„CSVç»“æœ, æ€»tokenä½¿ç”¨ç»Ÿè®¡, å¤„ç†çš„æ€»è®°å½•æ•°)
        """
        try:
            from flask import current_app
            from modules.auth import db
            from modules.excel.models import WorkorderData, WorkorderUselessdata1, WorkorderUselessdata2
            import tempfile
            import os
            
            print("\n" + "="*80)
            print("[åˆ†æ‰¹è´¨é‡å·¥å•æ£€æµ‹] å¼€å§‹å¤„ç†")
            print("="*80)
            print(f"æ–‡ä»¶å: {filename}")
            print(f"æ‰¹æ¬¡å¤§å°: {batch_size}æ¡/æ‰¹")
            print("-"*80)
            
            # ç¬¬ä¸€æ­¥ï¼šå­¦ä¹ è§„åˆ™ï¼ˆåªéœ€è¦æ‰§è¡Œä¸€æ¬¡ï¼‰
            print("\n[æ­¥éª¤1] å­¦ä¹ è´¨é‡åˆ¤æ–­è§„åˆ™...")
            messages, rules, usage1 = self.learn_quality_rules(training_excel)
            print(f"âœ… è§„åˆ™å­¦ä¹ å®Œæˆ")
            print("-"*80)
            
            # ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢æ€»è®°å½•æ•°
            print("\n[æ­¥éª¤2] æŸ¥è¯¢æ•°æ®åº“è®°å½•...")
            total_records = WorkorderData.query.filter_by(filename=filename).count()
            print(f"æ€»è®°å½•æ•°: {total_records}æ¡")
            
            if total_records == 0:
                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•è®°å½•")
                return "", {'strict_rules': 'n/a'}, 0
            
            # è®¡ç®—æ‰¹æ¬¡æ•°
            total_batches = (total_records + batch_size - 1) // batch_size
            print(f"æ‰¹æ¬¡æ•°: {total_batches}æ‰¹")
            print("-"*80)
            
            # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ‰¹å¤„ç†
            all_results = []
            header_line = None
            total_token_usage = {'strict_rules': 'n/a'}
            
            for batch_num in range(total_batches):
                offset = batch_num * batch_size
                limit = batch_size
                
                print(f"\n[æ‰¹æ¬¡ {batch_num + 1}/{total_batches}] å¤„ç†è®°å½• {offset + 1} è‡³ {min(offset + limit, total_records)}")
                
                # ä»æ•°æ®åº“æŸ¥è¯¢æœ¬æ‰¹æ¬¡è®°å½•
                records = WorkorderData.query.filter_by(filename=filename).offset(offset).limit(limit).all()
                
                if not records:
                    print(f"  âš ï¸  æœ¬æ‰¹æ¬¡æ— è®°å½•ï¼Œè·³è¿‡")
                    continue
                
                print(f"  æŸ¥è¯¢åˆ° {len(records)} æ¡è®°å½•")
                
                # æ„é€ 19å­—æ®µæ•°æ®
                expected_columns = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
                
                temp_data = []
                for record in records:
                    u1 = WorkorderUselessdata1.query.filter_by(filename=filename, workAlone=record.workAlone).first()
                    u2 = WorkorderUselessdata2.query.filter_by(filename=filename, workAlone=record.workAlone).first()
                    
                    def norm(v):
                        return '' if v is None or v == 'None' or (isinstance(v, float) and pd.isna(v)) else str(v)
                    
                    row_data = {
                        'å·¥å•å•å·': norm(record.workAlone),
                        'å·¥å•æ€§è´¨': norm(record.workOrderNature),
                        'åˆ¤å®šä¾æ®': norm(record.judgmentBasis),
                        'ä¿å†…ä¿å¤–': norm(u1.internalExternalInsurance if u1 else ''),
                        'æ‰¹æ¬¡å…¥åº“æ—¥æœŸ': norm(u1.batchWarehousingDate if u1 else ''),
                        'å®‰è£…æ—¥æœŸ': norm(u1.installDate if u1 else ''),
                        'è´­æœºæ—¥æœŸ': norm(u1.purchaseDate if u1 else ''),
                        'äº§å“åç§°': norm(u1.productName if u1 else ''),
                        'å¼€å‘ä¸»ä½“': norm(u1.developmentSubject if u1 else ''),
                        'æ•…éšœéƒ¨ä½åç§°': norm(record.replacementPartName),
                        'æ•…éšœç»„': norm(record.faultGroup),
                        'æ•…éšœç±»åˆ«': norm(record.faultClassification),
                        'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡': norm(record.faultPhenomenon),
                        'ç»´ä¿®æ–¹å¼': norm(u2.maintenanceMode if u2 else ''),
                        'æ—§ä»¶åç§°': norm(u2.oldPartName if u2 else ''),
                        'æ–°ä»¶åç§°': norm(u2.newPartName if u2 else ''),
                        'æ¥ç”µå†…å®¹': norm(record.callContent),
                        'ç°åœºè¯Šæ–­æ•…éšœç°è±¡': norm(record.onsiteFaultPhenomenon),
                        'å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨': norm(record.remarks),
                    }
                    temp_data.append({k: row_data.get(k, '') for k in expected_columns})
                
                df_batch = pd.DataFrame(temp_data, columns=expected_columns)
                print(f"  æ„é€ æ•°æ®: {len(df_batch)}è¡Œ x {len(df_batch.columns)}åˆ—")
                
                # åˆ›å»ºä¸´æ—¶Excelæ–‡ä»¶
                with tempfile.NamedTemporaryFile(mode='wb', suffix='.xlsx', delete=False) as tmp:
                    temp_excel_path = tmp.name
                    df_batch.to_excel(temp_excel_path, index=False)
                    print(f"  ä¸´æ—¶æ–‡ä»¶: {os.path.basename(temp_excel_path)}")
                
                try:
                    # è°ƒç”¨AIåˆ¤æ–­ï¼ˆä½¿ç”¨å·²å­¦ä¹ çš„è§„åˆ™ï¼‰
                    print(f"  å¼€å§‹AIåˆ¤æ–­...")
                    batch_result, batch_usage = self.apply_quality_rules(messages, temp_excel_path)
                    print(f"  âœ… AIåˆ¤æ–­å®Œæˆ")
                    
                    # è§£æç»“æœ
                    result_lines = batch_result.strip().split('\n')
                    
                    # ä¿å­˜è¡¨å¤´ï¼ˆç¬¬ä¸€æ‰¹æ—¶ï¼‰
                    if header_line is None and len(result_lines) > 0:
                        header_line = result_lines[0]
                        print(f"  è¡¨å¤´: {header_line[:100]}...")
                    
                    # æ·»åŠ æ•°æ®è¡Œï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
                    data_lines = result_lines[1:] if len(result_lines) > 1 else []
                    all_results.extend(data_lines)
                    print(f"  ç»“æœè¡Œæ•°: {len(data_lines)}è¡Œ")
                    
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_excel_path):
                        os.remove(temp_excel_path)
                        print(f"  ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                
                print(f"  æ‰¹æ¬¡å¤„ç†å®Œæˆ ({len(all_results)}è¡Œå·²ç´¯ç§¯)")
            
            # ç¬¬å››æ­¥ï¼šåˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
            print("\n" + "="*80)
            print("[æ­¥éª¤3] åˆå¹¶æ‰¹æ¬¡ç»“æœ")
            print("="*80)
            
            if header_line is None:
                print("âŒ é”™è¯¯: æœªè·å–åˆ°è¡¨å¤´")
                return "", total_token_usage, 0
            
            # ç»„è£…å®Œæ•´CSV
            final_csv = header_line + '\n' + '\n'.join(all_results)
            final_row_count = len(all_results)
            
            print(f"âœ… åˆå¹¶å®Œæˆ")
            print(f"æ€»æ•°æ®è¡Œ: {final_row_count}è¡Œ")
            print(f"é¢„æœŸè®°å½•: {total_records}æ¡")
            
            if final_row_count != total_records:
                print(f"âš ï¸  è­¦å‘Š: ç»“æœè¡Œæ•°({final_row_count})ä¸è®°å½•æ•°({total_records})ä¸ä¸€è‡´")
            
            print("="*80 + "\n")
            
            return final_csv, total_token_usage, total_records
            
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: åˆ†æ‰¹å¤„ç†å¤±è´¥")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            raise Exception(f"åˆ†æ‰¹å¤„ç†å¤±è´¥: {str(e)}")

    def _check_empty_quality(self, csv_content):
        """
        æ£€æŸ¥å¹¶è®°å½•å·¥å•æ€§è´¨ä¸ºç©ºçš„è¡Œï¼Œä½†ä¸è¿›è¡Œè‡ªåŠ¨å¡«å……
        è®©AIæ¨¡å‹è‡ªå·±å­¦ä¹ åˆ¤æ–­ï¼Œè€Œä¸æ˜¯ä¾èµ–ç®€å•çš„å…³é”®è¯è§„åˆ™

        Args:
            csv_content: CSVæ ¼å¼çš„å­—ç¬¦ä¸²

        Returns:
            åŸå§‹CSVå­—ç¬¦ä¸²ï¼ˆä»…è®°å½•é—®é¢˜ï¼Œä¸ä¿®æ”¹ï¼‰
        """
        import io
        import csv

        lines = csv_content.strip().split('\n')
        if len(lines) <= 1:
            return csv_content

        # è§£æCSVæ£€æŸ¥ç©ºå€¼
        reader = csv.reader(io.StringIO(csv_content))
        header = next(reader)

        # æ‰¾åˆ°å·¥å•æ€§è´¨åˆ—çš„ç´¢å¼•
        try:
            quality_index = header.index('å·¥å•æ€§è´¨')
        except ValueError:
            return csv_content

        # ç»Ÿè®¡ç©ºå€¼æƒ…å†µ
        empty_count = 0
        total_count = 0
        empty_rows = []

        for idx, row in enumerate(reader, start=2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
            total_count += 1
            if len(row) > quality_index:
                quality_value = row[quality_index].strip()
                if not quality_value or quality_value.lower() in ['', 'nan', 'null', 'none']:
                    empty_count += 1
                    empty_rows.append(idx)

        # è¾“å‡ºç©ºå€¼æ£€æŸ¥ç»“æœ
        if empty_count > 0:
            print(f"âš ï¸  è­¦å‘Šï¼šæ£€æµ‹åˆ° {empty_count}/{total_count} è¡Œçš„'å·¥å•æ€§è´¨'ä¸ºç©º")
            if len(empty_rows) <= 10:
                print(f"   ç©ºå€¼è¡Œå·: {', '.join(map(str, empty_rows))}")
            else:
                print(f"   ç©ºå€¼è¡Œå·ï¼ˆå‰10ä¸ªï¼‰: {', '.join(map(str, empty_rows[:10]))}...")
            print(f"   å»ºè®®ï¼šæ£€æŸ¥AIæ¨¡å‹è¾“å‡ºï¼Œæˆ–è°ƒæ•´æç¤ºè¯ä»¥æé«˜å®Œæ•´æ€§")
        else:
            print(f"âœ… æ‰€æœ‰ {total_count} è¡Œçš„'å·¥å•æ€§è´¨'å‡å·²å¡«å†™")

        return csv_content

    def _enrich_non_quality_basis(self, csv_content: str) -> str:
        import io
        import csv
        import logging
        import os

        logger = logging.getLogger('quality_basis')
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            os.makedirs('logs', exist_ok=True)
            fh = logging.FileHandler('logs/quality_basis.log', encoding='utf-8')
            fmt = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
            fh.setFormatter(fmt)
            logger.addHandler(fh)

        lines = csv_content.strip().split('\n')
        if len(lines) <= 1:
            return csv_content

        reader = csv.reader(io.StringIO(csv_content))
        header = next(reader)

        try:
            idx_order = header.index('å·¥å•å•å·')
            idx_basis = header.index('åˆ¤å®šä¾æ®')
            idx_nature = header.index('å·¥å•æ€§è´¨')
            idx_old = header.index('æ—§ä»¶åç§°') if 'æ—§ä»¶åç§°' in header else None
            idx_new = header.index('æ–°ä»¶åç§°') if 'æ–°ä»¶åç§°' in header else None
        except ValueError:
            return csv_content

        col_map = {name: i for i, name in enumerate(header)}
        def get(row, name):
            i = col_map.get(name, -1)
            return (row[i].strip() if i >= 0 and len(row) > i else '')

        rows = []
        updated = 0
        total = 0

        def well_structured(text: str) -> bool:
            t = (text or '').strip()
            if not t:
                return False
            keys = ['æ’é™¤', 'å› ç´ ', 'æ¡ˆä¾‹', 'ç½®ä¿¡åº¦', 'åŸå› ', 'ä¾æ®']
            return sum(1 for k in keys if k in t) >= 2

        def build(row):
            svc = get(row, 'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡')
            call = get(row, 'æ¥ç”µå†…å®¹')
            diag = get(row, 'ç°åœºè¯Šæ–­æ•…éšœç°è±¡')
            plan = get(row, 'å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨')
            oldp = (row[idx_old].strip() if idx_old is not None and len(row) > idx_old else '')
            newp = (row[idx_new].strip() if idx_new is not None and len(row) > idx_new else '')

            factors = []
            if svc:
                factors.append(f"æœåŠ¡/ç°è±¡: {svc}")
            if call:
                factors.append(f"æ¥ç”µ: {call}")
            if diag:
                factors.append(f"è¯Šæ–­: {diag}")
            if plan:
                factors.append(f"æ–¹æ¡ˆ: {plan}")
            if oldp and newp and oldp != newp:
                factors.append(f"æ–°æ—§ä»¶æ›´æ›¿: {oldp} â†’ {newp}")

            excluded = ['äº§å“åˆ¶é€ /è®¾è®¡/æ¥æ–™ç¼ºé™·', 'é›¶éƒ¨ä»¶å›ºæœ‰è´¨é‡ä¸è¾¾æ ‡']
            cases = ['å‚è€ƒè®­ç»ƒæ¡ˆä¾‹ï¼šå®‰è£…/è°ƒè¯•/ç»´æŠ¤/è€—ææ›´æ¢ç±»æ ·æœ¬çš„æ ‡æ³¨å€¾å‘']

            conf = 65
            if oldp and newp and oldp != newp:
                conf = max(conf, 85)

            proof_items = []
            text_all = ' '.join([svc or '', call or '', diag or '', plan or '']).lower()
            if any(k in text_all for k in ['å®¢æˆ·', 'å®šåˆ¶', 'éœ€æ±‚', 'è¦æ±‚']):
                proof_items.append('å®¢æˆ·ç‰¹æ®Šè¦æ±‚å•/é‚®ä»¶/æ²Ÿé€šè®°å½•')
            if any(k in text_all for k in ['è§„æ ¼', 'å‚æ•°', 'å˜æ›´', 'å‡çº§']):
                proof_items.append('æŠ€æœ¯è§„æ ¼å˜æ›´è®°å½•/é…ç½®å˜æ›´å•')
            if any(k in text_all for k in ['å®‰è£…', 'è°ƒè¯•', 'æ”¹è£…', 'åŠ è£…']):
                proof_items.append('å®‰è£…/è°ƒè¯•è®°å½•æˆ–å·¥å•è¯´æ˜')
            if any(k in text_all for k in ['ç°åœº', 'æ£€æµ‹', 'è¯Šæ–­', 'æŠ¥å‘Š']):
                proof_items.append('ç°åœºè¯Šæ–­/æ£€æµ‹æŠ¥å‘Š')
            if not proof_items:
                proof_items = ['å®¢æˆ·éœ€æ±‚æ–‡æ¡£', 'æŠ€æœ¯è§„æ ¼å˜æ›´è®°å½•', 'å®‰è£…/è°ƒè¯•è®°å½•', 'ç°åœºè¯Šæ–­æŠ¥å‘Š']

            part1 = 'å…·ä½“æ’é™¤çš„è´¨é‡é—®é¢˜ç±»å‹: ' + 'ï¼›'.join(excluded)
            part2 = 'å…³é”®åˆ¤æ–­å› ç´ åˆ†æ: ' + ('ï¼›'.join(factors) if factors else 'ä¿¡æ¯æŒ‡å‘æœåŠ¡/ç¯å¢ƒ/ç”¨æˆ·ä½¿ç”¨å› ç´ ')
            part3 = 'ç›¸å…³è®­ç»ƒæ¡ˆä¾‹å‚è€ƒ: ' + ('ï¼›'.join(cases))
            part4 = f'ç½®ä¿¡åº¦è¯„åˆ†: {conf}%'
            part5 = 'è¯æ˜ææ–™æˆ–è¯´æ˜æ–‡æ¡£: ' + 'ï¼›'.join(proof_items)
            return '\n'.join([part1, part2, part3, part4, part5])

        for row in reader:
            if not row or all((c or '').strip() == '' for c in row):
                continue
            nature = (row[idx_nature].strip() if len(row) > idx_nature else '')
            if nature == 'éè´¨é‡å·¥å•':
                total += 1
                basis = (row[idx_basis].strip() if len(row) > idx_basis else '')
                if not well_structured(basis):
                    nb = build(row)
                    if len(row) <= idx_basis:
                        row.extend([''] * (idx_basis - len(row) + 1))
                    row[idx_basis] = nb
                    updated += 1
                    order_no = row[idx_order] if len(row) > idx_order else ''
                    logger.info(f"basis_generated order={order_no} chars={len(nb)}")
            rows.append(row)

        output = io.StringIO()
        w = csv.writer(output, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header)
        for r in rows:
            w.writerow(r)

        if total:
            logger.info(f"basis_summary non_quality={total} updated={updated}")

        return output.getvalue()

    def _strip_unexpected_header_rows(self, csv_text: str, expected_header: str) -> str:
        import io
        import csv
        reader = csv.reader(io.StringIO(csv_text))
        rows = list(reader)
        if not rows:
            return csv_text
        cleaned = []
        header_written = False
        for r in rows:
            line = ','.join(r).strip()
            if line == expected_header:
                if not header_written:
                    cleaned.append(r)
                    header_written = True
                continue
            if line.startswith('ç¼–å·(ç»´ä¿®è¡Œ)'):
                continue
            cleaned.append(r)
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        for r in cleaned:
            w.writerow(r)
        return out.getvalue()

    def _apply_strict_rules(self, df: pd.DataFrame) -> pd.DataFrame:
        import pandas as pd
        core_parts = set([
            'ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ'
        ])
        exchange_words = ['æ¢æœº','é€€æœº','é€€è´§']
        filter_words = ['æ¼ç‚­','é»‘ç‚¹','é»‘æ¸£','ç¢³ç²‰','æ´»æ€§ç‚­ç²‰æœ«']
        env_keywords = ['ç”¨æˆ·','å®¢æˆ·','å°ç›†','å¨æˆ¿','ä¸‹æ°´','ç¬¬ä¸‰æ–¹','æ°´å‹','æ°´è´¨']
        env_causes = ['é—®é¢˜','æ¸—æ°´','åæ°´','ç®¡é“','å µ','æ¼']
        maintenance_actions = ['é‡å¯','å¤ä½','è°ƒè¯•','æ¸…æ´—','ç´§å›º','é‡æ–°å®‰è£…','åŠ å›º','æŒ‡å¯¼','è§£é‡Š','æ£€æŸ¥æ­£å¸¸','æ— å¼‚å¸¸','ä¸€åˆ‡æ­£å¸¸']
        fault_keywords = ['æ¼æ°´','ä¸é€šç”µ','ä¸åˆ¶æ°´','ä¸å‡ºæ°´','ä¸åŠ çƒ­','å™ªéŸ³å¤§','æ˜¾ç¤ºå¼‚å¸¸','E1','E3','E6']

        def contains_any(text, keys):
            t = str(text or '')
            return any(k in t for k in keys)

        def within_30_days(purchase, install):
            today = pd.Timestamp.today()
            days = []
            for s in [purchase, install]:
                dt = pd.to_datetime(str(s or ''), errors='coerce')
                if pd.notna(dt):
                    days.append((today - dt).days)
            return len(days) > 0 and min(days) <= 30

        def is_core_part(oldp, newp):
            t1 = str(oldp or '')
            t2 = str(newp or '')
            return any(p in t1 for p in core_parts) or any(p in t2 for p in core_parts)

        def is_filter(oldp, newp):
            t1 = str(oldp or '')
            t2 = str(newp or '')
            return ('æ»¤èŠ¯' in t1) or ('æ»¤èŠ¯' in t2)

        out_rows = []
        for _, row in df.iterrows():
            svc = str(row.get('æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', '') or '')
            plan = str(row.get('å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨', '') or '')
            call = str(row.get('æ¥ç”µå†…å®¹', '') or '')
            diag = str(row.get('ç°åœºè¯Šæ–­æ•…éšœç°è±¡', '') or '')
            bn = str(row.get('ä¿å†…ä¿å¤–', '') or '')
            oldp = str(row.get('æ—§ä»¶åç§°', '') or '')
            newp = str(row.get('æ–°ä»¶åç§°', '') or '')
            purchase = str(row.get('è´­æœºæ—¥æœŸ', '') or '')
            install = str(row.get('å®‰è£…æ—¥æœŸ', '') or '')

            nature = ''
            basis = ''

            if within_30_days(purchase, install) and (contains_any(plan, exchange_words) or contains_any(call, exchange_words)):
                hits = ','.join([w for w in exchange_words if w in plan or w in call])
                nature, basis = 'è´¨é‡å·¥å•', f'Rule A.1: æ–°æœºé»„é‡‘æ³•åˆ™ï¼Œå‘½ä¸­å…³é”®è¯ï¼šâ€œ{hits}â€'
            elif ((('äº§å“é‰´å®š' in svc) or ('åªæ¢ä¸ä¿®æ”¿ç­–äº§å“è´¨é‡é‰´å®š' in svc)) and (contains_any(plan, exchange_words) or contains_any(call, exchange_words))):
                hits = ','.join([w for w in exchange_words if w in plan or w in call])
                nature, basis = 'è´¨é‡å·¥å•', f'Rule A.2: åªæ¢ä¸ä¿®/é‰´å®šæ”¿ç­–ï¼Œå‘½ä¸­å…³é”®è¯ï¼šâ€œ{hits}â€'
            elif (oldp or newp) and is_core_part(oldp, newp):
                nature, basis = 'è´¨é‡å·¥å•', 'Rule A.3: æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢'
            elif is_filter(oldp, newp) and contains_any(call + diag, filter_words):
                hits = ','.join([w for w in filter_words if w in (call + diag)])
                nature, basis = 'è´¨é‡å·¥å•', f'Rule A.4: æ»¤èŠ¯è´¨é‡ç¼ºé™·ï¼Œå‘½ä¸­å…³é”®è¯ï¼šâ€œ{hits}â€'
            elif bn == 'ä¿å¤–è½¬ä¿å†…' and is_filter(oldp, newp):
                nature, basis = 'è´¨é‡å·¥å•', 'Rule A.5: ç‰¹æ®Šæ”¿ç­–è§¦å‘ï¼ˆä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯ï¼‰'
            elif (('å™ªéŸ³' in plan) or ('å™ªéŸ³' in call) or ('åˆ†è´' in plan) or ('åˆ†è´' in call)) and (contains_any(plan, exchange_words) or contains_any(call, exchange_words)):
                nature, basis = 'è´¨é‡å·¥å•', 'Rule A.6: ä¸»è§‚æ€§èƒ½ç¼ºé™·ï¼ˆå™ªéŸ³/åˆ†è´+æ¢é€€ï¼‰'
            elif 'åŠ è£…' in svc:
                nature, basis = 'éè´¨é‡å·¥å•', 'Rule B.1: å¤–éƒ¨åŠ è£…'
            elif 'å®‰å…¨ç»´æŠ¤' in svc:
                nature, basis = 'éè´¨é‡å·¥å•', 'Rule B.2: å®‰å…¨ç»´æŠ¤'
            elif contains_any(plan, env_keywords) and contains_any(plan, env_causes):
                nature, basis = 'éè´¨é‡å·¥å•', 'Rule B.3: ç”¨æˆ·/ç¯å¢ƒè´£ä»»'
            else:
                if plan.strip() in ['ä¸Šé—¨ç»´ä¿®', 'ä¸Šé—¨æ£€æŸ¥']:
                    pass
                f = contains_any(call + diag, fault_keywords)
                m = contains_any(plan, maintenance_actions)
                if f and m:
                    nature, basis = 'éè´¨é‡å·¥å•', 'Rule 8.3: æœ‰æ•…éšœä½†ç»´æŠ¤/å¤ä½è§£å†³'
                elif f and not m:
                    nature, basis = 'è´¨é‡å·¥å•', 'Rule 8.3: æœ‰æ•…éšœä¸”éç®€å•ç»´æŠ¤'
                elif (not f) and m:
                    nature, basis = 'éè´¨é‡å·¥å•', 'Rule 8.3: æ— æ•…éšœä»…ç»´æŠ¤/è§£é‡Š'
                else:
                    nature, basis = 'éè´¨é‡å·¥å•', 'Rule 3: æœ€ç»ˆå®‰å…¨ç½‘'

            row_out = row.copy()
            row_out['å·¥å•æ€§è´¨'] = nature
            row_out['åˆ¤å®šä¾æ®'] = basis
            out_rows.append(row_out)

        return pd.DataFrame(out_rows)
    def _fix_quality_column_position(self, csv_text: str) -> str:
        import io
        import csv
        reader = csv.reader(io.StringIO(csv_text))
        header = next(reader, None)
        if not header:
            return csv_text
        try:
            qidx = header.index('å·¥å•æ€§è´¨')
        except ValueError:
            return csv_text
        rows = []
        for r in reader:
            if len(r) == len(header):
                if (not r[qidx]) and any(val in ['è´¨é‡å·¥å•', 'éè´¨é‡å·¥å•'] for val in r):
                    for j, val in enumerate(r):
                        if val in ['è´¨é‡å·¥å•', 'éè´¨é‡å·¥å•'] and j != qidx:
                            r[qidx] = val
                            r[j] = ''
                            break
            rows.append(r)
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header)
        for r in rows:
            w.writerow(r)
        return out.getvalue()

    def _ensure_order_numbers(self, csv_text: str, df_test) -> str:
        import io
        import csv
        reader = csv.reader(io.StringIO(csv_text))
        header = next(reader, None)
        if not header:
            return csv_text
        try:
            oidx = header.index('å·¥å•å•å·')
        except ValueError:
            return csv_text
        orders = list(df_test['å·¥å•å•å·']) if 'å·¥å•å•å·' in df_test.columns else list(range(1, len(df_test)+1))
        rows = []
        i = 0
        for r in reader:
            if len(r) == len(header):
                if (not r[oidx]) and i < len(orders):
                    r[oidx] = str(orders[i])
                i += 1
            rows.append(r)
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header)
        for r in rows:
            w.writerow(r)
        return out.getvalue()

    def _double_validate_replacement(self, csv_content: str) -> str:
        import io
        import csv
        import logging
        import os

        logger = logging.getLogger('quality_double_check')
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            os.makedirs('logs', exist_ok=True)
            fh = logging.FileHandler('logs/quality_double_check.log', encoding='utf-8')
            fmt = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
            fh.setFormatter(fmt)
            logger.addHandler(fh)

        reader = csv.reader(io.StringIO(csv_content))
        header = next(reader)
        if 'æ—§ä»¶åç§°' not in header or 'æ–°ä»¶åç§°' not in header:
            return csv_content
        idx_order = header.index('å·¥å•å•å·') if 'å·¥å•å•å·' in header else None
        idx_old = header.index('æ—§ä»¶åç§°')
        idx_new = header.index('æ–°ä»¶åç§°')

        for row in reader:
            oldp = row[idx_old].strip() if len(row) > idx_old else ''
            newp = row[idx_new].strip() if len(row) > idx_new else ''
            if oldp and newp and oldp != newp:
                order_no = row[idx_order] if idx_order is not None and len(row) > idx_order else ''
                logger.info(f"replacement_detected order={order_no} old={oldp} new={newp}")

        return csv_content

    def _force_realign_columns(self, csv_text: str, expected_header: str, expected_count: int) -> str:
        import io
        import csv
        lines = [l for l in csv_text.strip().split('\n') if l.strip()]
        if not lines:
            return csv_text
        header_line = lines[0].strip()
        if header_line != expected_header:
            header_line = expected_header
        def clean_token(t):
            s = t.strip().strip('\r').strip()
            if s.lower() in ['nan', 'null', 'none']:
                return ''
            return s
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header_line.split(','))
        for raw in lines[1:]:
            if not raw.strip():
                continue
            tokens = [clean_token(x) for x in raw.split(',')]
            if len(tokens) == expected_count:
                w.writerow(tokens)
                continue
            last_idx = None
            for i in range(len(tokens)-1, -1, -1):
                v = tokens[i].strip()
                if v in ['è´¨é‡å·¥å•', 'éè´¨é‡å·¥å•']:
                    last_idx = i
                    break
            if last_idx is None:
                while len(tokens) < expected_count:
                    tokens.append('')
                if len(tokens) > expected_count:
                    tokens = tokens[:expected_count]
                w.writerow(tokens)
                continue
            last_field = tokens[last_idx]
            middle = tokens[:last_idx]
            if not middle:
                middle = ['']
            while len(middle) < expected_count - 1:
                middle.append('')
            if len(middle) > expected_count - 1:
                pos_list = [1, 2, 5, 9, 10, 11]
                target = expected_count - 1
                cur = len(middle)
                for pos in pos_list:
                    while cur > target and pos < len(middle)-1:
                        middle[pos] = f"{middle[pos]},{middle[pos+1]}"
                        middle.pop(pos+1)
                        cur -= 1
                while len(middle) > target:
                    k = min(len(middle)-2, target-1)
                    middle[k] = f"{middle[k]},{middle[k+1]}"
                    middle.pop(k+1)
            row = middle[:expected_count-1] + [last_field]
            w.writerow(row)
        return out.getvalue()

