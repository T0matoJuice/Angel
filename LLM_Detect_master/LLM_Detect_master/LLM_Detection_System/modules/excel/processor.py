#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelå·¥å•æ™ºèƒ½å¤„ç†å™¨æ¨¡å— - æä¾›åŸºäºç¡…åŸºæµåŠ¨å¤§æ¨¡å‹çš„å·¥å•æ•°æ®æ™ºèƒ½å¡«å……å’Œåˆ†æåŠŸèƒ½
"""
import json
import os
import pandas as pd
from pathlib import Path
from openai import OpenAI
from modules.common.prompts import load_prompt
from threading import Lock
import tempfile
import traceback
import threading
from typing import Tuple, List, Dict, Optional
import copy
import time
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
# å…¨å±€é”ï¼šç”¨äºæ—¥å¿—æ‰“å°å’Œè¡¨å¤´å†™å…¥ï¼ˆé¿å…å¤šçº¿ç¨‹æ··ä¹±ï¼‰
print_lock = Lock()
header_lock = Lock()

class Processor:
    """Excelå·¥å•æ™ºèƒ½å¡«å……å¤„ç†å™¨

    ä½¿ç”¨ç¡…åŸºæµåŠ¨å¤§æ¨¡å‹è¿›è¡Œå·¥å•æ•°æ®çš„æ™ºèƒ½å­¦ä¹ å’Œæ¨ç†å¡«å……ï¼Œæ”¯æŒä¸¤é˜¶æ®µå¤„ç†ï¼š
    1. ä»è®­ç»ƒæ•°æ®å­¦ä¹ è§„åˆ™å’Œæ¨¡å¼
    2. å°†å­¦åˆ°çš„è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•æ•°æ®è¿›è¡Œæ™ºèƒ½å¡«å……
    """

    def __init__(self):
        """åˆå§‹åŒ–å¤§æ¨¡å‹å¤„ç†å™¨

        é…ç½®Kimiå®˜æ–¹APIå®¢æˆ·ç«¯å’Œç›¸å…³å‚æ•°ï¼Œå‡†å¤‡è¿›è¡Œæ™ºèƒ½å¤„ç†
        """
        
        # è·å–ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥å’Œæ¨¡å‹
        # ä½¿ç”¨Kimiå®˜æ–¹APIé…ç½®ï¼ˆæ›¿ä»£ç¡…åŸºæµåŠ¨ï¼‰
        # æœ¬åœ°å¤§æ¨¡å‹é…ç½®ï¼ˆå·²åœç”¨ï¼‰:
        # self.api_key = 'Angel@20250428'
        # self.model = 'Qwen3-80B-FP8'
        # self.base_url = 'http://10.2.32.163:8000/v1'
        # self.api_key = os.getenv('EXCEL_API_KEY', 'Angel@20250428')
        # self.model = os.getenv('EXCEL_MODEL_NAME', 'Qwen3-80B-FP8')
        # self.base_url = os.getenv('EXCEL_BASE_URL', 'http://10.2.32.163:8000/v1')
        
        # Kimiå®˜æ–¹APIé…ç½®ï¼ˆMoonshotï¼‰
        # self.api_key = os.getenv('MOONSHOT_API_KEY', 'sk-IJmn6jASTNLPTyGtP3ShBJj9YjUc8EHFereXUZBi265sHiQG')
        # self.model = os.getenv('MOONSHOT_MODEL_0711', 'kimi-k2-0905-preview')  # ä½¿ç”¨K2-0711æ¨¡å‹
        # self.base_url = os.getenv('MOONSHOT_BASE_URL', 'https://api.moonshot.cn/v1')
        self.api_key = 'sk-IJmn6jASTNLPTyGtP3ShBJj9YjUc8EHFereXUZBi265sHiQG'
        self.model = 'kimi-k2-turbo-preview'  # æˆ–è€…ä½ æƒ³ç”¨çš„æ¨¡å‹åç§°
        self.base_url = 'https://api.moonshot.cn/v1'
        self._thread_local = threading.local()

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°MOONSHOT_API_KEYç¯å¢ƒå˜é‡")

        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¼å®¹OpenAIæ¥å£æ ¼å¼ï¼‰
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=600  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°10åˆ†é’Ÿï¼Œé€‚åº”å¤§æ‰¹é‡æ•°æ®å¤„ç†
        )

    def _read_excel_to_text(self, excel_path: str) -> str:
        """å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œç”¨äºAIåˆ†æ

        Args:
            excel_path (str): Excelæ–‡ä»¶è·¯å¾„

        Returns:
            str: Excelå†…å®¹çš„æ–‡æœ¬è¡¨ç¤º
        """
        try:
            df = pd.read_excel(excel_path, dtype=str)

            # æ¸…ç†æ‰€æœ‰å­—æ®µä¸­çš„æ¢è¡Œç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼
            # è¿™æ ·å¯ä»¥é¿å…AIç”Ÿæˆçš„CSVæ ¼å¼å‡ºç°é—®é¢˜
            for col in df.columns:
                df[col] = df[col].astype(str).str.replace('\r\n', ' ', regex=False)
                df[col] = df[col].astype(str).str.replace('\n', ' ', regex=False)
                df[col] = df[col].astype(str).str.replace('\r', ' ', regex=False)

            # å°†DataFrameè½¬æ¢ä¸ºCSVæ ¼å¼æ–‡æœ¬
            return df.to_csv(index=False)
        except Exception as e:
            raise Exception(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")

    def _fix_csv_format(self, csv_text: str) -> str:
        """ä¿®å¤CSVæ ¼å¼ï¼Œç¡®ä¿åŒ…å«é€—å·ã€æ¢è¡Œç¬¦çš„å­—æ®µè¢«æ­£ç¡®å¼•å·åŒ…è£¹

        Args:
            csv_text (str): åŸå§‹CSVæ–‡æœ¬

        Returns:
            str: ä¿®å¤åçš„CSVæ–‡æœ¬
        """
        try:
            import io
            import csv

            # ä½¿ç”¨StringIOè¯»å–æ•´ä¸ªCSVæ–‡æœ¬ï¼ˆæ”¯æŒå¤šè¡Œå­—æ®µï¼‰
            input_stream = io.StringIO(csv_text.strip())
            output_stream = io.StringIO()

            # ä½¿ç”¨csv.readerè¯»å–ï¼ˆè‡ªåŠ¨å¤„ç†å¼•å·å’Œæ¢è¡Œç¬¦ï¼‰
            reader = csv.reader(input_stream)
            # ä½¿ç”¨csv.writerå†™å…¥ï¼ˆè‡ªåŠ¨æ·»åŠ å¿…è¦çš„å¼•å·ï¼‰
            writer = csv.writer(output_stream, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')

            for row in reader:
                # è·³è¿‡ç©ºè¡Œ
                if not row or all(field.strip() == '' for field in row):
                    continue
                writer.writerow(row)

            return output_stream.getvalue()
        except Exception as e:
            # å¦‚æœä¿®å¤å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            print(f"è­¦å‘Šï¼šCSVæ ¼å¼ä¿®å¤å¤±è´¥: {str(e)}")
            return csv_text

    def _validate_and_fix_csv_fields(self, csv_text: str, expected_field_count: int = 9) -> str:
        """éªŒè¯å¹¶ä¿®å¤CSVå­—æ®µæ•°é‡

        Args:
            csv_text (str): CSVæ–‡æœ¬
            expected_field_count (int): æœŸæœ›çš„å­—æ®µæ•°é‡

        Returns:
            str: ä¿®å¤åçš„CSVæ–‡æœ¬
        """
        try:
            import io
            import csv

            lines = csv_text.strip().split('\n')
            if not lines:
                return csv_text

            # è¯»å–å¤´éƒ¨
            header = lines[0]
            header_fields = header.split(',')

            # å¦‚æœå¤´éƒ¨å­—æ®µæ•°é‡ä¸å¯¹ï¼Œç›´æ¥è¿”å›
            if len(header_fields) != expected_field_count:
                print(f"è­¦å‘Šï¼šCSVå¤´éƒ¨å­—æ®µæ•°é‡ä¸æ­£ç¡®ï¼ŒæœŸæœ›{expected_field_count}ä¸ªï¼Œå®é™…{len(header_fields)}ä¸ª")
                return csv_text

            # ä½¿ç”¨csv.readerè§£ææ¯ä¸€è¡Œ
            input_stream = io.StringIO(csv_text.strip())
            reader = csv.reader(input_stream)

            output_stream = io.StringIO()
            writer = csv.writer(output_stream, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')

            row_num = 0
            for row in reader:
                row_num += 1

                # è·³è¿‡ç©ºè¡Œ
                if not row or all(field.strip() == '' for field in row):
                    continue

                # æ£€æŸ¥å­—æ®µæ•°é‡
                if len(row) != expected_field_count:
                    print(f"è­¦å‘Šï¼šç¬¬{row_num}è¡Œå­—æ®µæ•°é‡ä¸æ­£ç¡®ï¼ŒæœŸæœ›{expected_field_count}ä¸ªï¼Œå®é™…{len(row)}ä¸ª")
                    print(f"  åŸå§‹æ•°æ®: {row[:3]}... (å…±{len(row)}ä¸ªå­—æ®µ)")

                    # å¦‚æœå­—æ®µå¤ªå°‘ï¼Œç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½
                    if len(row) < expected_field_count:
                        row.extend([''] * (expected_field_count - len(row)))
                        print(f"  å·²è¡¥é½ä¸º{expected_field_count}ä¸ªå­—æ®µ")
                    # å¦‚æœå­—æ®µå¤ªå¤šï¼Œæ™ºèƒ½å¤„ç†
                    elif len(row) > expected_field_count:
                        # å¦‚æœæ˜¯10ä¸ªå­—æ®µï¼Œå¾ˆå¯èƒ½æ˜¯æ¼æ‰äº†"æ•…éšœç»„"å­—æ®µ
                        # å°è¯•ä¿ç•™æœ€åä¸€ä¸ªå­—æ®µï¼ˆå·¥å•æ€§è´¨ï¼‰ï¼Œåˆ é™¤ä¸­é—´çš„æŸä¸ªå­—æ®µ
                        if len(row) == 10:
                            # ä¿ç•™æœ€åä¸€ä¸ªå­—æ®µï¼ˆå·¥å•æ€§è´¨ï¼‰
                            last_field = row[-1]
                            # åˆ é™¤ç¬¬3ä¸ªå­—æ®µï¼ˆç´¢å¼•2ï¼‰ï¼Œå› ä¸ºå¾ˆå¯èƒ½æ˜¯"æ•…éšœç»„"è¢«æ¼æ‰ï¼Œå¯¼è‡´åé¢å­—æ®µé”™ä½
                            # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬ä¿ç•™å‰2ä¸ªå’Œæœ€å1ä¸ªï¼Œä¸­é—´çš„æˆªæ–­åˆ°6ä¸ª
                            fixed_row = row[:2] + row[3:9] + [last_field]
                            print(f"  æ™ºèƒ½ä¿®å¤ï¼šä¿ç•™æœ€åå­—æ®µ'{last_field}'ï¼Œåˆ é™¤å¤šä½™å­—æ®µ")
                            row = fixed_row
                        else:
                            # å…¶ä»–æƒ…å†µï¼Œç®€å•æˆªæ–­
                            row = row[:expected_field_count]
                            print(f"  å·²æˆªæ–­ä¸º{expected_field_count}ä¸ªå­—æ®µ")

                writer.writerow(row)

            return output_stream.getvalue()
        except Exception as e:
            print(f"è­¦å‘Šï¼šCSVå­—æ®µéªŒè¯å¤±è´¥: {str(e)}")
            return csv_text


    def learn_rules(self, training_excel: str, use_cache: bool = True) -> tuple:
        """ç¬¬ä¸€é˜¶æ®µï¼šä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ å·¥å•é—®é¢˜ç‚¹æ¨ç†è§„åˆ™

        åˆ†æè®­ç»ƒExcelæ•°æ®ï¼Œå­¦ä¹ å¦‚ä½•æ ¹æ®å·¥å•ä¿¡æ¯æ¨ç†å‡ºç»´ä¿®é—®é¢˜ç‚¹å’ŒäºŒçº§é—®é¢˜ç‚¹

        Args:
            training_excel (str): è®­ç»ƒæ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (å­¦ä¹ åˆ°çš„è§„åˆ™å’Œå¯¹è¯æ¶ˆæ¯, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¯»å–
            rules_file = os.path.join(os.path.dirname(training_excel), "rules.json")
            print(rules_file)
            if use_cache and os.path.exists(rules_file):
                with open(rules_file, "r", encoding="utf-8") as f:
                    cached_data = json.load(f)
                messages = cached_data["messages"]
                rules = cached_data["rules"]
                return messages, rules, None

            # è¯»å–è®­ç»ƒExcelæ–‡ä»¶å†…å®¹ï¼ˆç¡…åŸºæµåŠ¨ä¸æ”¯æŒæ–‡ä»¶ä¸Šä¼ ï¼Œç›´æ¥è¯»å–å†…å®¹ï¼‰
            def _compact_training(df: pd.DataFrame) -> str:
                df = df.copy()
                drop_cols = [c for c in df.columns if str(c).startswith('Unnamed:')]
                if drop_cols:
                    df = df.drop(columns=drop_cols)
                for c in df.columns:
                    df[c] = df[c].astype(str).fillna('')

                def cnt(series_cond):
                    try:
                        return int(series_cond.sum())
                    except Exception:
                        return 0

                nature_counts = df.get('å·¥å•æ€§è´¨', pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
                svc = df.get('æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', pd.Series(dtype=str))
                cat = df.get('æ•…éšœç±»åˆ«', pd.Series(dtype=str))
                bn = df.get('ä¿å†…ä¿å¤–', pd.Series(dtype=str))
                oldp = df.get('æ—§ä»¶åç§°', pd.Series(dtype=str))
                newp = df.get('æ–°ä»¶åç§°', pd.Series(dtype=str))
                call = df.get('æ¥ç”µå†…å®¹', pd.Series(dtype=str))
                diag = df.get('ç°åœºè¯Šæ–­æ•…éšœç°è±¡', pd.Series(dtype=str))
                plan = df.get('å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨', pd.Series(dtype=str))

                core_parts = ['ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ']
                exchange_words = ['æ¢æœº','é€€æœº','é€€è´§']
                filter_words = ['æ¼ç‚­','é»‘ç‚¹','é»‘æ¸£','ç¢³ç²‰','æ´»æ€§ç‚­ç²‰æœ«']

                def contains_any(series, words):
                    s = series.astype(str)
                    return s.apply(lambda x: any(w in x for w in words))

                stats_lines = []
                stats_lines.append(f"æ ·æœ¬é‡: {len(df)}")
                if nature_counts:
                    stats_lines.append(f"å·¥å•æ€§è´¨åˆ†å¸ƒ: {nature_counts}")
                stats_lines.append(f"æœåŠ¡é¡¹ç›®Top10: {svc.value_counts().head(10).to_dict()}")
                stats_lines.append(f"æ•…éšœç±»åˆ«Top10: {cat.value_counts().head(10).to_dict()}")
                core_hit = cnt(oldp.apply(lambda x: any(p in str(x) for p in core_parts)) | newp.apply(lambda x: any(p in str(x) for p in core_parts)))
                stats_lines.append(f"æ ¸å¿ƒéƒ¨ä»¶å‘½ä¸­æ•°: {core_hit}")
                filt_hit = cnt(oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False))
                stats_lines.append(f"æ»¤èŠ¯æ›´æ¢è®°å½•æ•°: {filt_hit}")
                policy_hit = cnt(bn.astype(str).eq('ä¿å¤–è½¬ä¿å†…') & (oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False)))
                stats_lines.append(f"ä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯è®°å½•æ•°: {policy_hit}")
                noise_exchange = cnt(((plan.str.contains('å™ªéŸ³|åˆ†è´', na=False)) | (call.str.contains('å™ªéŸ³|åˆ†è´', na=False))) & (contains_any(plan, exchange_words) | contains_any(call, exchange_words)))
                stats_lines.append(f"å™ªéŸ³/åˆ†è´+æ¢é€€è®°å½•æ•°: {noise_exchange}")
                add_hit = cnt(svc.str.contains('åŠ è£…', na=False))
                safe_hit = cnt(svc.str.contains('å®‰å…¨ç»´æŠ¤', na=False))
                stats_lines.append(f"åŠ è£…è®°å½•æ•°: {add_hit}")
                stats_lines.append(f"å®‰å…¨ç»´æŠ¤è®°å½•æ•°: {safe_hit}")

                preview_cols = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
                for c in preview_cols:
                    if c not in df.columns:
                        df[c] = ''
                df_preview = df[preview_cols].copy()
                for c in ['æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨','åˆ¤å®šä¾æ®']:
                    df_preview[c] = df_preview[c].astype(str).str.slice(0, 120)
                max_rows = min(200, len(df_preview))
                df_preview = df_preview.head(max_rows)
                preview_csv = df_preview.to_csv(index=False)
                summary = "\n".join(stats_lines)
                return f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}\n\n# æ ·æœ¬é¢„è§ˆ(æœ€å¤š{max_rows}è¡Œ)\n{preview_csv}"

            training_content = _compact_training(df_training)

            # æ„å»ºå­¦ä¹ è§„åˆ™çš„æç¤ºè¯
            prompt1 = load_prompt('excel_learn_rules').format(training_content=training_content)

            messages = [{"role": "user", "content": prompt1}]

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹å­¦ä¹ è§„åˆ™
            resp1 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.6,  # æ§åˆ¶åˆ›é€ æ€§
                max_tokens=4096  # å…è®¸è¾ƒé•¿å›ç­”
            )

            # æå–å­¦ä¹ åˆ°çš„è§„åˆ™
            rules = resp1.choices[0].message.content.strip()
            messages.append({"role": "assistant", "content": rules})

            with open(rules_file, "w", encoding="utf-8") as f:
                json.dump({"messages": messages, "rules": rules}, f, ensure_ascii=False, indent=4)

            return messages, rules, resp1.usage  # è¿”å›å¯¹è¯å†å²ã€è§„åˆ™å’Œtokenä½¿ç”¨é‡

        except Exception as e:
            raise Exception(f"å­¦ä¹ è§„åˆ™å¤±è´¥: {str(e)}")

    def apply_rules(self, messages: list, test_excel: str) -> tuple:
        """ç¬¬äºŒé˜¶æ®µï¼šåº”ç”¨å­¦ä¹ åˆ°çš„è§„åˆ™å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œæ™ºèƒ½å¡«å……

        å°†ç¬¬ä¸€é˜¶æ®µå­¦åˆ°çš„æ¨ç†è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•Excelæ•°æ®ï¼Œè‡ªåŠ¨å¡«å……ç»´ä¿®é—®é¢˜ç‚¹å’ŒäºŒçº§é—®é¢˜ç‚¹

        Args:
            messages (list): åŒ…å«å­¦ä¹ è§„åˆ™çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            test_excel (str): æµ‹è¯•æ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (å¡«å……ç»“æœCSVå†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–æµ‹è¯•Excelæ–‡ä»¶å†…å®¹
            test_content = self._read_excel_to_text(test_excel)

            # è¯»å–Excelè·å–æ•°æ®è¡Œæ•°ï¼Œç¡®ä¿AIè¾“å‡ºå®Œæ•´çš„ç»“æœ
            df_test = pd.read_excel(test_excel, dtype=str)
            test_row_count = len(df_test)

            # æ„å»ºåº”ç”¨è§„åˆ™çš„æç¤ºè¯ï¼ŒæŒ‡å¯¼AIå°†å­¦åˆ°çš„è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•æ•°æ®
            prompt2 = load_prompt('excel_apply_rules').format(
                test_content=test_content,
                test_row_count=test_row_count
            )

            # å°†åº”ç”¨è§„åˆ™çš„è¯·æ±‚æ·»åŠ åˆ°å¯¹è¯å†å²ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
            messages.append({"role": "user", "content": prompt2})

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹åº”ç”¨è§„åˆ™è¿›è¡Œå¡«å……
            resp2 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,  # åŒ…å«ä¹‹å‰å­¦ä¹ çš„è§„åˆ™
                temperature=0.6,
                max_tokens=4096
            )

            # æå–å¡«å……ç»“æœ
            filled_result = resp2.choices[0].message.content.strip()

            # æ¸…ç†AIè¿”å›çš„ç»“æœï¼Œç§»é™¤Markdownæ ‡è®°
            if filled_result.startswith('```csv'):
                filled_result = filled_result[6:]  # ç§»é™¤å¼€å¤´çš„```csv
            if filled_result.endswith('```'):
                filled_result = filled_result[:-3]  # ç§»é™¤ç»“å°¾çš„```
            filled_result = filled_result.strip()

            # ç¡®ä¿æœ‰æ­£ç¡®çš„CSVå¤´éƒ¨ï¼ˆæ ¹æ®å·¥å•é—®é¢˜ç‚¹æ£€æµ‹çš„å­—æ®µï¼‰
            expected_header = 'ç¼–å·(ç»´ä¿®è¡Œ),æ¥ç”µå†…å®¹(ç»´ä¿®è¡Œ),ç°åœºè¯Šæ–­æ•…éšœç°è±¡(ç»´ä¿®è¡Œ),æ•…éšœéƒ¨ä½(ç»´ä¿®è¡Œ),æ•…éšœä»¶åç§°(ç»´ä¿®è¡Œ),å¤„ç†æ–¹æ¡ˆç®€è¿°(ç»´ä¿®è¡Œ),æ•…éšœç±»åˆ«(ç»´ä¿®è¡Œ),ç»´ä¿®é—®é¢˜ç‚¹,äºŒçº§é—®é¢˜ç‚¹'
            if not filled_result.startswith('ç¼–å·(ç»´ä¿®è¡Œ)'):
                filled_result = expected_header + '\n' + filled_result

            return filled_result, resp2.usage  # è¿”å›æ¸…ç†åçš„å¡«å……ç»“æœå’Œtokenä½¿ç”¨é‡

        except Exception as e:
            raise Exception(f"åº”ç”¨è§„åˆ™å¤±è´¥: {str(e)}")

    def learn_quality_rules_v1_backup(self, training_excel: str) -> tuple:
        """å¤‡ä»½ç‰ˆæœ¬ï¼šä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ è´¨é‡å·¥å•è¯†åˆ«è§„åˆ™

        ç¬¬ä¸€é˜¶æ®µå¤„ç†ï¼šåˆ†æè®­ç»ƒExcelæ•°æ®ï¼Œå­¦ä¹ è´¨é‡å·¥å•çš„åˆ¤æ–­è§„å¾‹å’Œæ ‡æ³¨æ ‡å‡†

        Args:
            training_excel (str): è®­ç»ƒæ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (å­¦ä¹ åˆ°çš„è§„åˆ™å†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–è®­ç»ƒExcelæ–‡ä»¶å†…å®¹
            # å‹ç¼©è®­ç»ƒå†…å®¹ï¼Œé¿å…è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
            def _contains_any(series, words):
                s = series.astype(str)
                return s.apply(lambda x: any(w in x for w in words))
            df_compact = df_training.copy()
            drop_cols = [c for c in df_compact.columns if str(c).startswith('Unnamed:')]
            if drop_cols:
                df_compact = df_compact.drop(columns=drop_cols)
            for c in df_compact.columns:
                df_compact[c] = df_compact[c].astype(str).fillna('')
            nature_counts = df_compact.get('å·¥å•æ€§è´¨', pd.Series(dtype=str)).value_counts(dropna=False).to_dict()
            svc = df_compact.get('æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', pd.Series(dtype=str))
            cat = df_compact.get('æ•…éšœç±»åˆ«', pd.Series(dtype=str))
            bn = df_compact.get('ä¿å†…ä¿å¤–', pd.Series(dtype=str))
            oldp = df_compact.get('æ—§ä»¶åç§°', pd.Series(dtype=str))
            newp = df_compact.get('æ–°ä»¶åç§°', pd.Series(dtype=str))
            call = df_compact.get('æ¥ç”µå†…å®¹', pd.Series(dtype=str))
            diag = df_compact.get('ç°åœºè¯Šæ–­æ•…éšœç°è±¡', pd.Series(dtype=str))
            plan = df_compact.get('å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨', pd.Series(dtype=str))
            core_parts = ['ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ']
            stats_lines = []
            stats_lines.append(f"æ ·æœ¬é‡: {len(df_compact)}")
            if nature_counts:
                stats_lines.append(f"å·¥å•æ€§è´¨åˆ†å¸ƒ: {nature_counts}")
            stats_lines.append(f"æœåŠ¡é¡¹ç›®Top10: {svc.value_counts().head(10).to_dict() if svc is not None else {}}")
            stats_lines.append(f"æ•…éšœç±»åˆ«Top10: {cat.value_counts().head(10).to_dict() if cat is not None else {}}")
            core_hit = int((oldp.apply(lambda x: any(p in str(x) for p in core_parts)) | newp.apply(lambda x: any(p in str(x) for p in core_parts))).sum()) if (oldp is not None and newp is not None) else 0
            stats_lines.append(f"æ ¸å¿ƒéƒ¨ä»¶å‘½ä¸­æ•°: {core_hit}")
            filt_hit = int((oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False)).sum()) if (oldp is not None and newp is not None) else 0
            stats_lines.append(f"æ»¤èŠ¯æ›´æ¢è®°å½•æ•°: {filt_hit}")
            policy_hit = int((bn.astype(str).eq('ä¿å¤–è½¬ä¿å†…') & (oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False))).sum()) if (bn is not None and oldp is not None and newp is not None) else 0
            stats_lines.append(f"ä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯è®°å½•æ•°: {policy_hit}")
            preview_cols = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
            for c in preview_cols:
                if c not in df_compact.columns:
                    df_compact[c] = ''
            df_preview = df_compact[preview_cols].copy()
            for c in ['æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨','åˆ¤å®šä¾æ®']:
                df_preview[c] = df_preview[c].astype(str).str.slice(0, 120)
            max_rows = min(200, len(df_preview))
            df_preview = df_preview.head(max_rows)
            preview_csv = df_preview.to_csv(index=False)
            summary = "\n".join(stats_lines)
            training_content = f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}\n\n# æ ·æœ¬é¢„è§ˆ(æœ€å¤š{max_rows}è¡Œ)\n{preview_csv}"

            # æ„å»ºå­¦ä¹ è§„åˆ™çš„æç¤ºè¯
            prompt1 = load_prompt('quality_learn_rules_optimized').format(training_content=training_content)

            messages = [{"role": "user", "content": prompt1}]

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹å­¦ä¹ è§„åˆ™
            resp1 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.6,
                max_tokens=4096
            )

            # æå–å­¦ä¹ åˆ°çš„è§„åˆ™
            rules = resp1.choices[0].message.content.strip()
            messages.append({"role": "assistant", "content": rules})

            return messages, rules, resp1.usage

        except Exception as e:
            raise Exception(f"å­¦ä¹ éè´¨é‡é—®é¢˜è¯†åˆ«è§„åˆ™å¤±è´¥: {str(e)}")

    def apply_quality_rules_v1_backup(self, messages: list, test_excel: str) -> tuple:
        """å¤‡ä»½ç‰ˆæœ¬ï¼šåº”ç”¨å­¦ä¹ åˆ°çš„è§„åˆ™å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œè´¨é‡å·¥å•åˆ¤æ–­

        ç¬¬äºŒé˜¶æ®µå¤„ç†ï¼šå°†ç¬¬ä¸€é˜¶æ®µå­¦åˆ°çš„è§„åˆ™åº”ç”¨åˆ°æµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œè´¨é‡å·¥å•åˆ†ç±»

        Args:
            messages (list): åŒ…å«å­¦ä¹ è§„åˆ™çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            test_excel (str): æµ‹è¯•æ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (åˆ¤æ–­ç»“æœCSVå†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            # è¯»å–æµ‹è¯•Excelæ–‡ä»¶å†…å®¹
            test_content = self._read_excel_to_text(test_excel)

            # è¯»å–Excelè·å–æ•°æ®è¡Œæ•°
            df_test = pd.read_excel(test_excel, dtype=str)
            test_row_count = len(df_test)

            # æ„å»ºåº”ç”¨è§„åˆ™çš„æç¤ºè¯
            prompt2 = load_prompt('quality_apply_rules_optimized').format(
                test_content=test_content,
                test_row_count=test_row_count
            )

            # æ·»åŠ åº”ç”¨è§„åˆ™çš„ç”¨æˆ·è¯·æ±‚åˆ°å¯¹è¯å†å²
            messages.append({"role": "user", "content": prompt2})

            # è°ƒç”¨ç¡…åŸºæµåŠ¨æ¨¡å‹åº”ç”¨è§„åˆ™è¿›è¡Œåˆ¤æ–­
            resp2 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.2,
                max_tokens=4096
            )

            # æå–åˆ¤æ–­ç»“æœ
            quality_result = resp2.choices[0].message.content.strip()

            # æ¸…ç†AIè¿”å›çš„ç»“æœï¼Œç§»é™¤Markdownæ ‡è®°
            if quality_result.startswith('```csv'):
                quality_result = quality_result[6:]  # ç§»é™¤å¼€å¤´çš„```csv
            if quality_result.endswith('```'):
                quality_result = quality_result[:-3]  # ç§»é™¤ç»“å°¾çš„```
            quality_result = quality_result.strip()

            # ç¡®ä¿æœ‰CSVå¤´éƒ¨ï¼ˆ13åˆ—ç»“æ„ï¼‰
            expected_header = 'å·¥å•å•å·,åˆ¤å®šä¾æ®,æ•…éšœéƒ¨ä½åç§°,æ•…éšœç»„,æ•…éšœç±»åˆ«,æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡,æ•…éšœä»¶ç®€ç§°,æ—§ä»¶åç§°,æ–°ä»¶åç§°,æ¥ç”µå†…å®¹,ç°åœºè¯Šæ–­æ•…éšœç°è±¡,å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨,å·¥å•æ€§è´¨'
            if not quality_result.startswith('å·¥å•å•å·'):
                quality_result = expected_header + '\n' + quality_result

            quality_result = self._strip_unexpected_header_rows(quality_result, expected_header)
            quality_result = self._force_realign_columns(quality_result, expected_header, 13)
            quality_result = self._fix_csv_format(quality_result)
            quality_result = self._validate_and_fix_csv_fields(quality_result, expected_field_count=13)
            quality_result = self._fix_quality_column_position(quality_result)
            quality_result = self._ensure_order_numbers(quality_result, df_test)
            quality_result = self._enrich_non_quality_basis(quality_result)

            return quality_result, resp2.usage

        except Exception as e:
            raise Exception(f"åº”ç”¨éè´¨é‡é—®é¢˜è¯†åˆ«è§„åˆ™å¤±è´¥: {str(e)}")

    def learn_quality_rules(self, training_excel: str, use_cache: bool = True) -> tuple:
        """æ–°ç‰ˆæœ¬ï¼šä¸¤é˜¶æ®µæ¨ç† - ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ ç»´ä¿®é—®é¢˜ç‚¹æ¨ç†è§„åˆ™ - æ–¹æ³•"""
        try:
            rules_file = os.path.join(os.path.dirname(training_excel), "rules.json")

            # å¦‚æœä½¿ç”¨ç¼“å­˜ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™ç›´æ¥è¯»å–
            if use_cache and os.path.exists(rules_file):
                with open(rules_file, "r", encoding="utf-8") as f:
                    cached_data = json.load(f)
                messages = cached_data["messages"]
                rules = cached_data["rules"]
                return messages, rules, None
            import time

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤1: åŠ è½½è®­ç»ƒæ•°æ®")
            print("="*80)
            print(f"è®­ç»ƒæ–‡ä»¶: {training_excel}")

            # è¯»å–è®­ç»ƒExcelæ–‡ä»¶å†…å®¹
            try:
                df_training = pd.read_excel(training_excel, dtype=str)
                training_rows = len(df_training)
                training_cols = len(df_training.columns)
                print(f"åŠ è½½çŠ¶æ€: âœ… æˆåŠŸ")
                print(f"æ•°æ®è§„æ¨¡: {training_rows}è¡Œ x {training_cols}åˆ—")
                print(f"åˆ—å: {', '.join(df_training.columns.tolist())}")
            except Exception as e:
                print(f"åŠ è½½çŠ¶æ€: âŒ å¤±è´¥")
                print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                raise

            # æ„å»ºç²¾ç®€è®­ç»ƒå†…å®¹ï¼Œé¿å…è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦
            df_compact = df_training.copy()
            drop_cols = [c for c in df_compact.columns if str(c).startswith('Unnamed:')]
            if drop_cols:
                df_compact = df_compact.drop(columns=drop_cols)
            for c in df_compact.columns:
                df_compact[c] = df_compact[c].astype(str).fillna('')

            def _safe_vc(series):
                try:
                    return series.value_counts().head(10).to_dict()
                except Exception:
                    return {}

            nature_counts = (df_compact['å·¥å•æ€§è´¨'].value_counts(dropna=False).to_dict() if 'å·¥å•æ€§è´¨' in df_compact.columns else {})
            svc = (df_compact['æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡'] if 'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡' in df_compact.columns else pd.Series(dtype=str))
            cat = (df_compact['æ•…éšœç±»åˆ«'] if 'æ•…éšœç±»åˆ«' in df_compact.columns else pd.Series(dtype=str))
            bn = (df_compact['ä¿å†…ä¿å¤–'] if 'ä¿å†…ä¿å¤–' in df_compact.columns else pd.Series(dtype=str))
            oldp = (df_compact['æ—§ä»¶åç§°'] if 'æ—§ä»¶åç§°' in df_compact.columns else pd.Series(dtype=str))
            newp = (df_compact['æ–°ä»¶åç§°'] if 'æ–°ä»¶åç§°' in df_compact.columns else pd.Series(dtype=str))

            core_parts = ['ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ']
            core_hit = int((oldp.apply(lambda x: any(p in str(x) for p in core_parts)) | newp.apply(lambda x: any(p in str(x) for p in core_parts))).sum())
            filt_hit = int((oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False)).sum())
            policy_hit = int((bn.astype(str).eq('ä¿å¤–è½¬ä¿å†…') & (oldp.str.contains('æ»¤èŠ¯', na=False) | newp.str.contains('æ»¤èŠ¯', na=False))).sum())

            stats_lines = []
            stats_lines.append(f"æ ·æœ¬é‡: {len(df_compact)}")
            stats_lines.append(f"å·¥å•æ€§è´¨åˆ†å¸ƒ: {nature_counts}")
            stats_lines.append(f"æœåŠ¡é¡¹ç›®Top10: {_safe_vc(svc)}")
            stats_lines.append(f"æ•…éšœç±»åˆ«Top10: {_safe_vc(cat)}")
            stats_lines.append(f"æ ¸å¿ƒéƒ¨ä»¶å‘½ä¸­æ•°: {core_hit}")
            stats_lines.append(f"æ»¤èŠ¯æ›´æ¢è®°å½•æ•°: {filt_hit}")
            stats_lines.append(f"ä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯è®°å½•æ•°: {policy_hit}")

            preview_cols = ['å·¥å•å•å·','å·¥å•æ€§è´¨','åˆ¤å®šä¾æ®','ä¿å†…ä¿å¤–','æ‰¹æ¬¡å…¥åº“æ—¥æœŸ','å®‰è£…æ—¥æœŸ','è´­æœºæ—¥æœŸ','äº§å“åç§°','å¼€å‘ä¸»ä½“','æ•…éšœéƒ¨ä½åç§°','æ•…éšœç»„','æ•…éšœç±»åˆ«','æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡','ç»´ä¿®æ–¹å¼','æ—§ä»¶åç§°','æ–°ä»¶åç§°','æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨']
            for c in preview_cols:
                if c not in df_compact.columns:
                    df_compact[c] = ''
            df_preview = df_compact[preview_cols].copy()
            for c in ['æ¥ç”µå†…å®¹','ç°åœºè¯Šæ–­æ•…éšœç°è±¡','å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨','åˆ¤å®šä¾æ®']:
                df_preview[c] = df_preview[c].astype(str).str.slice(0, 120)
            max_rows = min(80, len(df_preview))
            df_preview = df_preview.head(max_rows)
            preview_csv = df_preview.to_csv(index=False)
            summary = "\n".join(stats_lines)
            training_content = f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}\n\n# æ ·æœ¬é¢„è§ˆ(æœ€å¤š{max_rows}è¡Œ)\n{preview_csv}"
            print("-"*80)

            print("\n" + "="*80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤2: AIå­¦ä¹ é˜¶æ®µï¼ˆç¬¬ä¸€æ­¥æ¨ç†ï¼‰")
            print("="*80)
            print(f"æç¤ºè¯æ–‡ä»¶: prompts/quality_learn_rules_optimized.txt")

            # ä½¿ç”¨è´¨é‡å·¥å•åˆ¤æ–­çš„å­¦ä¹ æç¤ºè¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ - é‡‡ç”¨åˆ†ç‰‡å¤šè½®æ³¨å…¥ï¼Œé¿å…è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦
            # ä»…æŠŠç»Ÿè®¡æ‘˜è¦ä½œä¸ºä¸»æç¤ºè¯å†…å®¹
            summary_text = f"# è®­ç»ƒæ•°æ®ç»Ÿè®¡æ‘˜è¦\n{summary}"
            header_prompt = load_prompt('quality_learn_rules_optimized').format(training_content=summary_text)

            # é¢„è§ˆCSVåˆ†ç‰‡ï¼ˆé™åˆ¶æ€»é•¿åº¦ï¼‰
            chunk_size = 3000
            chunks = []
            for i in range(0, len(preview_csv), chunk_size):
                chunks.append(preview_csv[i:i+chunk_size])
            chunks = chunks[:5]

            messages = [{"role": "user", "content": header_prompt}]
            for idx, ch in enumerate(chunks, start=1):
                messages.append({"role": "user", "content": f"è®­ç»ƒæ ·æœ¬ç‰‡æ®µ{idx}:\n{ch}"})
            messages.append({"role": "user", "content": "è¯·åŸºäºä¸Šè¿°ç»Ÿè®¡æ‘˜è¦ä¸æ ·æœ¬ç‰‡æ®µï¼Œæ€»ç»“è´¨é‡å·¥å•ä¸éè´¨é‡å·¥å•çš„è§„åˆ™ã€æƒé‡ä¸å†³ç­–æµç¨‹ï¼Œä¸¥æ ¼æŒ‰è¦æ±‚æ ¼å¼è¾“å‡ºã€‚"})

            print(f"æç¤ºè¯ä¸»å†…å®¹é•¿åº¦: {len(header_prompt)} å­—ç¬¦ï¼Œæ ·æœ¬ç‰‡æ®µæ•°: {len(chunks)}")
            print("-"*80)

            print("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹å­¦ä¹ åˆ¤æ–­è§„å¾‹...")
            start_time = time.time()

            # è°ƒç”¨AIæ¨¡å‹å­¦ä¹ è§„åˆ™
            resp1 = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œæ›´èšç„¦äºè§„åˆ™å­¦ä¹ 
                max_tokens=4096
            )

            elapsed_time = time.time() - start_time

            # æå–å­¦ä¹ åˆ°çš„è§„åˆ™
            rules = resp1.choices[0].message.content.strip()
            messages.append({"role": "assistant", "content": rules})

            print(f"âœ… AIå­¦ä¹ å®Œæˆ")
            print(f"è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"å­¦ä¹ ç»“æœé•¿åº¦: {len(rules)} å­—ç¬¦")
            print(f"å­¦ä¹ ç»“æœé¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
            print(rules[:500] + "...")
            print(f"Tokenä½¿ç”¨: è¾“å…¥={resp1.usage.prompt_tokens}, è¾“å‡º={resp1.usage.completion_tokens}, æ€»è®¡={resp1.usage.total_tokens}")
            print("-"*80)

            with open(rules_file, "w", encoding="utf-8") as f:
                json.dump({
                    "messages": messages,
                    "rules": rules,
                }, f, ensure_ascii=False, indent=4)

            return messages, rules, resp1.usage

        except Exception as e:
            print(f"\nâŒ é”™è¯¯: å­¦ä¹ ç»´ä¿®é—®é¢˜ç‚¹æ¨ç†è§„åˆ™å¤±è´¥")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            raise Exception(f"å­¦ä¹ ç»´ä¿®é—®é¢˜ç‚¹æ¨ç†è§„åˆ™å¤±è´¥: {str(e)}")

    def _get_thread_client(self) -> OpenAI:
        """è·å–çº¿ç¨‹ä¸“ç”¨çš„å®¢æˆ·ç«¯å®ä¾‹"""
        if not hasattr(self._thread_local, 'client'):
            self._thread_local.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=600
            )
        return self._thread_local.client

    def _deep_copy_messages(self, original_messages: List[Dict]) -> List[Dict]:
        """æ·±æ‹·è´æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿æ¯ä¸ªçº¿ç¨‹æœ‰ç‹¬ç«‹çš„å‰¯æœ¬"""
        return copy.deepcopy(original_messages)

    def _call_ai_api_with_retry(self, messages, max_retries=None, initial_delay=1):
        """å¸¦é‡è¯•çš„APIè°ƒç”¨"""
        if max_retries is None:
            max_retries = self.max_retries

        last_exception = None

        for attempt in range(max_retries + 1):  # åŒ…æ‹¬ç¬¬ä¸€æ¬¡å°è¯•
            try:
                if attempt > 0:  # ä¸æ˜¯ç¬¬ä¸€æ¬¡å°è¯•
                    delay = initial_delay * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿
                    delay = min(delay, 60)  # æœ€å¤§å»¶è¿Ÿ60ç§’
                    print(f"ğŸ”„ ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œç­‰å¾…{delay:.1f}ç§’...")
                    time.sleep(delay)

                print(f"ğŸ“¡ è°ƒç”¨AI API (å°è¯• {attempt + 1}/{max_retries + 1})...")

                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=24576
                )

                if attempt > 0:
                    print(f"âœ… é‡è¯•æˆåŠŸï¼")

                return resp

            except Exception as e:
                last_exception = e
                error_type = type(e).__name__

                # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
                if "rate limit" in str(e).lower() or "429" in str(e):
                    print(f"â³ é‡åˆ°é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…é‡è¯•...")
                elif "timeout" in str(e).lower():
                    print(f"â° è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­...")
                elif "connection" in str(e).lower():
                    print(f"ğŸ”Œ è¿æ¥é”™è¯¯ï¼Œé‡è¯•ä¸­...")
                else:
                    print(f"âš ï¸  APIè°ƒç”¨å¤±è´¥ ({error_type}): {str(e)[:100]}...")

                if attempt < max_retries:
                    print(f"   å°†åœ¨{initial_delay * (2 ** attempt)}ç§’åé‡è¯•...")
                else:
                    print(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"APIè°ƒç”¨å¤±è´¥ï¼Œé‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {str(last_exception)}")

    def apply_quality_rules(self, messages: list, test_excel: str) -> tuple:
        """å·¥å•ç±»å‹æ£€æµ‹ï¼šä¸¤é˜¶æ®µæ¨ç†å¤„ç†

        ç¬¬ä¸€æ­¥ï¼šæ¨ç†ç»´ä¿®é—®é¢˜ç‚¹å’ŒäºŒçº§é—®é¢˜ç‚¹
        ç¬¬äºŒæ­¥ï¼šåŸºäºé—®é¢˜ç‚¹åˆ¤æ–­æ˜¯å¦ä¸ºè´¨é‡å·¥å•

        Args:
            messages (list): åŒ…å«å­¦ä¹ è§„åˆ™çš„å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            test_excel (str): æµ‹è¯•æ•°æ®Excelæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (è´¨é‡å·¥å•åˆ¤æ–­ç»“æœCSVå†…å®¹, APIä½¿ç”¨ç»Ÿè®¡)
        """
        try:
            import time
            import csv
            from io import StringIO

            # 1. åˆ›å»ºçº¿ç¨‹ç‹¬ç«‹çš„æ¶ˆæ¯å‰¯æœ¬
            thread_messages = self._deep_copy_messages(messages)
            # 2. è·å–çº¿ç¨‹ä¸“ç”¨çš„å®¢æˆ·ç«¯
            client = self._get_thread_client()
            print("\n" + "=" * 80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤3: åŠ è½½æµ‹è¯•æ•°æ®")
            print("=" * 80)
            print(f"æµ‹è¯•æ–‡ä»¶: {test_excel}")

            # è¯»å–Excelæ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†
            try:
                df_test = pd.read_excel(test_excel, dtype=str)
                test_rows = len(df_test)
                test_cols = len(df_test.columns)
                print(f"åŠ è½½çŠ¶æ€: âœ… æˆåŠŸ")
                print(f"æ•°æ®è§„æ¨¡: {test_rows}è¡Œ x {test_cols}åˆ—")
                print(f"åˆ—å: {', '.join(df_test.columns.tolist())}")
            except Exception as e:
                print(f"åŠ è½½çŠ¶æ€: âŒ å¤±è´¥")
                print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                raise

            # æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–åˆ—åå’Œæ·»åŠ å¿…è¦å­—æ®µ
            if 'å·¥å•å•å·' not in df_test.columns and 'ç»´ä¿®å·¥å•å·' not in df_test.columns:
                df_test.insert(0, 'å·¥å•å•å·', range(1, len(df_test) + 1))
                print("âš ï¸  æœªæ‰¾åˆ°å·¥å•ç¼–å·åˆ—ï¼Œå·²è‡ªåŠ¨ç”Ÿæˆåºå·")
            elif 'ç»´ä¿®å·¥å•å·' in df_test.columns and 'å·¥å•å•å·' not in df_test.columns:
                df_test = df_test.rename(columns={'ç»´ä¿®å·¥å•å·': 'å·¥å•å•å·'})
                print("âœ… å·²å°†'ç»´ä¿®å·¥å•å·'é‡å‘½åä¸º'å·¥å•å•å·'")

            if 'å·¥å•æ€§è´¨' not in df_test.columns:
                df_test['å·¥å•æ€§è´¨'] = ''
                print("âœ… å·²æ·»åŠ 'å·¥å•æ€§è´¨'åˆ—")
            if 'æ—§ä»¶åç§°' not in df_test.columns:
                df_test['æ—§ä»¶åç§°'] = ''
                print("âœ… å·²æ·»åŠ 'æ—§ä»¶åç§°'åˆ—")
            if 'æ–°ä»¶åç§°' not in df_test.columns:
                df_test['æ–°ä»¶åç§°'] = ''
                print("âœ… å·²æ·»åŠ 'æ–°ä»¶åç§°'åˆ—")

            # ä¿å­˜é¢„å¤„ç†åçš„æ–‡ä»¶
            processed_file = test_excel.replace('.xlsx', '_processed.xlsx')
            df_test.to_excel(processed_file, index=False)
            print(f"é¢„å¤„ç†åæ–‡ä»¶: {processed_file}")
            print("-" * 80)

            # è¯»å–é¢„å¤„ç†åçš„æ–‡ä»¶å†…å®¹
            test_content = self._read_excel_to_text(processed_file)
            print('è¾“å…¥æ•°æ®')
            print(test_content)
            test_row_count = len(df_test)

            # è®¡ç®—åŸå§‹æ•°æ®çš„åˆ—æ•°
            original_columns = list(df_test.columns)
            expected_column_count = len(original_columns)
            print(f"åŸå§‹æ•°æ®åˆ—æ•°: {expected_column_count}åˆ—")

            print("\n" + "=" * 80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤4: AIæ™ºèƒ½åˆ¤æ–­å·¥å•æ€§è´¨")
            print("=" * 80)
            print(f"æµ‹è¯•æ•°æ®: {test_row_count} è¡Œ")
            print("-" * 80)

            # æ„å»ºAIåˆ¤æ–­æç¤ºè¯
            quality_prompt = f"""
æ ¹æ®åˆšæ‰ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ åˆ°çš„åˆ¤æ–­è§„å¾‹ï¼Œå¯¹ä¸‹é¢çš„æµ‹è¯•æ•°æ®è¿›è¡Œå·¥å•æ€§è´¨åˆ¤æ–­ã€‚

æµ‹è¯•æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰ï¼š
{test_content}

**æ ¸å¿ƒè¦æ±‚ï¼šä¸¥æ ¼æŒ‰ç…§ä½ åˆšæ‰å­¦ä¹ çš„è§„åˆ™è¿›è¡Œåˆ¤æ–­ï¼**

åˆ¤æ–­æµç¨‹ï¼ˆå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š

**ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¡¬æ€§è§„åˆ™ï¼ˆç¬¬ä¸€å±‚ï¼‰**
å¯¹æ¯æ¡è®°å½•ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å‘½ä¸­ä»¥ä¸‹ç¡¬æ€§è§„åˆ™ï¼š

Aç±»ï¼ˆè´¨é‡å·¥å•ï¼‰ï¼š
1. æ–°æœºé»„é‡‘æ³•åˆ™ï¼šè´­æœº/å®‰è£…æ—¥æœŸâ‰¤30å¤© + æ¢æœº/é€€æœº/é€€è´§
2. äº§å“é‰´å®šæ”¿ç­–ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"äº§å“é‰´å®š" + æ¢æœº/é€€æœº/é€€è´§
3. æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢ï¼šæ—§ä»¶/æ–°ä»¶åœ¨æ ¸å¿ƒéƒ¨ä»¶åº“ä¸­
4. æ»¤èŠ¯è´¨é‡ç¼ºé™·ï¼šæ»¤èŠ¯ + æ¼ç¢³/é»‘ç‚¹/é»‘æ¸£/ç¢³ç²‰
5. ä¿å¤–è½¬ä¿å†…ï¼šä¿å¤–è½¬ä¿å†… + æ»¤èŠ¯
6. å™ªéŸ³æ¢æœºï¼šå™ªéŸ³/åˆ†è´ + æ¢æœº/é€€æœº/é€€è´§

Bç±»ï¼ˆéè´¨é‡å·¥å•ï¼‰ï¼š
1. å¤–éƒ¨åŠ è£…ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"åŠ è£…"
2. å®‰å…¨ç»´æŠ¤ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"å®‰å…¨ç»´æŠ¤"
3. ç”¨æˆ·/ç¯å¢ƒè´£ä»»ï¼šå¤„ç†æ–¹æ¡ˆåŒ…å«ç”¨æˆ·/å®¢æˆ·/å°ç›†/å¨æˆ¿/ä¸‹æ°´/ç¬¬ä¸‰æ–¹/æ°´å‹/æ°´è´¨

**å¦‚æœå‘½ä¸­ç¡¬æ€§è§„åˆ™ï¼Œç«‹å³åˆ¤å®šï¼Œä¸å†ç»§ç»­ï¼**

**ç¬¬äºŒæ­¥ï¼šåº”ç”¨å­¦ä¹ çš„æ¨¡å¼ï¼ˆç¬¬äºŒå±‚ï¼‰**
å¦‚æœç¬¬ä¸€æ­¥æœªå‘½ä¸­ï¼Œåˆ™åº”ç”¨ä½ ä»è®­ç»ƒæ•°æ®ä¸­å­¦åˆ°çš„è§„åˆ™ï¼š
- å­—æ®µæƒé‡ä½“ç³»ï¼ˆæœåŠ¡é¡¹ç›®40%ã€åˆ¤å®šä¾æ®30%ã€æ•…éšœç±»åˆ«15%ã€éƒ¨ä»¶æ›´æ¢10%ã€è¾…åŠ©5%ï¼‰
- äº¤å‰éªŒè¯è§„åˆ™ï¼ˆåŠŸèƒ½æ€§æ•…éšœ vs ç»´æŠ¤è¡Œä¸ºï¼‰
- è¾¹ç•Œæƒ…å†µå¤„ç†

**ç¬¬ä¸‰æ­¥ï¼šé»˜è®¤ç­–ç•¥**
å¦‚æœä¿¡æ¯ä¸¥é‡ä¸è¶³ï¼Œé»˜è®¤ä¸º"éè´¨é‡å·¥å•"

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»åŒ…å«æ‰€æœ‰19ä¸ªå­—æ®µï¼šå·¥å•å•å·,å·¥å•æ€§è´¨,åˆ¤å®šä¾æ®,ä¿å†…ä¿å¤–,æ‰¹æ¬¡å…¥åº“æ—¥æœŸ,å®‰è£…æ—¥æœŸ,è´­æœºæ—¥æœŸ,äº§å“åç§°,å¼€å‘ä¸»ä½“,æ•…éšœéƒ¨ä½åç§°,æ•…éšœç»„,æ•…éšœç±»åˆ«,æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡,ç»´ä¿®æ–¹å¼,æ—§ä»¶åç§°,æ–°ä»¶åç§°,æ¥ç”µå†…å®¹,ç°åœºè¯Šæ–­æ•…éšœç°è±¡,å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨
- **å·¥å•æ€§è´¨**ï¼šåªèƒ½æ˜¯"è´¨é‡å·¥å•"æˆ–"éè´¨é‡å·¥å•"
- **åˆ¤å®šä¾æ®**ï¼šå¿…é¡»æ˜ç¡®è¯´æ˜ï¼š
  * å¦‚æœå‘½ä¸­ç¡¬æ€§è§„åˆ™ï¼šRule A[ç¼–å·] æˆ– Rule B[ç¼–å·]ï¼Œå…³é”®è¯ï¼š"XXX"
  * å¦‚æœåº”ç”¨å­¦ä¹ è§„åˆ™ï¼šå­—æ®µæƒé‡åˆ†æï¼Œä¸»è¦ä¾æ®ï¼š"XXX"
  * ç¤ºä¾‹ï¼š"Rule A3: æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢ï¼Œæ—§ä»¶=ä¸»æ¿ï¼Œæ–°ä»¶=ä¸»æ¿"
  * ç¤ºä¾‹ï¼š"å­—æ®µæƒé‡åˆ†æï¼ŒæœåŠ¡é¡¹ç›®=åŠ è£…å‹åŠ›æ¡¶(B1è§„åˆ™)ï¼Œåˆ¤å®šä¸ºéè´¨é‡å·¥å•"
- æ¯è¡Œå¿…é¡»ä¸¥æ ¼åŒ…å«19ä¸ªå­—æ®µ
- ä»…è¾“å‡ºCSVæ ¼å¼æ•°æ®ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š

CSVæ ¼å¼è§„èŒƒï¼š
- å¦‚æœå­—æ®µå†…å®¹åŒ…å«é€—å·(,)ã€å¼•å·(")æˆ–æ¢è¡Œç¬¦ï¼Œå¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹è¯¥å­—æ®µ
- å­—æ®µå†…çš„åŒå¼•å·éœ€è¦è½¬ä¹‰ä¸ºä¸¤ä¸ªåŒå¼•å·("")
- ä¸è¦åœ¨CSVä¸­æ’å…¥ç©ºè¡Œæˆ–åˆ†éš”çº¿

é‡è¦æé†’ï¼š
- å¿…é¡»å…ˆæ£€æŸ¥ç¡¬æ€§è§„åˆ™ï¼ˆç¬¬ä¸€å±‚ï¼‰
- ç¡¬æ€§è§„åˆ™ä¼˜å…ˆçº§æœ€é«˜
- åˆ¤å®šä¾æ®å¿…é¡»è¯¦ç»†ã€æ˜ç¡®
- ä¸è¦è¿›è¡Œä»»ä½•ä¸»è§‚æ¨æ–­æˆ–è¯­ä¹‰æ‰©å±•

ğŸš¨ **å¼ºåˆ¶è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
1. **å¿…é¡»è¾“å‡ºæ‰€æœ‰{test_row_count}æ¡è®°å½•ï¼Œä¸€æ¡éƒ½ä¸èƒ½å°‘ï¼**
2. **ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯è¡¨å¤´è¡Œï¼ˆåˆ—åï¼‰**
3. **ä»ç¬¬äºŒè¡Œå¼€å§‹æ˜¯æ•°æ®è¡Œï¼Œå…±{test_row_count}è¡Œæ•°æ®**
4. **æ€»è¾“å‡ºè¡Œæ•° = 1ï¼ˆè¡¨å¤´ï¼‰+ {test_row_count}ï¼ˆæ•°æ®ï¼‰= {test_row_count + 1}è¡Œ**
5. **ä¸è¦å› ä¸ºè®°å½•ç›¸ä¼¼å°±çœç•¥ï¼Œæ¯æ¡è®°å½•éƒ½å¿…é¡»ç‹¬ç«‹è¾“å‡º**
6. **ä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æ–‡å­—ã€æ€»ç»“æˆ–è§£é‡Šï¼Œåªè¾“å‡ºçº¯CSVæ•°æ®**


è¯·å¼€å§‹åˆ¤æ–­ï¼ˆå…±{test_row_count}æ¡è®°å½•ï¼Œå¿…é¡»å…¨éƒ¨è¾“å‡ºï¼‰ï¼š

âš ï¸ æœ€åæé†’ï¼šè¾“å‡ºå®Œæˆåï¼Œè¯·ç¡®è®¤ä½ è¾“å‡ºäº†{test_row_count + 1}è¡Œï¼ˆ1è¡Œè¡¨å¤´ + {test_row_count}è¡Œæ•°æ®ï¼‰
"""

            print(f"æç¤ºè¯é•¿åº¦: {len(quality_prompt)} å­—ç¬¦")
            thread_messages.append({"role": "user", "content": quality_prompt})
            print("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹åˆ¤æ–­å·¥å•æ€§è´¨...")
            start_time = time.time()

            # è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ¤æ–­
            resp2 = self._call_ai_api_with_retry(
                messages=thread_messages,
                max_retries=100
            )

            elapsed_time = time.time() - start_time

            # æå–åˆ¤æ–­ç»“æœ
            quality_result = resp2.choices[0].message.content.strip()

            # æ¸…ç†AIè¿”å›çš„ç»“æœ
            if quality_result.startswith('```csv'):
                quality_result = quality_result[6:]
            elif quality_result.startswith('```'):
                quality_result = quality_result[3:]
            if quality_result.endswith('```'):
                quality_result = quality_result[:-3]
            quality_result = quality_result.strip()

            # ç¡®ä¿CSVæœ‰æ­£ç¡®çš„è¡¨å¤´
            lines = quality_result.split('\n')
            if lines and not lines[0].startswith('å·¥å•å•å·'):
                print(f"âš ï¸  è­¦å‘Š: AIè¿”å›çš„CSVç¼ºå°‘è¡¨å¤´ï¼Œè‡ªåŠ¨æ·»åŠ ")
                # ä½¿ç”¨åŸå§‹åˆ—åä½œä¸ºè¡¨å¤´
                standard_header = ','.join(original_columns)
                quality_result = standard_header + '\n' + quality_result

            print(f"âœ… å·¥å•æ€§è´¨åˆ¤æ–­å®Œæˆ")
            print(f"è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"åˆ¤æ–­ç»“æœé•¿åº¦: {len(quality_result)} å­—ç¬¦")

            # ç»Ÿè®¡åˆ¤æ–­ç»“æœçš„è¡Œæ•°
            quality_lines = quality_result.split('\n')
            quality_row_count = len([line for line in quality_lines if line.strip()]) - 1
            print(f"åˆ¤æ–­ç»“æœè¡Œæ•°: {quality_row_count} è¡Œï¼ˆé¢„æœŸ {test_row_count} è¡Œï¼‰")

            print(
                f"Tokenä½¿ç”¨: è¾“å…¥={resp2.usage.prompt_tokens}, è¾“å‡º={resp2.usage.completion_tokens}, æ€»è®¡={resp2.usage.total_tokens}")
            print("-" * 80)

            print("\n" + "=" * 80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] æ­¥éª¤5: æ•°æ®è¾“å‡ºä¸éªŒè¯")
            print("=" * 80)

            # ç»Ÿè®¡æœ€ç»ˆè¾“å‡ºçš„æ•°æ®è¡Œæ•°
            final_lines = quality_result.split('\n')
            final_row_count = len([line for line in final_lines if line.strip()]) - 1
            print(f"æœ€ç»ˆè¾“å‡ºè¡Œæ•°: {final_row_count} è¡Œ")

            # è¯»å–åŸå§‹æµ‹è¯•æ•°æ®
            original_df = pd.read_csv(StringIO(test_content), dtype=str)
            all_order_numbers = original_df['å·¥å•å•å·'].astype(str).str.strip().tolist()

            # éªŒè¯è¾“å‡ºå®Œæ•´æ€§å¹¶è‡ªåŠ¨é‡è¯•ç¼ºå¤±è®°å½•
            max_retries = 10
            retry_count = 0
            accumulated_usage = resp2.usage

            # åˆå§‹åŒ–éœ€è¦é‡è¯•çš„å·¥å•å·é›†åˆ
            missing_order_numbers = set()
            column_mismatch_order_numbers = set()

            while True:
                # æ¯æ¬¡å¾ªç¯éƒ½é‡æ–°æ£€æŸ¥å½“å‰ç»“æœ
                try:
                    # å°è¯•è§£æå½“å‰ç»“æœ
                    try:
                        current_df = pd.read_csv(StringIO(quality_result), dtype=str, on_bad_lines='skip')
                        current_order_numbers = set(current_df['å·¥å•å•å·'].astype(str).str.strip())
                    except Exception as e:
                        print(f"  âš ï¸  è§£æç»“æœå¤±è´¥: {str(e)}")
                        # å¦‚æœè§£æå¤±è´¥ï¼Œè§†ä¸ºæ‰€æœ‰è¡Œéƒ½æœ‰é—®é¢˜
                        current_order_numbers = set()

                    # æ£€æŸ¥1: æ˜¯å¦æœ‰ç¼ºå¤±çš„è®°å½•
                    all_missing = {num for num in all_order_numbers if num not in current_order_numbers}

                    # æ£€æŸ¥2: æ˜¯å¦æœ‰åˆ—æ•°ä¸åŒ¹é…çš„è®°å½•
                    # ä½¿ç”¨csv.readeré€è¡Œæ£€æŸ¥
                    csv_reader = csv.reader(StringIO(quality_result))
                    all_csv_rows = list(csv_reader)

                    if all_csv_rows:  # ç¡®ä¿æœ‰æ•°æ®
                        header = all_csv_rows[0]
                        for i, row in enumerate(all_csv_rows[1:], 1):  # ä»ç¬¬1è¡Œå¼€å§‹ï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
                            if len(row) != len(header):  # ä½¿ç”¨è¡¨å¤´é•¿åº¦ä½œä¸ºé¢„æœŸåˆ—æ•°
                                if row:  # å¦‚æœä¸æ˜¯ç©ºè¡Œ
                                    try:
                                        # å°è¯•è·å–å·¥å•å·ï¼ˆå‡è®¾æ˜¯ç¬¬ä¸€åˆ—ï¼‰
                                        order_num = str(row[0]).strip() if len(row) > 0 else f"è¡Œ{i}"
                                        column_mismatch_order_numbers.add(order_num)
                                        print(
                                            f"  âš ï¸  è¡Œ{i} (å·¥å•: {order_num}) åˆ—æ•°ä¸åŒ¹é…: é¢„æœŸ{len(header)}åˆ—, å®é™…{len(row)}åˆ—")
                                    except:
                                        print(f"  âš ï¸  è¡Œ{i} åˆ—æ•°ä¸åŒ¹é…: é¢„æœŸ{len(header)}åˆ—, å®é™…{len(row)}åˆ—")

                    # åˆå¹¶éœ€è¦é‡è¯•çš„å·¥å•å·
                    need_retry_order_numbers = missing_order_numbers.union(column_mismatch_order_numbers)

                    # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                    need_retry = False

                    if all_missing:
                        missing_order_numbers = missing_order_numbers.union(all_missing)
                        need_retry = True
                        print(f"âš ï¸  å‘ç°ç¼ºå¤±è®°å½•: {len(all_missing)} æ¡")

                    if column_mismatch_order_numbers:
                        need_retry = True
                        print(f"âš ï¸  å‘ç°åˆ—æ•°ä¸åŒ¹é…è®°å½•: {len(column_mismatch_order_numbers)} æ¡")
                        if len(column_mismatch_order_numbers) <= 10:
                            print(f"  åˆ—æ•°ä¸åŒ¹é…çš„å·¥å•å·: {', '.join(column_mismatch_order_numbers)}")

                    # æ£€æŸ¥é‡è¯•æ¬¡æ•°å’Œæ˜¯å¦éœ€è¦é‡è¯•
                    if not need_retry or retry_count >= max_retries:
                        break

                    # è®¡ç®—å®é™…éœ€è¦é‡è¯•çš„å·¥å•
                    actual_retry_order_numbers = need_retry_order_numbers
                    if not actual_retry_order_numbers:
                        break

                    print(f"\nğŸ”„ å¼€å§‹ç¬¬ {retry_count + 1} æ¬¡é‡è¯•ï¼Œå¤„ç† {len(actual_retry_order_numbers)} æ¡è®°å½•...")
                    print(
                        f"  é‡è¯•åŸå› : ç¼ºå¤±è®°å½•={len(missing_order_numbers)}æ¡, åˆ—æ•°ä¸åŒ¹é…={len(column_mismatch_order_numbers)}æ¡")

                    # æ„å»ºä»…åŒ…å«éœ€è¦é‡è¯•è®°å½•çš„æ•°æ®
                    retry_df = original_df[original_df['å·¥å•å•å·'].astype(str).str.strip()
                    .isin(actual_retry_order_numbers)].copy()

                    if len(retry_df) == 0:
                        print("  âš ï¸  æœªæ‰¾åˆ°éœ€è¦é‡è¯•çš„è®°å½•ï¼Œåœæ­¢é‡è¯•")
                        break

                    # ç¡®ä¿é‡è¯•æ•°æ®åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—
                    for col in original_columns:
                        if col not in retry_df.columns:
                            retry_df[col] = ''

                    retry_csv = retry_df.to_csv(index=False)

                    # æ„å»ºé‡è¯•æç¤ºè¯
                    retry_prompt = f"""
æ ¹æ®åˆšæ‰å­¦ä¹ çš„è§„åˆ™ï¼Œå¯¹ä¸‹é¢è¿™äº›**éœ€è¦é‡æ–°åˆ¤æ–­çš„è®°å½•**è¿›è¡Œå·¥å•æ€§è´¨åˆ¤æ–­ã€‚

éœ€è¦é‡è¯•çš„è®°å½•æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰ï¼š
{retry_csv}

**é‡è¦è¯´æ˜ï¼š**
1. è¿™æ˜¯ç¬¬ {retry_count + 1} æ¬¡é‡è¯•
2. è¿™äº›è®°å½•åœ¨ä¹‹å‰çš„åˆ¤æ–­ä¸­å­˜åœ¨é—®é¢˜ï¼ˆç¼ºå¤±æˆ–åˆ—æ•°ä¸æ­£ç¡®ï¼‰
3. å¿…é¡»å¯¹æ‰€æœ‰ {len(actual_retry_order_numbers)} æ¡è®°å½•éƒ½è¿›è¡Œåˆ¤æ–­
4. ä¸¥æ ¼æŒ‰ç…§ä¹‹å‰å­¦ä¹ çš„è§„åˆ™è¿›è¡Œåˆ¤æ–­
5. **ç‰¹åˆ«æ³¨æ„ï¼šå¿…é¡»ç¡®ä¿æ¯æ¡è®°å½•çš„åˆ—æ•°æ­£ç¡®ï¼**

åˆ¤æ–­æµç¨‹ï¼ˆå¿…é¡»ä¸¥æ ¼æ‰§è¡Œï¼‰ï¼š

**ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¡¬æ€§è§„åˆ™ï¼ˆç¬¬ä¸€å±‚ï¼‰**
å¯¹æ¯æ¡è®°å½•ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å‘½ä¸­ä»¥ä¸‹ç¡¬æ€§è§„åˆ™ï¼š

Aç±»ï¼ˆè´¨é‡å·¥å•ï¼‰ï¼š
1. æ–°æœºé»„é‡‘æ³•åˆ™ï¼šè´­æœº/å®‰è£…æ—¥æœŸâ‰¤30å¤© + æ¢æœº/é€€æœº/é€€è´§
2. äº§å“é‰´å®šæ”¿ç­–ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"äº§å“é‰´å®š" + æ¢æœº/é€€æœº/é€€è´§
3. æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢ï¼šæ—§ä»¶/æ–°ä»¶åœ¨æ ¸å¿ƒéƒ¨ä»¶åº“ä¸­
4. æ»¤èŠ¯è´¨é‡ç¼ºé™·ï¼šæ»¤èŠ¯ + æ¼ç¢³/é»‘ç‚¹/é»‘æ¸£/ç¢³ç²‰
5. ä¿å¤–è½¬ä¿å†…ï¼šä¿å¤–è½¬ä¿å†… + æ»¤èŠ¯
6. å™ªéŸ³æ¢æœºï¼šå™ªéŸ³/åˆ†è´ + æ¢æœº/é€€æœº/é€€è´§

Bç±»ï¼ˆéè´¨é‡å·¥å•ï¼‰ï¼š
1. å¤–éƒ¨åŠ è£…ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"åŠ è£…"
2. å®‰å…¨ç»´æŠ¤ï¼šæœåŠ¡é¡¹ç›®åŒ…å«"å®‰å…¨ç»´æŠ¤"
3. ç”¨æˆ·/ç¯å¢ƒè´£ä»»ï¼šå¤„ç†æ–¹æ¡ˆåŒ…å«ç”¨æˆ·/å®¢æˆ·/å°ç›†/å¨æˆ¿/ä¸‹æ°´/ç¬¬ä¸‰æ–¹/æ°´å‹/æ°´è´¨

**å¦‚æœå‘½ä¸­ç¡¬æ€§è§„åˆ™ï¼Œç«‹å³åˆ¤å®šï¼Œä¸å†ç»§ç»­ï¼**

**ç¬¬äºŒæ­¥ï¼šåº”ç”¨å­¦ä¹ çš„æ¨¡å¼ï¼ˆç¬¬äºŒå±‚ï¼‰**
å¦‚æœç¬¬ä¸€æ­¥æœªå‘½ä¸­ï¼Œåˆ™åº”ç”¨ä½ ä»è®­ç»ƒæ•°æ®ä¸­å­¦åˆ°çš„è§„åˆ™ã€‚

**ç¬¬ä¸‰æ­¥ï¼šé»˜è®¤ç­–ç•¥**
å¦‚æœä¿¡æ¯ä¸¥é‡ä¸è¶³ï¼Œé»˜è®¤ä¸º"éè´¨é‡å·¥å•"

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- å¿…é¡»åŒ…å«æ‰€æœ‰19ä¸ªå­—æ®µï¼šå·¥å•å•å·,å·¥å•æ€§è´¨,åˆ¤å®šä¾æ®,ä¿å†…ä¿å¤–,æ‰¹æ¬¡å…¥åº“æ—¥æœŸ,å®‰è£…æ—¥æœŸ,è´­æœºæ—¥æœŸ,äº§å“åç§°,å¼€å‘ä¸»ä½“,æ•…éšœéƒ¨ä½åç§°,æ•…éšœç»„,æ•…éšœç±»åˆ«,æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡,ç»´ä¿®æ–¹å¼,æ—§ä»¶åç§°,æ–°ä»¶åç§°,æ¥ç”µå†…å®¹,ç°åœºè¯Šæ–­æ•…éšœç°è±¡,å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨
- **å·¥å•æ€§è´¨**ï¼šåªèƒ½æ˜¯"è´¨é‡å·¥å•"æˆ–"éè´¨é‡å·¥å•"
- **åˆ¤å®šä¾æ®**ï¼šå¿…é¡»æ˜ç¡®è¯´æ˜åˆ¤æ–­ç†ç”±
- æ¯è¡Œå¿…é¡»ä¸¥æ ¼åŒ…å«19ä¸ªå­—æ®µ
- ä»…è¾“å‡ºCSVæ ¼å¼æ•°æ®ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Š
- **ç‰¹åˆ«æ³¨æ„CSVæ ¼å¼**ï¼šå¦‚æœå­—æ®µå†…å®¹åŒ…å«é€—å·ã€å¼•å·æˆ–æ¢è¡Œç¬¦ï¼Œå¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹

ğŸš¨ **å¼ºåˆ¶è¦æ±‚ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š**
1. **å¿…é¡»è¾“å‡ºæ‰€æœ‰{len(actual_retry_order_numbers)}æ¡è®°å½•ï¼Œä¸€æ¡éƒ½ä¸èƒ½å°‘ï¼**
2. **æ¯è¡Œå¿…é¡»æœ‰ä¸”åªæœ‰{expected_column_count}ä¸ªå­—æ®µï¼**
3. **ç¬¬ä¸€è¡Œå¿…é¡»æ˜¯è¡¨å¤´è¡Œï¼ˆåˆ—åï¼‰**
4. **ä»ç¬¬äºŒè¡Œå¼€å§‹æ˜¯æ•°æ®è¡Œï¼Œå…±{len(actual_retry_order_numbers)}è¡Œæ•°æ®**

è¯·å¼€å§‹é‡æ–°åˆ¤æ–­ï¼ˆå…±{len(actual_retry_order_numbers)}æ¡éœ€è¦é‡è¯•çš„è®°å½•ï¼‰ï¼š
"""

                    # è°ƒç”¨AIæ¨¡å‹é‡è¯•
                    retry_messages = messages.copy()
                    retry_messages.append({"role": "user", "content": retry_prompt})

                    print(f"  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹é‡è¯•...")
                    retry_start_time = time.time()

                    retry_resp = self._call_ai_api_with_retry(
                        messages=thread_messages,
                        max_retries=100
                    )

                    retry_elapsed = time.time() - retry_start_time
                    print(f"  âœ… é‡è¯•å®Œæˆï¼Œè€—æ—¶: {retry_elapsed:.2f} ç§’")

                    # æå–é‡è¯•ç»“æœ
                    retry_result = retry_resp.choices[0].message.content.strip()

                    # æ¸…ç†é‡è¯•ç»“æœ
                    if retry_result.startswith('```csv'):
                        retry_result = retry_result[6:]
                    elif retry_result.startswith('```'):
                        retry_result = retry_result[3:]
                    if retry_result.endswith('```'):
                        retry_result = retry_result[:-3]
                    retry_result = retry_result.strip()

                    # ç¡®ä¿æœ‰æ­£ç¡®çš„è¡¨å¤´
                    retry_lines = retry_result.split('\n')
                    if retry_lines and not retry_lines[0].startswith('å·¥å•å•å·'):
                        retry_result = ','.join(original_columns) + '\n' + retry_result
                        retry_lines = retry_result.split('\n')

                    # è§£æé‡è¯•ç»“æœï¼Œåªä¿ç•™åˆ—æ•°æ­£ç¡®çš„è¡Œ
                    valid_retry_data = []
                    invalid_retry_orders = set()

                    csv_reader = csv.reader(StringIO(retry_result))
                    retry_csv_rows = list(csv_reader)

                    if retry_csv_rows:
                        retry_header = retry_csv_rows[0]
                        for row in retry_csv_rows[1:]:
                            if not row:  # è·³è¿‡ç©ºè¡Œ
                                continue
                            if len(row) == len(retry_header):
                                # åˆ—æ•°æ­£ç¡®ï¼Œæ·»åŠ åˆ°æœ‰æ•ˆæ•°æ®
                                valid_retry_data.append(','.join(
                                    [f'"{field}"' if ',' in str(field) or '"' in str(field) else str(field) for field in
                                     row]))
                            else:
                                # åˆ—æ•°ä¸æ­£ç¡®ï¼Œè®°å½•å·¥å•å·
                                if row and len(row) > 0:
                                    order_num = str(row[0]).strip('"\'')
                                    invalid_retry_orders.add(order_num)
                                    print(
                                        f"  âš ï¸  é‡è¯•ç»“æœä¸­å·¥å• {order_num} åˆ—æ•°ä¸åŒ¹é…: é¢„æœŸ{len(retry_header)}åˆ—, å®é™…{len(row)}åˆ—")

                    retry_valid_count = len(valid_retry_data)
                    retry_invalid_count = len(invalid_retry_orders)

                    print(f"  é‡è¯•ç»“æœ: æœ‰æ•ˆ{retry_valid_count}è¡Œ, åˆ—æ•°é”™è¯¯{retry_invalid_count}è¡Œ")

                    if retry_valid_count > 0:
                        # ä»å½“å‰ç»“æœä¸­ç§»é™¤éœ€è¦é‡è¯•çš„è¡Œ
                        try:
                            # å…ˆå°è¯•è§£æå½“å‰ç»“æœ
                            current_csv_reader = csv.reader(StringIO(quality_result))
                            current_csv_rows = list(current_csv_reader)

                            if current_csv_rows:
                                # è·å–å½“å‰ç»“æœçš„è¡¨å¤´
                                current_header = current_csv_rows[0]

                                # ç­›é€‰å‡ºä¸éœ€è¦é‡è¯•çš„è¡Œ
                                valid_current_rows = []
                                for row in current_csv_rows[1:]:
                                    if not row:
                                        continue
                                    if len(row) > 0:
                                        order_num = str(row[0]).strip('"\'').strip()
                                        if order_num not in actual_retry_order_numbers:
                                            # ç¡®ä¿è¿™è¡Œåˆ—æ•°æ­£ç¡®
                                            if len(row) == len(current_header):
                                                valid_current_rows.append(row)
                                            else:
                                                print(f"  âš ï¸  å½“å‰ç»“æœä¸­å·¥å• {order_num} åˆ—æ•°ä¸æ­£ç¡®ï¼Œå°†è¢«ç§»é™¤")

                                # åˆå¹¶ç»“æœ
                                merged_rows = [current_header] + valid_current_rows

                                # æ·»åŠ æœ‰æ•ˆçš„é‡è¯•è¡Œ
                                for retry_line in valid_retry_data:
                                    retry_csv_reader = csv.reader(StringIO(retry_line))
                                    retry_row = next(retry_csv_reader)
                                    merged_rows.append(retry_row)

                                # é‡æ–°ç”ŸæˆCSV
                                output_lines = []
                                for row in merged_rows:
                                    # æ­£ç¡®å¤„ç†CSVæ ¼å¼
                                    formatted_row = []
                                    for field in row:
                                        field_str = str(field)
                                        if ',' in field_str or '"' in field_str or '\n' in field_str:
                                            # è½¬ä¹‰åŒå¼•å·å¹¶åŒ…è£¹å­—æ®µ
                                            field_str = field_str.replace('"', '""')
                                            field_str = f'"{field_str}"'
                                        formatted_row.append(field_str)
                                    output_lines.append(','.join(formatted_row))

                                quality_result = '\n'.join(output_lines)

                                # é‡æ–°è®¡ç®—è¡Œæ•°
                                final_lines = quality_result.split('\n')
                                final_row_count = len([line for line in final_lines if line.strip()]) - 1

                                print(f"  âœ… åˆå¹¶å®Œæˆï¼Œå½“å‰æ€»è¡Œæ•°: {final_row_count} è¡Œ")
                        except Exception as e:
                            print(f"  âš ï¸  åˆå¹¶ç»“æœæ—¶å‡ºé”™: {str(e)}")
                            # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨é‡è¯•ç»“æœ
                            quality_result = '\n'.join([','.join(original_columns)] + valid_retry_data)

                        # ç´¯ç§¯tokenä½¿ç”¨é‡
                        accumulated_usage = type('obj', (object,), {
                            'prompt_tokens': accumulated_usage.prompt_tokens + retry_resp.usage.prompt_tokens,
                            'completion_tokens': accumulated_usage.completion_tokens + retry_resp.usage.completion_tokens,
                            'total_tokens': accumulated_usage.total_tokens + retry_resp.usage.total_tokens
                        })()
                    else:
                        print(f"  âš ï¸  é‡è¯•æœªè¿”å›æœ‰æ•ˆæ•°æ®")

                    retry_count += 1

                    # æ›´æ–°éœ€è¦é‡è¯•çš„é›†åˆï¼ˆåªä¿ç•™ä»ç„¶æœ‰é—®é¢˜çš„ï¼‰
                    column_mismatch_order_numbers = invalid_retry_orders
                    missing_order_numbers = set()  # é‡è¯•åæ¸…ç©ºç¼ºå¤±è®°å½•

                except Exception as e:
                    print(f"  âŒ è§£æé‡è¯•ç»“æœæ—¶å‡ºé”™: {str(e)}")
                    break

            # æœ€ç»ˆéªŒè¯
            final_lines = quality_result.split('\n')
            final_row_count = len([line for line in final_lines if line.strip()]) - 1

            if final_row_count < test_row_count:
                missing_count = test_row_count - final_row_count
                print(
                    f"âš ï¸  è­¦å‘Š: ç»è¿‡ {retry_count} æ¬¡é‡è¯•åä»ä¸å®Œæ•´ï¼ç¼ºå°‘ {missing_count} æ¡è®°å½• ({final_row_count}/{test_row_count})")
            elif final_row_count > test_row_count:
                extra_count = final_row_count - test_row_count
                print(f"âš ï¸  è­¦å‘Š: è¾“å‡ºè¡Œæ•°è¶…å‡ºé¢„æœŸï¼å¤šå‡º {extra_count} æ¡è®°å½•")
            else:
                if retry_count > 0:
                    print(f"âœ… è¾“å‡ºå®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼ˆç»è¿‡ {retry_count} æ¬¡é‡è¯•ï¼‰")
                else:
                    print(f"âœ… è¾“å‡ºå®Œæ•´æ€§éªŒè¯é€šè¿‡")

            # æœ€ç»ˆåˆ—æ•°éªŒè¯
            print("\nğŸ” æœ€ç»ˆåˆ—æ•°éªŒè¯:")
            all_column_valid = True
            csv_reader = csv.reader(StringIO(quality_result))
            csv_rows = list(csv_reader)

            if csv_rows:
                header = csv_rows[0]
                expected_cols = len(header)
                for i, row in enumerate(csv_rows[1:], 1):
                    if not row:
                        continue
                    actual_cols = len(row)
                    if actual_cols != expected_cols:
                        all_column_valid = False
                        order_num = str(row[0]).strip('"\'') if row and len(row) > 0 else f"ç¬¬{i}è¡Œ"
                        print(f"  âŒ è¡Œ{i} (å·¥å•: {order_num}): åˆ—æ•°ä¸åŒ¹é… (é¢„æœŸ{expected_cols}, å®é™…{actual_cols})")

            if all_column_valid:
                print("  âœ… æ‰€æœ‰è¡Œåˆ—æ•°æ­£ç¡®")

            print("-" * 80)

            # ä½¿ç”¨ç´¯ç§¯çš„tokenä½¿ç”¨é‡
            resp2.usage.prompt_tokens = accumulated_usage.prompt_tokens
            resp2.usage.completion_tokens = accumulated_usage.completion_tokens
            resp2.usage.total_tokens = accumulated_usage.total_tokens

            # åˆå¹¶tokenä½¿ç”¨æƒ…å†µ
            total_usage = {
                'learn_prompt_tokens': messages[0].get('usage', {}).get('prompt_tokens', 0) if len(messages) > 0 else 0,
                'learn_completion_tokens': messages[0].get('usage', {}).get('completion_tokens', 0) if len(
                    messages) > 0 else 0,
                'judge_prompt_tokens': resp2.usage.prompt_tokens,
                'judge_completion_tokens': resp2.usage.completion_tokens,
                'total_tokens': resp2.usage.total_tokens
            }

            print("\n" + "=" * 80)
            print("[è´¨é‡å·¥å•æ£€æµ‹] å¤„ç†å®Œæˆ")
            print("=" * 80)
            print(f"âœ… æ‰€æœ‰æ­¥éª¤å·²å®Œæˆ")
            print(f"è¾“å…¥æ•°æ®: {test_row_count} è¡Œ")
            print(f"è¾“å‡ºæ•°æ®: {final_row_count} è¡Œ")
            print(f"é‡è¯•æ¬¡æ•°: {retry_count} æ¬¡")
            print(
                f"æ€»Tokenä½¿ç”¨: è¾“å…¥={resp2.usage.prompt_tokens}, è¾“å‡º={resp2.usage.completion_tokens}, æ€»è®¡={resp2.usage.total_tokens}")
            print("=" * 80 + "\n")

            return quality_result, total_usage

        except Exception as e:
            print(f"\n" + "=" * 80)
            print("âŒ é”™è¯¯: ä¸¤é˜¶æ®µæ¨ç†å¤±è´¥")
            print("=" * 80)
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            import traceback
            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
            print("=" * 80 + "\n")
            raise Exception(f"ä¸¤é˜¶æ®µæ¨ç†å¤±è´¥: {str(e)}")

    import pandas as pd
    from typing import Tuple, List, Dict, Optional
    def batch_process_quality_from_db(self, filename: str, training_excel: str, batch_size: int = 50,
                                      max_workers: int = 5) -> tuple:
        """åˆ†æ‰¹ä»æ•°æ®åº“è¯»å–æ•°æ®å¹¶è¿›è¡Œè´¨é‡å·¥å•åˆ¤æ–­ï¼ˆå¹¶è¡Œç‰ˆï¼‰

        Args:
            filename (str): workorder_dataè¡¨ä¸­çš„filenameå­—æ®µå€¼
            training_excel (str): è®­ç»ƒæ•°æ®Excelæ–‡ä»¶è·¯å¾„
            batch_size (int): æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°ï¼Œé»˜è®¤50æ¡
            max_workers (int): æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤5ï¼ˆæ ¹æ®AI/æ•°æ®åº“èƒ½åŠ›è°ƒæ•´ï¼‰

        Returns:
            tuple: (åˆå¹¶åçš„CSVç»“æœ, æ€»tokenä½¿ç”¨ç»Ÿè®¡, å¤„ç†çš„æ€»è®°å½•æ•°)
        """
        from modules.auth import db
        from modules.excel.models import WorkorderData, WorkorderUselessdata1, WorkorderUselessdata2
        from flask import current_app
        from concurrent.futures import ThreadPoolExecutor, as_completed

        try:
            print("\n" + "=" * 80)
            print("[åˆ†æ‰¹è´¨é‡å·¥å•æ£€æµ‹] å¼€å§‹å¤„ç†ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰")
            print("=" * 80)
            print(f"æ–‡ä»¶å: {filename}")
            print(f"æ‰¹æ¬¡å¤§å°: {batch_size}æ¡/æ‰¹")
            print(f"æœ€å¤§å¹¶å‘æ•°: {max_workers}")
            print("-" * 80)

            # ç¬¬ä¸€æ­¥ï¼šå­¦ä¹ è§„åˆ™ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼Œä¸²è¡Œï¼‰
            print("\n[æ­¥éª¤1] å­¦ä¹ è´¨é‡åˆ¤æ–­è§„åˆ™...")
            messages, rules, usage1 = self.learn_quality_rules(training_excel, True)
            print(f"âœ… è§„åˆ™å­¦ä¹ å®Œæˆ")
            print("-" * 80)

            # ç¬¬äºŒæ­¥ï¼šæŸ¥è¯¢æ€»è®°å½•æ•°ï¼ˆä¸²è¡Œï¼‰
            print("\n[æ­¥éª¤2] æŸ¥è¯¢æ•°æ®åº“è®°å½•...")
            total_records = WorkorderData.query.filter_by(filename=filename).count()
            print(f"æ€»è®°å½•æ•°: {total_records}æ¡")

            if total_records == 0:
                print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•è®°å½•")
                return "", {'strict_rules': 'n/a'}, 0

            # è®¡ç®—æ‰¹æ¬¡æ•°
            total_batches = (total_records + batch_size - 1) // batch_size
            print(f"æ‰¹æ¬¡æ•°: {total_batches}æ‰¹")
            print("-" * 80)

            # å‡†å¤‡æ‰¹æ¬¡å‚æ•°ï¼ˆéœ€ä¼ é€’appå¯¹è±¡ï¼Œè§£å†³ä¸Šä¸‹æ–‡é—®é¢˜ï¼‰
            app = current_app._get_current_object()  # è·å–çœŸå®çš„appå¯¹è±¡ï¼ˆéä»£ç†ï¼‰
            expected_columns = [
                'å·¥å•å•å·', 'å·¥å•æ€§è´¨', 'åˆ¤å®šä¾æ®', 'ä¿å†…ä¿å¤–', 'æ‰¹æ¬¡å…¥åº“æ—¥æœŸ', 'å®‰è£…æ—¥æœŸ',
                'è´­æœºæ—¥æœŸ', 'äº§å“åç§°', 'å¼€å‘ä¸»ä½“', 'æ•…éšœéƒ¨ä½åç§°', 'æ•…éšœç»„', 'æ•…éšœç±»åˆ«',
                'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', 'ç»´ä¿®æ–¹å¼', 'æ—§ä»¶åç§°', 'æ–°ä»¶åç§°', 'æ¥ç”µå†…å®¹',
                'ç°åœºè¯Šæ–­æ•…éšœç°è±¡', 'å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨'
            ]
            batch_params = []
            for batch_num in range(total_batches):
                offset = batch_num * batch_size
                limit = min(batch_size, total_records - offset)
                batch_params.append({
                    'app': app,
                    'filename': filename,
                    'batch_num': batch_num,
                    'offset': offset,
                    'limit': limit,
                    'expected_columns': expected_columns,
                    'messages': messages
                })


            # ç¬¬ä¸‰æ­¥ï¼šå¹¶è¡Œå¤„ç†æ‰¹æ¬¡
            all_results: List[str] = []
            header_line: Optional[str] = None
            total_token_usage: Dict = {'strict_rules': 'n/a'}
            processed_records = 0  # å·²å¤„ç†çš„è®°å½•æ•°

            print(f"\n[æ­¥éª¤3] å¯åŠ¨{max_workers}ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰¹æ¬¡...")
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
                future_to_batch = {
                    executor.submit(self._process_single_batch, params): params
                    for params in batch_params
                }

                # éå†å®Œæˆçš„ä»»åŠ¡ï¼Œæ”¶é›†ç»“æœ
                for future in as_completed(future_to_batch):
                    params = future_to_batch[future]
                    batch_num = params['batch_num'] + 1
                    total_batch = total_batches
                    try:
                        # è·å–æ‰¹æ¬¡ç»“æœ
                        batch_header, batch_data, batch_usage, batch_count = future.result()
                        processed_records += batch_count

                        # çº¿ç¨‹å®‰å…¨åœ°å¤„ç†è¡¨å¤´ï¼ˆä»…ä¿ç•™ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„è¡¨å¤´ï¼‰
                        with header_lock:
                            if header_line is None and batch_header:
                                header_line = batch_header

                        # æ”¶é›†æ•°æ®è¡Œ
                        if batch_data:
                            all_results.extend(batch_data)

                        # åˆå¹¶tokenä½¿ç”¨ï¼ˆæ­¤å¤„å¯æ ¹æ®å®é™…éœ€æ±‚ç´¯åŠ ï¼Œç¤ºä¾‹ä»…ä¿ç•™æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼‰
                        if batch_usage:
                            total_token_usage = batch_usage

                        with print_lock:
                            print(f"âœ… æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆï¼ˆç´¯è®¡{len(all_results)}/{total_records}è¡Œï¼‰")

                    except Exception as e:
                        import traceback
                        with print_lock:
                            print(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥ï¼š{str(e)}")
                            print(f"é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")

            # ç¬¬å››æ­¥ï¼šåˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
            print("\n" + "=" * 80)
            print(f"[æ­¥éª¤4] åˆå¹¶æ‰¹æ¬¡ç»“æœï¼Œ{len(all_results)}")
            # for item in all_results:
            #     print(item + '\n')
            print("=" * 80)

            if header_line is None:
                print("âŒ é”™è¯¯: æœªè·å–åˆ°è¡¨å¤´")
                return "", total_token_usage, 0

            # ç»„è£…å®Œæ•´CSV

            final_csv = header_line + '\n' + '\n'.join(all_results)
            final_row_count = len(all_results)

            print(f"âœ… åˆå¹¶å®Œæˆ")
            print(f"æ€»æ•°æ®è¡Œ: {final_row_count}è¡Œ")
            print(f"å·²å¤„ç†è®°å½•: {processed_records}æ¡")
            print(f"é¢„æœŸè®°å½•: {total_records}æ¡")

            if final_row_count != total_records:
                print(f"âš ï¸  è­¦å‘Š: ç»“æœè¡Œæ•°({final_row_count})ä¸è®°å½•æ•°({total_records})ä¸ä¸€è‡´")

            print("=" * 80 + "\n")

            return final_csv, total_token_usage, processed_records

        except Exception as e:
            import traceback
            print(f"\nâŒ é”™è¯¯: åˆ†æ‰¹å¤„ç†å¤±è´¥")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            return "", {'strict_rules': 'n/a', 'error': f"åˆ†æ‰¹å¤„ç†å¤±è´¥: {str(e)}"}, 0

    def _process_single_batch(self, params: dict) -> Tuple[Optional[str], List[str], Optional[Dict], int]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„é€»è¾‘ï¼ˆä¾›çº¿ç¨‹è°ƒç”¨ï¼‰

        Returns:
            Tuple: (è¡¨å¤´è¡Œ, æ•°æ®è¡Œåˆ—è¡¨, tokenä½¿ç”¨ç»Ÿè®¡, å¤„ç†çš„è®°å½•æ•°)
        """
        app = params['app']
        filename = params['filename']
        batch_num = params['batch_num']
        offset = params['offset']
        limit = params['limit']
        expected_columns = params['expected_columns']
        messages = params['messages']

        # æ¨é€Flask appä¸Šä¸‹æ–‡ï¼ˆå…³é”®ï¼šè§£å†³å¤šçº¿ç¨‹ä¸­current_app/dbä¸å¯ç”¨çš„é—®é¢˜ï¼‰
        with app.app_context():
            from modules.auth import db  # é‡æ–°å¯¼å…¥dbï¼Œç¡®ä¿çº¿ç¨‹å†…çš„ä¸Šä¸‹æ–‡
            from modules.excel.models import WorkorderData, WorkorderUselessdata1, WorkorderUselessdata2

            batch_header: Optional[str] = None
            batch_data: List[str] = []
            batch_usage: Optional[Dict] = None
            batch_count = 0
            temp_excel_path = None

            try:
                # è®¡ç®—æ‰¹æ¬¡èŒƒå›´
                start = offset + 1
                end = min(offset + limit, WorkorderData.query.filter_by(filename=filename).count())

                with print_lock:
                    print(f"\n[æ‰¹æ¬¡ {batch_num + 1}] å¤„ç†è®°å½• {start} è‡³ {end}")

                # 1. ä»æ•°æ®åº“æŸ¥è¯¢æœ¬æ‰¹æ¬¡è®°å½•ï¼ˆæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹çš„Sessionï¼Œè‡ªåŠ¨æäº¤/å›æ»šï¼‰
                records = WorkorderData.query.filter_by(filename=filename).offset(offset).limit(limit).all()
                # print("æ•°æ®åº“")
                # print(records)
                batch_count = len(records)

                if not records:
                    with print_lock:
                        print(f"[æ‰¹æ¬¡ {batch_num + 1}] âš ï¸  æœ¬æ‰¹æ¬¡æ— è®°å½•ï¼Œè·³è¿‡")
                    return None, [], None, 0

                with print_lock:
                    print(f"[æ‰¹æ¬¡ {batch_num + 1}] æŸ¥è¯¢åˆ° {batch_count} æ¡è®°å½•")

                # 2. æ„é€ 19å­—æ®µæ•°æ®
                def norm(v):
                    return '' if v is None or v == 'None' or (isinstance(v, float) and pd.isna(v)) else str(v)

                temp_data = []
                for record in records:
                    # å…³è”æŸ¥è¯¢å­è¡¨æ•°æ®ï¼ˆæ¯ä¸ªè®°å½•ç‹¬ç«‹æŸ¥è¯¢ï¼Œç¡®ä¿æ•°æ®æ­£ç¡®æ€§ï¼‰
                    u1 = WorkorderUselessdata1.query.filter_by(filename=filename, workAlone=record.workAlone).first()
                    u2 = WorkorderUselessdata2.query.filter_by(filename=filename, workAlone=record.workAlone).first()

                    row_data = {
                        'å·¥å•å•å·': norm(record.workAlone),
                        'å·¥å•æ€§è´¨': norm(record.workOrderNature),
                        'åˆ¤å®šä¾æ®': norm(record.judgmentBasis),
                        'ä¿å†…ä¿å¤–': norm(u1.internalExternalInsurance if u1 else ''),
                        'æ‰¹æ¬¡å…¥åº“æ—¥æœŸ': norm(u1.batchWarehousingDate if u1 else ''),
                        'å®‰è£…æ—¥æœŸ': norm(u1.installDate if u1 else ''),
                        'è´­æœºæ—¥æœŸ': norm(u1.purchaseDate if u1 else ''),
                        'äº§å“åç§°': norm(u1.productName if u1 else ''),
                        'å¼€å‘ä¸»ä½“': norm(u1.developmentSubject if u1 else ''),
                        'æ•…éšœéƒ¨ä½åç§°': norm(record.replacementPartName),
                        'æ•…éšœç»„': norm(record.faultGroup),
                        'æ•…éšœç±»åˆ«': norm(record.faultClassification),
                        'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡': norm(record.faultPhenomenon),
                        'ç»´ä¿®æ–¹å¼': norm(u2.maintenanceMode if u2 else ''),
                        'æ—§ä»¶åç§°': norm(u2.oldPartName if u2 else ''),
                        'æ–°ä»¶åç§°': norm(u2.newPartName if u2 else ''),
                        'æ¥ç”µå†…å®¹': norm(record.callContent),
                        'ç°åœºè¯Šæ–­æ•…éšœç°è±¡': norm(record.onsiteFaultPhenomenon),
                        'å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨': norm(record.remarks),
                    }

                    temp_data.append({k: row_data.get(k, '') for k in expected_columns})

                # print("å®Œæˆæ•°æ®")
                # for item in temp_data:
                #     print(item)
                #     print('\n')
                df_batch = pd.DataFrame(temp_data, columns=expected_columns)
                with print_lock:
                    print(f"[æ‰¹æ¬¡ {batch_num + 1}] æ„é€ æ•°æ®: {len(df_batch)}è¡Œ x {len(df_batch.columns)}åˆ—")

                # 3. åˆ›å»ºä¸´æ—¶Excelæ–‡ä»¶
                with tempfile.NamedTemporaryFile(mode='wb', suffix='.xlsx', delete=False) as tmp:
                    temp_excel_path = tmp.name
                    df_batch.to_excel(temp_excel_path, index=False)
                    with print_lock:
                        print(f"[æ‰¹æ¬¡ {batch_num + 1}] ä¸´æ—¶æ–‡ä»¶: {os.path.basename(temp_excel_path)}")

                # 4. è°ƒç”¨AIåˆ¤æ–­ï¼ˆä½¿ç”¨å·²å­¦ä¹ çš„è§„åˆ™ï¼‰
                with print_lock:
                    print(f"[æ‰¹æ¬¡ {batch_num + 1}] å¼€å§‹AIåˆ¤æ–­...")

                batch_result, batch_usage = self.apply_quality_rules(messages, temp_excel_path)

                with print_lock:
                    print(f"[æ‰¹æ¬¡ {batch_num + 1}] âœ… AIåˆ¤æ–­å®Œæˆ")

                # 5. è§£æç»“æœ
                result_lines = batch_result.strip().split('\n')
                if len(result_lines) > 0:
                    print(len(result_lines))
                    batch_header = result_lines[0]
                    # æ•°æ®è¡Œè·³è¿‡è¡¨å¤´
                    batch_data = result_lines[1:] if len(result_lines) > 1 else []

                with print_lock:
                    print(f"[æ‰¹æ¬¡ {batch_num + 1}] ç»“æœè¡Œæ•°: {len(batch_data)}è¡Œ")
                # print('ä¸´æ—¶æ–‡ä»¶')
                # print(temp_excel_path)
                # print("åŸå§‹ç»“æœ")
                # print(batch_result)
                # print("ç»“æœ")
                # print(batch_data)
                return batch_header, batch_data, batch_usage, batch_count

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆç¡®ä¿æ— è®ºæ˜¯å¦å¼‚å¸¸éƒ½æ‰§è¡Œï¼‰
                if temp_excel_path and os.path.exists(temp_excel_path):
                    # os.remove(temp_excel_path)
                    with print_lock:
                        print(f"[æ‰¹æ¬¡ {batch_num + 1}] ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")

    def _check_empty_quality(self, csv_content):
        """
        æ£€æŸ¥å¹¶è®°å½•å·¥å•æ€§è´¨ä¸ºç©ºçš„è¡Œï¼Œä½†ä¸è¿›è¡Œè‡ªåŠ¨å¡«å……
        è®©AIæ¨¡å‹è‡ªå·±å­¦ä¹ åˆ¤æ–­ï¼Œè€Œä¸æ˜¯ä¾èµ–ç®€å•çš„å…³é”®è¯è§„åˆ™

        Args:
            csv_content: CSVæ ¼å¼çš„å­—ç¬¦ä¸²

        Returns:
            åŸå§‹CSVå­—ç¬¦ä¸²ï¼ˆä»…è®°å½•é—®é¢˜ï¼Œä¸ä¿®æ”¹ï¼‰
        """
        import io
        import csv

        lines = csv_content.strip().split('\n')
        if len(lines) <= 1:
            return csv_content

        # è§£æCSVæ£€æŸ¥ç©ºå€¼
        reader = csv.reader(io.StringIO(csv_content))
        header = next(reader)

        # æ‰¾åˆ°å·¥å•æ€§è´¨åˆ—çš„ç´¢å¼•
        try:
            quality_index = header.index('å·¥å•æ€§è´¨')
        except ValueError:
            return csv_content

        # ç»Ÿè®¡ç©ºå€¼æƒ…å†µ
        empty_count = 0
        total_count = 0
        empty_rows = []

        for idx, row in enumerate(reader, start=2):  # ä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œæ˜¯è¡¨å¤´ï¼‰
            total_count += 1
            if len(row) > quality_index:
                quality_value = row[quality_index].strip()
                if not quality_value or quality_value.lower() in ['', 'nan', 'null', 'none']:
                    empty_count += 1
                    empty_rows.append(idx)

        # è¾“å‡ºç©ºå€¼æ£€æŸ¥ç»“æœ
        if empty_count > 0:
            print(f"âš ï¸  è­¦å‘Šï¼šæ£€æµ‹åˆ° {empty_count}/{total_count} è¡Œçš„'å·¥å•æ€§è´¨'ä¸ºç©º")
            if len(empty_rows) <= 10:
                print(f"   ç©ºå€¼è¡Œå·: {', '.join(map(str, empty_rows))}")
            else:
                print(f"   ç©ºå€¼è¡Œå·ï¼ˆå‰10ä¸ªï¼‰: {', '.join(map(str, empty_rows[:10]))}...")
            print(f"   å»ºè®®ï¼šæ£€æŸ¥AIæ¨¡å‹è¾“å‡ºï¼Œæˆ–è°ƒæ•´æç¤ºè¯ä»¥æé«˜å®Œæ•´æ€§")
        else:
            print(f"âœ… æ‰€æœ‰ {total_count} è¡Œçš„'å·¥å•æ€§è´¨'å‡å·²å¡«å†™")

        return csv_content

    def _enrich_non_quality_basis(self, csv_content: str) -> str:
        import io
        import csv
        import logging
        import os

        logger = logging.getLogger('quality_basis')
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            os.makedirs('logs', exist_ok=True)
            fh = logging.FileHandler('logs/quality_basis.log', encoding='utf-8')
            fmt = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
            fh.setFormatter(fmt)
            logger.addHandler(fh)

        lines = csv_content.strip().split('\n')
        if len(lines) <= 1:
            return csv_content

        reader = csv.reader(io.StringIO(csv_content))
        header = next(reader)

        try:
            idx_order = header.index('å·¥å•å•å·')
            idx_basis = header.index('åˆ¤å®šä¾æ®')
            idx_nature = header.index('å·¥å•æ€§è´¨')
            idx_old = header.index('æ—§ä»¶åç§°') if 'æ—§ä»¶åç§°' in header else None
            idx_new = header.index('æ–°ä»¶åç§°') if 'æ–°ä»¶åç§°' in header else None
        except ValueError:
            return csv_content

        col_map = {name: i for i, name in enumerate(header)}
        def get(row, name):
            i = col_map.get(name, -1)
            return (row[i].strip() if i >= 0 and len(row) > i else '')

        rows = []
        updated = 0
        total = 0

        def well_structured(text: str) -> bool:
            t = (text or '').strip()
            if not t:
                return False
            keys = ['æ’é™¤', 'å› ç´ ', 'æ¡ˆä¾‹', 'ç½®ä¿¡åº¦', 'åŸå› ', 'ä¾æ®']
            return sum(1 for k in keys if k in t) >= 2

        def build(row):
            svc = get(row, 'æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡')
            call = get(row, 'æ¥ç”µå†…å®¹')
            diag = get(row, 'ç°åœºè¯Šæ–­æ•…éšœç°è±¡')
            plan = get(row, 'å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨')
            oldp = (row[idx_old].strip() if idx_old is not None and len(row) > idx_old else '')
            newp = (row[idx_new].strip() if idx_new is not None and len(row) > idx_new else '')

            factors = []
            if svc:
                factors.append(f"æœåŠ¡/ç°è±¡: {svc}")
            if call:
                factors.append(f"æ¥ç”µ: {call}")
            if diag:
                factors.append(f"è¯Šæ–­: {diag}")
            if plan:
                factors.append(f"æ–¹æ¡ˆ: {plan}")
            if oldp and newp and oldp != newp:
                factors.append(f"æ–°æ—§ä»¶æ›´æ›¿: {oldp} â†’ {newp}")

            excluded = ['äº§å“åˆ¶é€ /è®¾è®¡/æ¥æ–™ç¼ºé™·', 'é›¶éƒ¨ä»¶å›ºæœ‰è´¨é‡ä¸è¾¾æ ‡']
            cases = ['å‚è€ƒè®­ç»ƒæ¡ˆä¾‹ï¼šå®‰è£…/è°ƒè¯•/ç»´æŠ¤/è€—ææ›´æ¢ç±»æ ·æœ¬çš„æ ‡æ³¨å€¾å‘']

            conf = 65
            if oldp and newp and oldp != newp:
                conf = max(conf, 85)

            proof_items = []
            text_all = ' '.join([svc or '', call or '', diag or '', plan or '']).lower()
            if any(k in text_all for k in ['å®¢æˆ·', 'å®šåˆ¶', 'éœ€æ±‚', 'è¦æ±‚']):
                proof_items.append('å®¢æˆ·ç‰¹æ®Šè¦æ±‚å•/é‚®ä»¶/æ²Ÿé€šè®°å½•')
            if any(k in text_all for k in ['è§„æ ¼', 'å‚æ•°', 'å˜æ›´', 'å‡çº§']):
                proof_items.append('æŠ€æœ¯è§„æ ¼å˜æ›´è®°å½•/é…ç½®å˜æ›´å•')
            if any(k in text_all for k in ['å®‰è£…', 'è°ƒè¯•', 'æ”¹è£…', 'åŠ è£…']):
                proof_items.append('å®‰è£…/è°ƒè¯•è®°å½•æˆ–å·¥å•è¯´æ˜')
            if any(k in text_all for k in ['ç°åœº', 'æ£€æµ‹', 'è¯Šæ–­', 'æŠ¥å‘Š']):
                proof_items.append('ç°åœºè¯Šæ–­/æ£€æµ‹æŠ¥å‘Š')
            if not proof_items:
                proof_items = ['å®¢æˆ·éœ€æ±‚æ–‡æ¡£', 'æŠ€æœ¯è§„æ ¼å˜æ›´è®°å½•', 'å®‰è£…/è°ƒè¯•è®°å½•', 'ç°åœºè¯Šæ–­æŠ¥å‘Š']

            part1 = 'å…·ä½“æ’é™¤çš„è´¨é‡é—®é¢˜ç±»å‹: ' + 'ï¼›'.join(excluded)
            part2 = 'å…³é”®åˆ¤æ–­å› ç´ åˆ†æ: ' + ('ï¼›'.join(factors) if factors else 'ä¿¡æ¯æŒ‡å‘æœåŠ¡/ç¯å¢ƒ/ç”¨æˆ·ä½¿ç”¨å› ç´ ')
            part3 = 'ç›¸å…³è®­ç»ƒæ¡ˆä¾‹å‚è€ƒ: ' + ('ï¼›'.join(cases))
            part4 = f'ç½®ä¿¡åº¦è¯„åˆ†: {conf}%'
            part5 = 'è¯æ˜ææ–™æˆ–è¯´æ˜æ–‡æ¡£: ' + 'ï¼›'.join(proof_items)
            return '\n'.join([part1, part2, part3, part4, part5])

        for row in reader:
            if not row or all((c or '').strip() == '' for c in row):
                continue
            nature = (row[idx_nature].strip() if len(row) > idx_nature else '')
            if nature == 'éè´¨é‡å·¥å•':
                total += 1
                basis = (row[idx_basis].strip() if len(row) > idx_basis else '')
                if not well_structured(basis):
                    nb = build(row)
                    if len(row) <= idx_basis:
                        row.extend([''] * (idx_basis - len(row) + 1))
                    row[idx_basis] = nb
                    updated += 1
                    order_no = row[idx_order] if len(row) > idx_order else ''
                    logger.info(f"basis_generated order={order_no} chars={len(nb)}")
            rows.append(row)

        output = io.StringIO()
        w = csv.writer(output, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header)
        for r in rows:
            w.writerow(r)

        if total:
            logger.info(f"basis_summary non_quality={total} updated={updated}")

        return output.getvalue()

    def _strip_unexpected_header_rows(self, csv_text: str, expected_header: str) -> str:
        import io
        import csv
        reader = csv.reader(io.StringIO(csv_text))
        rows = list(reader)
        if not rows:
            return csv_text
        cleaned = []
        header_written = False
        for r in rows:
            line = ','.join(r).strip()
            if line == expected_header:
                if not header_written:
                    cleaned.append(r)
                    header_written = True
                continue
            if line.startswith('ç¼–å·(ç»´ä¿®è¡Œ)'):
                continue
            cleaned.append(r)
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        for r in cleaned:
            w.writerow(r)
        return out.getvalue()

    def _apply_strict_rules(self, df: pd.DataFrame) -> pd.DataFrame:
        import pandas as pd
        core_parts = set([
            'ç”µæºé€‚é…å™¨','å¾®ç”µè„‘ç”µæºæ¿','å¾®ç”µè„‘æ˜¾ç¤ºæ¿','æ§åˆ¶æ¿æ€»æˆ','ä¸»æ¿','ç»•ä¸åŠ çƒ­ä½“æ€»æˆ','ç”µçƒ­ç®¡','å¢å‹æ³µ','ç”µç£é˜€','è¿›æ°´é˜€','é«˜å‹å¼€å…³','TDSä¼ æ„Ÿå™¨','ç”µæ§é¾™å¤´','ç¯æ˜¾é¾™å¤´','æµ®çƒå¼€å…³','æµ®çƒç»„ä»¶','æµé‡è®¡','æ¸©åº¦æ„Ÿåº”å™¨','æ»¤ç½‘æ€»æˆ','æŒ‡ç¤ºç¯æ¿','æ’æ°´æ¥å¤´','å¯†å°åœˆ','çœŸç©ºçƒ­ç½æ€»æˆ','æŠ½æ°´æ³µ','è··æ¿å¼€å…³','åæ¸—é€è†œæ»¤èŠ¯','æ»¤èŠ¯åº§æ€»æˆ'
        ])
        exchange_words = ['æ¢æœº','é€€æœº','é€€è´§']
        filter_words = ['æ¼ç‚­','é»‘ç‚¹','é»‘æ¸£','ç¢³ç²‰','æ´»æ€§ç‚­ç²‰æœ«']
        env_keywords = ['ç”¨æˆ·','å®¢æˆ·','å°ç›†','å¨æˆ¿','ä¸‹æ°´','ç¬¬ä¸‰æ–¹','æ°´å‹','æ°´è´¨']
        env_causes = ['é—®é¢˜','æ¸—æ°´','åæ°´','ç®¡é“','å µ','æ¼']
        maintenance_actions = ['é‡å¯','å¤ä½','è°ƒè¯•','æ¸…æ´—','ç´§å›º','é‡æ–°å®‰è£…','åŠ å›º','æŒ‡å¯¼','è§£é‡Š','æ£€æŸ¥æ­£å¸¸','æ— å¼‚å¸¸','ä¸€åˆ‡æ­£å¸¸']
        fault_keywords = ['æ¼æ°´','ä¸é€šç”µ','ä¸åˆ¶æ°´','ä¸å‡ºæ°´','ä¸åŠ çƒ­','å™ªéŸ³å¤§','æ˜¾ç¤ºå¼‚å¸¸','E1','E3','E6']

        def contains_any(text, keys):
            t = str(text or '')
            return any(k in t for k in keys)

        def within_30_days(purchase, install):
            today = pd.Timestamp.today()
            days = []
            for s in [purchase, install]:
                dt = pd.to_datetime(str(s or ''), errors='coerce')
                if pd.notna(dt):
                    days.append((today - dt).days)
            return len(days) > 0 and min(days) <= 30

        def is_core_part(oldp, newp):
            t1 = str(oldp or '')
            t2 = str(newp or '')
            return any(p in t1 for p in core_parts) or any(p in t2 for p in core_parts)

        def is_filter(oldp, newp):
            t1 = str(oldp or '')
            t2 = str(newp or '')
            return ('æ»¤èŠ¯' in t1) or ('æ»¤èŠ¯' in t2)

        out_rows = []
        for _, row in df.iterrows():
            svc = str(row.get('æœåŠ¡é¡¹ç›®æˆ–æ•…éšœç°è±¡', '') or '')
            plan = str(row.get('å¤„ç†æ–¹æ¡ˆç®€è¿°æˆ–å¤‡æ³¨', '') or '')
            call = str(row.get('æ¥ç”µå†…å®¹', '') or '')
            diag = str(row.get('ç°åœºè¯Šæ–­æ•…éšœç°è±¡', '') or '')
            bn = str(row.get('ä¿å†…ä¿å¤–', '') or '')
            oldp = str(row.get('æ—§ä»¶åç§°', '') or '')
            newp = str(row.get('æ–°ä»¶åç§°', '') or '')
            purchase = str(row.get('è´­æœºæ—¥æœŸ', '') or '')
            install = str(row.get('å®‰è£…æ—¥æœŸ', '') or '')

            nature = ''
            basis = ''

            if within_30_days(purchase, install) and (contains_any(plan, exchange_words) or contains_any(call, exchange_words)):
                hits = ','.join([w for w in exchange_words if w in plan or w in call])
                nature, basis = 'è´¨é‡å·¥å•', f'Rule A.1: æ–°æœºé»„é‡‘æ³•åˆ™ï¼Œå‘½ä¸­å…³é”®è¯ï¼šâ€œ{hits}â€'
            elif ((('äº§å“é‰´å®š' in svc) or ('åªæ¢ä¸ä¿®æ”¿ç­–äº§å“è´¨é‡é‰´å®š' in svc)) and (contains_any(plan, exchange_words) or contains_any(call, exchange_words))):
                hits = ','.join([w for w in exchange_words if w in plan or w in call])
                nature, basis = 'è´¨é‡å·¥å•', f'Rule A.2: åªæ¢ä¸ä¿®/é‰´å®šæ”¿ç­–ï¼Œå‘½ä¸­å…³é”®è¯ï¼šâ€œ{hits}â€'
            elif (oldp or newp) and is_core_part(oldp, newp):
                nature, basis = 'è´¨é‡å·¥å•', 'Rule A.3: æ ¸å¿ƒéƒ¨ä»¶æ›´æ¢'
            elif is_filter(oldp, newp) and contains_any(call + diag, filter_words):
                hits = ','.join([w for w in filter_words if w in (call + diag)])
                nature, basis = 'è´¨é‡å·¥å•', f'Rule A.4: æ»¤èŠ¯è´¨é‡ç¼ºé™·ï¼Œå‘½ä¸­å…³é”®è¯ï¼šâ€œ{hits}â€'
            elif bn == 'ä¿å¤–è½¬ä¿å†…' and is_filter(oldp, newp):
                nature, basis = 'è´¨é‡å·¥å•', 'Rule A.5: ç‰¹æ®Šæ”¿ç­–è§¦å‘ï¼ˆä¿å¤–è½¬ä¿å†…+æ»¤èŠ¯ï¼‰'
            elif (('å™ªéŸ³' in plan) or ('å™ªéŸ³' in call) or ('åˆ†è´' in plan) or ('åˆ†è´' in call)) and (contains_any(plan, exchange_words) or contains_any(call, exchange_words)):
                nature, basis = 'è´¨é‡å·¥å•', 'Rule A.6: ä¸»è§‚æ€§èƒ½ç¼ºé™·ï¼ˆå™ªéŸ³/åˆ†è´+æ¢é€€ï¼‰'
            elif 'åŠ è£…' in svc:
                nature, basis = 'éè´¨é‡å·¥å•', 'Rule B.1: å¤–éƒ¨åŠ è£…'
            elif 'å®‰å…¨ç»´æŠ¤' in svc:
                nature, basis = 'éè´¨é‡å·¥å•', 'Rule B.2: å®‰å…¨ç»´æŠ¤'
            elif contains_any(plan, env_keywords) and contains_any(plan, env_causes):
                nature, basis = 'éè´¨é‡å·¥å•', 'Rule B.3: ç”¨æˆ·/ç¯å¢ƒè´£ä»»'
            else:
                if plan.strip() in ['ä¸Šé—¨ç»´ä¿®', 'ä¸Šé—¨æ£€æŸ¥']:
                    pass
                f = contains_any(call + diag, fault_keywords)
                m = contains_any(plan, maintenance_actions)
                if f and m:
                    nature, basis = 'éè´¨é‡å·¥å•', 'Rule 8.3: æœ‰æ•…éšœä½†ç»´æŠ¤/å¤ä½è§£å†³'
                elif f and not m:
                    nature, basis = 'è´¨é‡å·¥å•', 'Rule 8.3: æœ‰æ•…éšœä¸”éç®€å•ç»´æŠ¤'
                elif (not f) and m:
                    nature, basis = 'éè´¨é‡å·¥å•', 'Rule 8.3: æ— æ•…éšœä»…ç»´æŠ¤/è§£é‡Š'
                else:
                    nature, basis = 'éè´¨é‡å·¥å•', 'Rule 3: æœ€ç»ˆå®‰å…¨ç½‘'

            row_out = row.copy()
            row_out['å·¥å•æ€§è´¨'] = nature
            row_out['åˆ¤å®šä¾æ®'] = basis
            out_rows.append(row_out)

        return pd.DataFrame(out_rows)
    def _fix_quality_column_position(self, csv_text: str) -> str:
        import io
        import csv
        reader = csv.reader(io.StringIO(csv_text))
        header = next(reader, None)
        if not header:
            return csv_text
        try:
            qidx = header.index('å·¥å•æ€§è´¨')
        except ValueError:
            return csv_text
        rows = []
        for r in reader:
            if len(r) == len(header):
                if (not r[qidx]) and any(val in ['è´¨é‡å·¥å•', 'éè´¨é‡å·¥å•'] for val in r):
                    for j, val in enumerate(r):
                        if val in ['è´¨é‡å·¥å•', 'éè´¨é‡å·¥å•'] and j != qidx:
                            r[qidx] = val
                            r[j] = ''
                            break
            rows.append(r)
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header)
        for r in rows:
            w.writerow(r)
        return out.getvalue()

    def _ensure_order_numbers(self, csv_text: str, df_test) -> str:
        import io
        import csv
        reader = csv.reader(io.StringIO(csv_text))
        header = next(reader, None)
        if not header:
            return csv_text
        try:
            oidx = header.index('å·¥å•å•å·')
        except ValueError:
            return csv_text
        orders = list(df_test['å·¥å•å•å·']) if 'å·¥å•å•å·' in df_test.columns else list(range(1, len(df_test)+1))
        rows = []
        i = 0
        for r in reader:
            if len(r) == len(header):
                if (not r[oidx]) and i < len(orders):
                    r[oidx] = str(orders[i])
                i += 1
            rows.append(r)
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header)
        for r in rows:
            w.writerow(r)
        return out.getvalue()

    def _double_validate_replacement(self, csv_content: str) -> str:
        import io
        import csv
        import logging
        import os

        logger = logging.getLogger('quality_double_check')
        if not logger.handlers:
            logger.setLevel(logging.INFO)
            os.makedirs('logs', exist_ok=True)
            fh = logging.FileHandler('logs/quality_double_check.log', encoding='utf-8')
            fmt = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
            fh.setFormatter(fmt)
            logger.addHandler(fh)

        reader = csv.reader(io.StringIO(csv_content))
        header = next(reader)
        if 'æ—§ä»¶åç§°' not in header or 'æ–°ä»¶åç§°' not in header:
            return csv_content
        idx_order = header.index('å·¥å•å•å·') if 'å·¥å•å•å·' in header else None
        idx_old = header.index('æ—§ä»¶åç§°')
        idx_new = header.index('æ–°ä»¶åç§°')

        for row in reader:
            oldp = row[idx_old].strip() if len(row) > idx_old else ''
            newp = row[idx_new].strip() if len(row) > idx_new else ''
            if oldp and newp and oldp != newp:
                order_no = row[idx_order] if idx_order is not None and len(row) > idx_order else ''
                logger.info(f"replacement_detected order={order_no} old={oldp} new={newp}")

        return csv_content

    def _force_realign_columns(self, csv_text: str, expected_header: str, expected_count: int) -> str:
        import io
        import csv
        lines = [l for l in csv_text.strip().split('\n') if l.strip()]
        if not lines:
            return csv_text
        header_line = lines[0].strip()
        if header_line != expected_header:
            header_line = expected_header
        def clean_token(t):
            s = t.strip().strip('\r').strip()
            if s.lower() in ['nan', 'null', 'none']:
                return ''
            return s
        out = io.StringIO()
        w = csv.writer(out, quoting=csv.QUOTE_MINIMAL, lineterminator='\n')
        w.writerow(header_line.split(','))
        for raw in lines[1:]:
            if not raw.strip():
                continue
            tokens = [clean_token(x) for x in raw.split(',')]
            if len(tokens) == expected_count:
                w.writerow(tokens)
                continue
            last_idx = None
            for i in range(len(tokens)-1, -1, -1):
                v = tokens[i].strip()
                if v in ['è´¨é‡å·¥å•', 'éè´¨é‡å·¥å•']:
                    last_idx = i
                    break
            if last_idx is None:
                while len(tokens) < expected_count:
                    tokens.append('')
                if len(tokens) > expected_count:
                    tokens = tokens[:expected_count]
                w.writerow(tokens)
                continue
            last_field = tokens[last_idx]
            middle = tokens[:last_idx]
            if not middle:
                middle = ['']
            while len(middle) < expected_count - 1:
                middle.append('')
            if len(middle) > expected_count - 1:
                pos_list = [1, 2, 5, 9, 10, 11]
                target = expected_count - 1
                cur = len(middle)
                for pos in pos_list:
                    while cur > target and pos < len(middle)-1:
                        middle[pos] = f"{middle[pos]},{middle[pos+1]}"
                        middle.pop(pos+1)
                        cur -= 1
                while len(middle) > target:
                    k = min(len(middle)-2, target-1)
                    middle[k] = f"{middle[k]},{middle[k+1]}"
                    middle.pop(k+1)
            row = middle[:expected_count-1] + [last_field]
            w.writerow(row)
        return out.getvalue()

